{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["EnjP4Zhat_8t","xaxxhKAgyeQq","JtMpu3BMyh6-","Pns8QF8VWrHR","TPz7fgRaRAUv","o38bsnD-EWcq","vNVT-lr5jejz","eHxAg4fuIc2T","1zqvsEJvx33W","HraJ3_5SJHSv","hdqvYkNsg7fx","dr3XCOHn7Nc_","tCeeWN-UjXaI","lNqdhhAit9q9"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyOrFWD79ZRueMaDivRn3zUh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìî READ_ME und TO_DO"],"metadata":{"id":"EnjP4Zhat_8t"}},{"cell_type":"markdown","source":["#### README"],"metadata":{"id":"xaxxhKAgyeQq"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/README.md'\n","# üöÄ SEO Automation Pipeline mit OpenAI & Retrieval (RAG)\n","\n","Dieses Projekt bietet mehrere **komplette End-to-End-Pipelines f√ºr die SEO-Optimierung von Websites**, inklusive **Web-Crawling, SEO-Analyse, KI-gest√ºtzter Text-Optimierung und Qualit√§tskontrolle**.\n","\n","Kern des Projekts sind **automatisierte Abl√§ufe**, die von der **Datengewinnung bis zur SEO-optimierten Textgenerierung** reichen.\n","Es werden mehrere Herangehensweisen erforscht, u.a. eine langchain pipeline zur Text-Optimierung und eine all-in-one L√∂sung zur Herstellung einer\n","Mithilfe von **OpenAI (ChatGPT)** und einer **Retrieval Augmented Generation (RAG)-Architektur** wird sichergestellt, dass die finalen Texte nicht nur **SEO-freundlich**, sondern auch **grammatikalisch korrekt und hochwertig** sind.\n","\n","Ziel ist die Herstellung eines kundenfreundlichen Produktes in Form einer SEO - optimierten Version einer website im HTML Format f√ºr Demonstrationszwecke im lokalen Browser.\n","\n","## üìö Inhaltsverzeichnis\n","\n","- Features\n","- Projektstruktur\n","- Ablauf & Module\n","- Technologien\n","- Installation\n","- Nutzung\n","- Ziele\n","- Roadmap\n","\n","## ‚úÖ Features\n","\n","- üåê **Automatisiertes Web Crawling** (inkl. Filter f√ºr relevante Inhalte)\n","- ‚úçÔ∏è **Generierung von SEO-optimierten Texten** mithilfe der OpenAI API\n","- üß† **RAG-gest√ºtzte Fehlererkennung & Textkorrektur** mit Vektordatenbank (FAISS)\n","- üìä **Analyse der Optimierungsergebnisse** (Statistiken, √Ñhnlichkeiten, Visualisierungen)\n","- üìà **Keyword-Analyse und Keyword-Optimierung**\n","- üì¶ Ausgabe in **HTML und PDF** f√ºr Kunden\n","- üìä Umfangreiche **Datenvisualisierungen** (Wordclouds, Cosine Similarity, Keyword-Verteilung)\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10oR2bcugvN2MClp14ia7gnzMGX5b896t\" alt=\"SEO Heatmap\" width=\"600\">\n","\n","\n","\n","\n","## üóÇÔ∏è Projektstruktur\n","\n","```\n","SEO-Project/\n","‚îú‚îÄ‚îÄ data/                # Prompts, Fehler-Korrektur-Daten, weitere JSON Dateien\n","‚îú‚îÄ‚îÄ notebooks/           # Colab/Notebooks zum Starten und Entwickeln\n","‚îú‚îÄ‚îÄ output/              # Erzeugte Dateien (HTML, PDF, Bilder)\n","‚îÇ   ‚îú‚îÄ‚îÄ final           # Dokumente f√ºr Kunden (HTML, PDF)\n","‚îÇ   ‚îî‚îÄ‚îÄ images          # Visualisierungen\n","‚îú‚îÄ‚îÄ src/                # Source Code (Python-Klassen und Module)\n","‚îÇ   ‚îú‚îÄ‚îÄ webscraper.py    # Webscrawling und Text-Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ llmprocessor.py # Anbindung an OpenAI API, Keyword Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py       # Zentrale Chatbot-Klasse zur Kommunikation mit GPT\n","‚îÇ   ‚îú‚îÄ‚îÄ seoanalyzer.py   # Analyse und Auswertung der Texte\n","‚îÇ   ‚îú‚îÄ‚îÄ github.py        # Automatischer Upload ins GitHub Repo\n","‚îÇ   ‚îú‚îÄ‚îÄ utils.py         # Hilfsmodule (z.B. f√ºr Prompt-Management)\n","‚îÇ   ‚îî‚îÄ‚îÄ embeddingdemo.py# 3D Embedding- und Cosine Similarity Visualisierungen\n","‚îú‚îÄ‚îÄ tests/              # pytest der Hauptfunktionalit√§ten\n","‚îî‚îÄ‚îÄ requirements.txt    # Python-Abh√§ngigkeiten\n","```\n","\n","## ‚öôÔ∏è Ablauf & Module\n","\n","### 1. **Web Crawling**\n","- **src/webscraper.py**: Holt Inhalte von Webseiten, filtert irrelevante Seiten (z.B. Impressum, AGB).\n","\n","### 2. **SEO-Optimierung mit OpenAI**\n","- **src/llmprocessor.py**:\n","  - Extrahiert Keywords aus den Inhalten.\n","  - Optimiert die Texte f√ºr SEO mit gezielten Prompts.\n","\n","### 3. **Analyse & Visualisierung**\n","- **src/seoanalyzer.py**: Verarbeitet und analysiert die Original- und optimierten Texte.\n","\n","### 4. **GitHub Automation**\n","- **src/github.py**: L√§dt finale Ergebnisse in ein GitHub-Repo hoch.\n","\n","## üß∞ Technologien\n","\n","| Technologie                  | Beschreibung                                       |\n","|-----------------------------|---------------------------------------------------|\n","| Python                      | Hauptsprache                                       |\n","| OpenAI API (ChatGPT, GPT-4)  | Generative KI f√ºr SEO-Texte                       |\n","| FAISS                      | Vektorsuche f√ºr RAG und Text-Fehler                |\n","| Pandas, NumPy               | Datenanalyse und Verarbeitung                      |\n","| Matplotlib, Seaborn         | Visualisierungen                                   |\n","| Sentence Transformers       | Embedding-Erstellung f√ºr Vektordatenbank          |\n","| BeautifulSoup, Requests     | Webcrawling                                        |\n","| Google Colab                | Entwicklung und Ausf√ºhrung                        |\n","\n","## üöÄ Installation\n","\n","```bash\n","pip install -r requirements.txt\n","python -m spacy download de_core_news_sm\n","pip install faiss-cpu sentence-transformers openai wordcloud matplotlib seaborn\n","```\n","\n","## üíª Nutzung\n","\n","```python\n","scraper = WebsiteScraper(start_url=\"https://www.example.com\")\n","scraper.scrape_website()\n","\n","llm_processor = LLMProcessor(prompts_folder, get_filtered_texts, google_ads_keywords)\n","llm_processor.run_all()\n","\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n","```\n","\n","## üéØ Ziele\n","\n","- ‚úÖ Vollst√§ndige Automatisierung der SEO-Optimierung\n","- ‚úÖ RAG f√ºr sprachliche Qualit√§tskontrolle\n","- ‚úÖ Kundenfertige PDF/HTML-Reports\n","\n","## üöß Roadmap\n","\n","- [ ] **Produkt f√ºr Kunden finalisieren:** all-in-one solution f√ºr webcrawl + SEO + optimierten html code\n","- [ ] Automatische SEO Scores (z.B. Google Ads API)\n","- [ ] Automatische Keyword-Erweiterung\n","- [ ] Mehrsprachigkeit\n","- [ ] WordPress-Integration\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"443XYilDN9NA","executionInfo":{"status":"ok","timestamp":1743608048197,"user_tz":-120,"elapsed":44,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"16a2f2e0-c211-4655-86e7-80bc4dd74553"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/README.md\n"]}]},{"cell_type":"markdown","source":["####TODO"],"metadata":{"id":"JtMpu3BMyh6-"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/TODO.md'\n","# To-Do Liste: SEO Automation & KI-Projekt\n","\n","Diese Liste fasst alle anstehenden Aufgaben im Projekt zusammen\n","\n","---\n","\n","## 0. **Aktuelles und dringendes**\n","- [ ] 18.3.2025 **Version missmatch**: numpy 2.2.3 und spacy 3.5 **side effects on**: dependencies.py, excelimporter.py, Installation.ipynb **ursachen**: spacy version 3.5 verlangt numpy 1.26.4 -> version missmatch\n","      -> 23.5. class SEOAnlyzer refaktoriert und spacy ersetzt\n","- [ ] 28.3.25 lokale SEO keywords liefern manchmal falsche Stadt\n","- [ ] prompts besser organisieren\n","- [ ] all-in-one l√∂sung weiter verfolgen\n","\n","---\n","\n","## 1. **Allgemeine Projektorganisation**\n","- [ ] **Projektstruktur verbessern**: Ordner √ºbersichtlich gestalten (z.B. `src/`, `data/`, `tests/`, `notebooks/`, dependencies.py).\n","- [ ] **Dokumentation erweitern**: READ_ME und Wiki (bzw. GitHub Pages) zu jedem Modul anlegen.\n","- [ ] **Automatisierte Tests** Pytest f√ºr Kernfunktionen ausbauen.\n","- [ ] **Produkt f√ºr Kunden finalisieren**\n","- [ ] **FAISS DB**: automatisierte Erweiterung bei neu gefundenen Fehlern\n","- [ ] **Template GitHub**: issues\n","- [ ] Funktionalit√§ten aus **utils.py** √ºberdenken\n","- [ ] langfristig Umstieg auf **langchain**\n","- [ ] textprocessor durch openai **function calling** ersetzen\n","- [ ] **dependencies** und versionen robuster machen\n","- [ ] **bug reporting system** einrichten\n","\n","---\n","\n","## 2. **Vector-Datenbank (FAISS) & Retrieval**\n","- [ ] **VectorDB-Klasse finalisieren**:\n","  - [ ] Kleinere Bugs beheben\n","  - [ ] Userfreundliche Methoden f√ºr neue Eintr√§ge\n","- [ ] **Einrichtung der DB** bei Projektstart (Neubau vs. Laden) vereinheitlichen\n","- [ ] **Konfigurierbare √Ñhnlichkeits-Schwelle** (z.B. `threshold=0.6`) besser dokumentieren\n","- [ ] **Dynamische Filter** f√ºr bestimmte Fehlerkategorien (z.B. Stil vs. Grammatik) √ºberlegen\n","- [ ] **hybrides system mit knowledge tree und RAG** etablieren\n","\n","---\n","\n","## 3. **SEO-Optimierungs-Pipeline**\n","- [ ] **Prompts in JSON-Dateien** verlagern (z.B. `/data/prompts/`) und sauber verlinken\n","- [ ] **Supervisor-Feedback** integrieren & QA-Schritte definieren\n","- [ ] Langchain und SEOPageOptimizer verbinden\n","\n","---\n","\n","## 4. **SEOGrammarChecker & PromptManager**\n","- [ ] Klassenrefactoring:\n","  - [ ] **`VectorDB`** vs. **`PromptManager`** vs. **`SEOGrammarChecker`** sauber trennen\n","  - [ ] M√∂glichst wenig Code-Duplikate, mehr modulare Testbarkeit\n","- [ ] **Konfigurationsdatei** (z.B. YAML) f√ºr Pfade, wie `FAISS_PATH` & Promptordner\n","- [ ] **Erweiterbare Prompt-Templates**:\n","  - [ ] Z.B. `seo_optimization.json`, `grammar_check.json`, `supervisor.json`, etc.\n","\n","---\n","\n","## 5. **Abschluss & Integration**\n","- [ ] **Dokumentation** aller Pipelines & Klassen in der README (oder in separater Doku)\n","- [ ] **Optionale WordPress-Integration** in der Zukunft (Ideenspeicher)\n","  - [ ] Upload via REST API\n","  - [ ] Metadaten (Title, Slug, Tags etc.)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBtOaSe-CHyO","executionInfo":{"status":"ok","timestamp":1743608048263,"user_tz":-120,"elapsed":39,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"7c916f58-b00b-44b3-801e-4d049d4f4f04"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/TODO.md\n"]}]},{"cell_type":"markdown","source":["#globale Parameter\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","# START_URL = 'https://www.ewms-tech.com/'\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","# START_URL = 'https://www.malerarbeiten-koenig.de/'\n","\n","\n","EXCLUDED_WEBSITES = [\"impressum\", \"datenschutz\", \"datenschutzerkl√§rung\", \"agb\"]\n","\n","\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","\n","SRC_PATH, DATA_PATH, TEST_PATH, OUTPUT_PATH = PROJECT_ROOT + \"/src\", PROJECT_ROOT + \"/data\", PROJECT_ROOT + '/tests', PROJECT_ROOT + '/output'\n","FAISS_PATH = DATA_PATH + '/faiss_db'\n","KEYWORD_PATH = DATA_PATH + '/keywords'\n","PROMPT_PATH = DATA_PATH + '/prompts'\n","\n","\n","FINAL_IMAGES = {}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1743608052871,"user_tz":-120,"elapsed":4607,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"032e6f6f-3a4b-4008-88a5-fb7005733454"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo","executionInfo":{"status":"ok","timestamp":1743608100831,"user_tz":-120,"elapsed":47957,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# %run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5","executionInfo":{"status":"ok","timestamp":1743608100850,"user_tz":-120,"elapsed":3,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["import importlib\n","import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"],"metadata":{"id":"do4Bf0uw-y8U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e72cf2ce-ee06-42b5-d123-362e67ad4e4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Klonen des GitHub-Repositories...\n"]}]},{"cell_type":"markdown","source":["# üï∏ crawl"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import importlib\n","import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_WEBSITES)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîë SEO keywords"],"metadata":{"id":"veMjko5Tvdtt"}},{"cell_type":"markdown","source":["### üì∫ google ads seo keywords"],"metadata":{"id":"eHxAg4fuIc2T"}},{"cell_type":"code","source":["import excelimporter\n","importlib.reload(excelimporter)\n","from excelimporter import ExcelImporter\n","\n","importer = ExcelImporter(project_folder=PROJECT_ROOT, header=2)\n","keyword_df = importer.import_all()\n","\n","if keyword_df is not None:\n","    excluded_seo_keywords = ['spange de', 'kfo zentrum essen', 'gerade zahne', 'zahn spange']\n","\n","    keyword_df = keyword_df[(keyword_df['Avg. monthly searches'] > 10) &\n","    (~keyword_df['Keyword'].isin(excluded_seo_keywords))\n","    ].sort_values(by='Avg. monthly searches', ascending=False).reset_index(drop=True).copy()\n","\n","    google_ads_keywords = list(keyword_df['Keyword'])\n","\n","    keyword_df.set_index(keyword_df.columns[0], inplace=True)\n","    searches_df = keyword_df[[col for col in keyword_df.columns if col.startswith('Searches')]]\n","\n","else:\n","    print(\"Error: importer.import_all() returned None. Check your data file and ExcelImporter class.\")\n","    google_ads_keywords = None"],"metadata":{"id":"REeveZ8rwEys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keywordvisualizer\n","importlib.reload(keywordvisualizer)\n","from keywordvisualizer import KeywordVisualizer\n","\n","\n","visualizer = KeywordVisualizer(\n","    df=searches_df,\n","    output_dir=OUTPUT_PATH+'/images',\n","    image_paths=FINAL_IMAGES\n",")\n","visualizer.heatmap()"],"metadata":{"id":"LnJ5j0VZ_JWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import llmprocessor\n","importlib.reload(llmprocessor)\n","from llmprocessor import LLMProcessor\n","\n","google_ads_keywords = None if not google_ads_keywords else google_ads_keywords\n","\n","llm_processor = LLMProcessor(PROMPT_PATH, original_texts, google_ads_keywords=google_ads_keywords)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"XXzSL_SR_JRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sXLiqhti_JOm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üõè embedding demo"],"metadata":{"id":"1zqvsEJvx33W"}},{"cell_type":"code","source":["import os\n","import embeddingdemo\n","importlib.reload(embeddingdemo)\n","from embeddingdemo import EmbeddingDemo\n","\n","demo = EmbeddingDemo(output_dir=OUTPUT_PATH+'/images', final_images=FINAL_IMAGES)\n","demo.run_all_visualizations()\n","\n","print(\"Alle Embedding-Bilder:\", demo.get_image_paths())\n","print(\"Gemeinsame Bilder (shared dict):\", FINAL_IMAGES)"],"metadata":{"id":"toL_RwWuO72N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• Tests"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"markdown","source":["#### üìÉ Doku"],"metadata":{"id":"hdqvYkNsg7fx"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/tests/DOKU_TESTS.md'\n","# ‚úÖ Pytest Template: Chatbot Klasse\n","\n","## 1. Klassen & Methoden die getestet werden sollen\n","\n","- **Chatbot**\n","  - `chat()`\n","  - `chat_with_streaming()`\n","\n","---\n","\n","## 2. Beispielhafte Inputs + erwartete Outputs pro Methode\n","\n","| Methode                      | Beispiel Input                                                           | Erwartete Ausgabe    |\n","|-----------------------------|-------------------------------------------------------------------------|----------------------|\n","| `Chatbot.chat()`             | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'           | \"Test erfolgreich\"   |\n","| `Chatbot.chat_with_streaming()` | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'         | \"Test erfolgreich\"   |\n","\n","---\n","\n","## 3. Return-Typen der Methoden\n","\n","| Methode                      | R√ºckgabe-Typ |\n","|-----------------------------|--------------|\n","| `Chatbot.chat()`             | `str`        |\n","| `Chatbot.chat_with_streaming()` | `str`     |\n","\n","---\n","\n","## 4. Externe Services mocken?\n","\n","| Service         |  Mocken?                           |\n","|-----------------|-------------------------------------|\n","| OpenAI API      |  Nein                                |\n","| FAISS Index     |  Ja (kleine Test-Datenbank f√ºr FAISS) |\n","\n","---\n","\n","## 5. Ordnerstruktur f√ºr Tests\n","\n","```bash\n","/project-root/\n","    /src/\n","        chatbot.py\n","    /tests/\n","        test_chatbot.py\n","    /logs/\n","        test_report.log\n","    ...\n","```\n","\n","---\n"],"metadata":{"id":"V1xV8mVljdkL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### üë∑ code"],"metadata":{"id":"dr3XCOHn7Nc_"}},{"cell_type":"code","source":["import pytest\n","pytest.main(['-v', TEST_PATH+'/chatbot_test/chatbot_test.py'])"],"metadata":{"id":"n68k1npffFsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® Error Collection"],"metadata":{"id":"tCeeWN-UjXaI"}},{"cell_type":"code","source":["error_corrections = {\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\":\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, sowie Kau- und Sprechprobleme effektiv behandeln.\",\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\":\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch und Swahili. Dazu hat sie Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\",\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf abgeschlossen.\":\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf promoviert.\",\n","    \"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln.\":\n","    \"Sie ist hoch qualifiziert und hat eine besondere Hingabe zu einem √§sthetischen L√§cheln.\",\n","    \"behandlungsorientierte Zahnberatung\": \"patientenorientierte Beratung\",\n","    \"√§stehthetisches L√§cheln\": \"√§sthetisches L√§cheln\",\n","    \"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\": \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\",\n","    \"Der aktuelle Text zur Zahnspangenpraxis\": \"Der aktuelle Text zur kieferorthop√§dischen Praxis\"\n","}"],"metadata":{"id":"Y_KjsdzFHBpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_error_corrections = {\"Das ist ein neuer Fehler.\": \"Das ist ein korrigierter Fehler.\"}"],"metadata":{"id":"AQJFBU_9-T9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® RAG"],"metadata":{"id":"lNqdhhAit9q9"}},{"cell_type":"code","source":["%%capture\n","import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 1) VectorDB\n","# -------------------------\n","\n","#test_text = seo_json[list(seo_json.keys())[0]][\"SEO\"]\n","\n","class VectorDB:\n","    \"\"\"\n","    Eine Klasse f√ºr alles rund um die Vektordatenbank:\n","    - Aufbauen & Laden (FAISS)\n","    - Neue Eintr√§ge hinzuf√ºgen\n","    - Querying f√ºr Context Retrieval\n","    \"\"\"\n","\n","    def __init__(self, db_folder):\n","        \"\"\"\n","        :param db_folder: Pfad zum Datenbank-Ordner\n","        \"\"\"\n","        self.db_folder = db_folder\n","        self.index_file = os.path.join(db_folder, \"faiss_index.bin\")\n","        self.json_file  = os.path.join(db_folder, \"faiss_index.json\")\n","\n","        self.index = None\n","        self.error_dict = {}\n","\n","        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    def build_index(self, error_corrections: dict):\n","        \"\"\"\n","        Baut einen neuen FAISS-Index aus den √ºbergebenen Fehler-Korrektur-Paaren.\n","        \"\"\"\n","        print(\"üî® Baue neuen FAISS-Index...\")\n","        os.makedirs(self.db_folder, exist_ok=True)\n","\n","        self.error_dict = error_corrections\n","        errors = list(self.error_dict.keys())\n","\n","        # Embeddings\n","        embeddings = np.array([self.model.encode(e) for e in errors], dtype=\"float32\")\n","\n","        # FAISS-Index anlegen\n","        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n","        self.index.add(embeddings)\n","\n","        # Daten auf Festplatte schreiben\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ Neuer Index + JSON in '{self.db_folder}' erstellt.\")\n","\n","    def load_index(self):\n","        \"\"\"\n","        L√§dt einen bereits existierenden FAISS-Index und die Fehler-Daten.\n","        \"\"\"\n","        if not (os.path.exists(self.index_file) and os.path.exists(self.json_file)):\n","            raise FileNotFoundError(\"‚ùå Kein FAISS-Index gefunden. Bitte build_index() aufrufen.\")\n","\n","        print(\"üîé Lade vorhandenen FAISS-Index...\")\n","        self.index = faiss.read_index(self.index_file)\n","\n","        with open(self.json_file, \"r\", encoding=\"utf-8\") as f:\n","            self.error_dict = json.load(f)\n","\n","        print(\"‚úÖ Index & Fehler-Korrekturen geladen.\")\n","\n","    def add_entries(self, new_error_corrections: dict):\n","        \"\"\"\n","        F√ºgt weitere Fehler-Korrektur-Paare hinzu, ohne alles neu zu bauen.\n","        \"\"\"\n","        if self.index is None:\n","            # Versuch zu laden, falls vorhanden\n","            if os.path.exists(self.index_file) and os.path.exists(self.json_file):\n","                self.load_index()\n","            else:\n","                raise FileNotFoundError(\"‚ùå Kein Index vorhanden. Bitte erst build_index() nutzen.\")\n","\n","        # Merge in self.error_dict\n","        for fehler, korrektur in new_error_corrections.items():\n","            self.error_dict[fehler] = korrektur\n","\n","        # embeddings nur f√ºr die neuen keys\n","        new_keys = list(new_error_corrections.keys())\n","        new_embeds = np.array([self.model.encode(k) for k in new_keys], dtype=\"float32\")\n","\n","        # An Index anh√§ngen\n","        self.index.add(new_embeds)\n","\n","        # Speichern\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ {len(new_keys)} neue Eintr√§ge hinzugef√ºgt und Index aktualisiert.\")\n","\n","    def query(self, text: str, top_k=3, threshold=0.6):\n","        \"\"\"\n","        Sucht in der DB nach √§hnlichen fehlerhaften Formulierungen.\n","\n","        :param text: Der zu pr√ºfende Satz/Abschnitt\n","        :param top_k: Anzahl der gesuchten √Ñhnlichkeiten\n","        :param threshold: Distanzschwelle\n","        :return: Liste [(fehler, korrektur), ...]\n","        \"\"\"\n","        if self.index is None:\n","            self.load_index()\n","\n","        embed = np.array([self.model.encode(text)], dtype=\"float32\")\n","        distances, indices = self.index.search(embed, top_k)\n","\n","        all_errors = list(self.error_dict.keys())\n","\n","        results = []\n","        for i in range(top_k):\n","          idx = indices[0][i]\n","          # Sicherstellen, dass idx in den Bereich von all_errors passt\n","          if idx < len(all_errors):\n","              if distances[0][i] < threshold:\n","                  fehler_key = all_errors[idx]\n","                  korrektur = self.error_dict[fehler_key]\n","                  results.append((fehler_key, korrektur))\n","        return results\n","\n","\n","    def retrieve_context(self, seo_text: str) -> str:\n","        \"\"\"\n","        Durchsucht den seo_text Satz f√ºr Satz, holt ggf. Korrekturvorschl√§ge\n","        und baut einen Kontextstring.\n","        \"\"\"\n","        lines = []\n","        for s in seo_text.split(\". \"):\n","            suggestions = self.query(s)\n","            for old, new in suggestions:\n","                lines.append(f\"- Fehler: {old} ‚ûù Verbesserung: {new}\")\n","\n","        if lines:\n","            return \"Bekannte Fehler/Korrekturen:\\n\" + \"\\n\".join(lines)\n","        else:\n","            return \"Keine bekannten Fehler gefunden.\"\n","\n","\n","\n","db = VectorDB(db_folder=FAISS_PATH)\n","db.build_index(error_corrections)\n","db.add_entries(new_error_corrections)\n","#db.retrieve_context(test_text)"],"metadata":{"id":"alRDUdT4I7o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 2) PromptManager\n","# -------------------------\n","\n","class PromptManager:\n","    \"\"\"\n","    L√§dt Prompts aus dem /data/prompts Ordner und kombiniert sie mit\n","    dem Context aus der VectorDB, um einen finalen Prompt zu erstellen.\n","    \"\"\"\n","\n","    def __init__(self, prompts_folder=\"./data/prompts\"):\n","        \"\"\"\n","        :param prompts_folder: Ordner, in dem .json (oder .txt) Prompts liegen\n","        \"\"\"\n","        self.prompts_folder = prompts_folder\n","\n","    def load_prompt(self, filename: str) -> dict:\n","        \"\"\"\n","        L√§dt einen JSON-Prompt aus dem Ordner, z.B. 'grammar_prompt.json'.\n","        \"\"\"\n","        path = os.path.join(self.prompts_folder, filename)\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                return json.load(f)\n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è Prompt-Datei {path} nicht gefunden!\")\n","            return {}\n","        except json.JSONDecodeError:\n","            print(f\"‚ö†Ô∏è Ung√ºltiges JSON in {path}\")\n","            return {}\n","\n","    def build_final_prompt(self, base_prompt_file: str, context: str, user_text: str) -> (str, str):\n","        \"\"\"\n","        Kombiniert:\n","         - base_prompt_file (System-/User-Prompts)\n","         - den 'context' aus der VectorDB\n","         - den 'user_text' (SEO-Text)\n","        und gibt final (system_prompt, user_prompt) zur√ºck.\n","        \"\"\"\n","        prompt_data = self.load_prompt(base_prompt_file)\n","\n","        system_prompt = prompt_data.get(\"system_prompt\", \"\")\n","        user_prompt   = prompt_data.get(\"user_prompt\", \"\")\n","\n","        # Kontext an system_prompt anh√§ngen\n","        system_prompt_full = system_prompt\n","\n","        # SEO-Text an user_prompt anh√§ngen\n","        user_prompt_full = user_prompt.format(context=context,optimized_text=user_text)\n","\n","        return (system_prompt_full, user_prompt_full)\n","\n","# pm = PromptManager(prompts_folder=PROMPT_PATH)\n","# context = db.retrieve_context(test_text)\n","# final_prompts = pm.build_final_prompt(\"grammar_check.json\", context, test_text)"],"metadata":{"id":"-TWN0JGYI7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# from chatbot import Chatbot\n","\n","# -------------------------\n","# 3) SEOGrammarChecker\n","# -------------------------\n","\n","# cb = Chatbot(systemprompt=final_prompts[0], userprompt=final_prompts[1])\n","# final_text = cb.chat()\n","# final_text"],"metadata":{"id":"ahrShDjzI7iz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìü all-in-one solution SEO Texte + html"],"metadata":{"id":"rRMI7Rf5Wsi_"}},{"cell_type":"code","source":["import os\n","import re\n","import json\n","import logging\n","import requests\n","import chardet\n","from bs4 import BeautifulSoup\n","from datetime import datetime\n","\n","from chatbot import Chatbot\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","class SEOPageOptimizer:\n","    \"\"\"\n","    Diese Klasse extrahiert die Texte in Bl√∂cken, l√§sst ChatGPT:\n","     - globale Analyse (analysis_original_text)\n","     - blockweise rewriting (large_text_optimization)\n","     - globale Beschreibung der √Ñnderungen (describe_improvements)\n","    und baut eine final HTML + JSON-Report.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 output_dir=OUTPUT_PATH+\"/final\",\n","                 prompts_file=PROMPT_PATH+\"/seo_prompts_2.json\",\n","                 google_ads_keywords=\"\"\n","    ):\n","        self.output_dir = output_dir\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        self.prompts = self.load_prompts(prompts_file)\n","        self.google_ads_keywords = keywords_final\n","\n","    def load_prompts(self, prompts_file):\n","        logging.info(f\"Lade Prompts aus Datei: {prompts_file}\")\n","        if not os.path.exists(prompts_file):\n","            logging.warning(f\"Prompt-Datei {prompts_file} nicht gefunden! R√ºckgabe leeres Dict.\")\n","            return {}\n","        with open(prompts_file, \"r\", encoding=\"utf-8\") as f:\n","            return json.load(f)\n","\n","    def fetch_html(self, url: str) -> str:\n","        logging.info(f\"Lade HTML (raw) von {url}...\")\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()\n","\n","        raw_data = response.content\n","        detected = chardet.detect(raw_data)\n","        encoding = \"utf-8\"\n","        logging.info(f\"chardet sagt: {encoding} (Confidence: {detected['confidence']})\")\n","\n","        # Nur eine Dekodierung\n","        text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","        logging.info(\"HTML erfolgreich geladen und manuell dekodiert.\")\n","        return text_data\n","\n","    def extract_dom_texts(self, html: str):\n","        soup = BeautifulSoup(html, \"html.parser\")\n","        title_tag = soup.find(\"title\")\n","        meta_desc_tag = soup.find(\"meta\", attrs={\"name\":\"description\"})\n","        title_text = title_tag.get_text(strip=True) if title_tag else \"\"\n","        meta_desc_text = meta_desc_tag.get(\"content\", \"\").strip() if meta_desc_tag else \"\"\n","\n","        text_blocks = []\n","        block_id_num = 1\n","\n","        # Body\n","        for tag_name in [\"p\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"li\"]:\n","            for elem in soup.find_all(tag_name):\n","                txt = elem.get_text(separator=\" \", strip=True)\n","                if txt:\n","                    block_id = f\"BLOCK_{block_id_num:03d}\"\n","                    text_blocks.append({\n","                        \"id\": block_id,\n","                        \"elem\": elem,\n","                        \"source\": \"body\",\n","                        \"original_text\": txt,\n","                        \"optimized_text\": None\n","                    })\n","                    block_id_num += 1\n","\n","        # Link-Attribute\n","        for a_tag in soup.find_all(\"a\"):\n","            if a_tag.has_attr(\"title\"):\n","                txt = a_tag[\"title\"].strip()\n","                if txt:\n","                    block_id = f\"BLOCK_{block_id_num:03d}\"\n","                    text_blocks.append({\n","                        \"id\": block_id,\n","                        \"elem\": a_tag,\n","                        \"source\": \"attribute\",\n","                        \"attr_name\": \"title\",\n","                        \"original_text\": txt,\n","                        \"optimized_text\": None\n","                    })\n","                    block_id_num += 1\n","\n","        # Accordion-Attribute\n","        for div_tag in soup.find_all(attrs={\"data-accordion-title\": True}):\n","            txt = div_tag[\"data-accordion-title\"].strip()\n","            if txt:\n","                block_id = f\"BLOCK_{block_id_num:03d}\"\n","                text_blocks.append({\n","                    \"id\": block_id,\n","                    \"elem\": div_tag,\n","                    \"source\": \"attribute\",\n","                    \"attr_name\": \"data-accordion-title\",\n","                    \"original_text\": txt,\n","                    \"optimized_text\": None\n","                })\n","                block_id_num += 1\n","\n","        logging.info(f\"Blocks gesamt: {len(text_blocks)}\")\n","        return text_blocks, title_text, meta_desc_text, soup\n","\n","    def build_big_string(self, text_blocks):\n","        \"\"\"\n","        Baut 1 gro√üen String => Original-Bl√∂cke\n","        \"\"\"\n","        lines = []\n","        for b in text_blocks:\n","            lines.append(f\"---{b['id']}_START---\")\n","            lines.append(b[\"original_text\"])\n","            lines.append(f\"---{b['id']}_END---\")\n","        return \"\\n\".join(lines)\n","\n","    def build_big_string_optimized(self, text_blocks):\n","        \"\"\"\n","        Dasselbe, aber mit optimized_text\n","        \"\"\"\n","        lines = []\n","        for b in text_blocks:\n","            text = b[\"optimized_text\"] or \"(keine Optimierung)\"\n","            lines.append(f\"---{b['id']}_START---\")\n","            lines.append(text)\n","            lines.append(f\"---{b['id']}_END---\")\n","        return \"\\n\".join(lines)\n","\n","    def generate_llm_text(self, agent_key, **kwargs):\n","        if agent_key not in self.prompts:\n","            logging.warning(f\"Agent-Key '{agent_key}' nicht in JSON. Return ''\")\n","            return \"\"\n","        system_prompt = self.prompts[agent_key].get(\"system_prompt\",\"\")\n","        user_template = self.prompts[agent_key].get(\"user_prompt_template\",\"\")\n","        if not user_template:\n","            logging.warning(f\"Kein 'user_prompt_template' f√ºr agent_key '{agent_key}'. Return leer.\")\n","            return \"\"\n","\n","        user_prompt = user_template.format(**kwargs)\n","        cb = Chatbot(systemprompt=system_prompt, userprompt=user_prompt)\n","        response = cb.chat()\n","        return response.strip()\n","\n","    def parse_llm_response(self, llm_text):\n","        pattern = re.compile(r'---(BLOCK_\\d+)_START---\\s*(.*?)\\s*---\\1_END---', re.DOTALL)\n","        found = pattern.findall(llm_text)\n","        return found\n","\n","    # ================== 1) Globale Analyse ==================\n","    def do_pre_analysis(self, text_blocks):\n","        full_text = self.build_big_string(text_blocks)\n","        analysis = self.generate_llm_text(\"analysis_original_text\", full_text=full_text, google_ads_keywords=google_ads_keywords)\n","        return analysis\n","\n","    # ================== 2) SEO-Optimierung ==================\n","    def do_large_text_optimization(self, text_blocks):\n","        full_text = self.build_big_string(text_blocks)\n","        llm_out = self.generate_llm_text(\n","            \"large_text_optimization\",\n","            full_text=full_text,\n","            google_ads_keywords=self.google_ads_keywords\n","        )\n","        results = self.parse_llm_response(llm_out)\n","        block_map = { b[\"id\"]: b for b in text_blocks }\n","        for b_id, new_txt in results:\n","            block_map[b_id][\"optimized_text\"] = new_txt.strip()\n","        return text_blocks\n","\n","    # ================== 3) Globale Beschreibung =============\n","    def describe_improvements_global(self, text_blocks):\n","        \"\"\"\n","        Original => build_big_string\n","        Neu => build_big_string_optimized\n","        => 'describe_improvements' => 1 Gesamter Bericht\n","        \"\"\"\n","        original_str = self.build_big_string(text_blocks)\n","        optimized_str = self.build_big_string_optimized(text_blocks)\n","\n","        desc = self.generate_llm_text(\n","            \"describe_improvements\",\n","            original_text=original_str,\n","            optimized_text=optimized_str\n","        )\n","        return desc\n","\n","    # ================== 4) Re-Inject in HTML ================\n","    def inject_content(self, soup, text_blocks, new_title=None, new_desc=None):\n","        for b in text_blocks:\n","            opt_txt = b[\"optimized_text\"]\n","            if not opt_txt:\n","                continue\n","            opt_txt_html = opt_txt.replace('\\n', '<br/>')\n","            elem = b[\"elem\"]\n","            if b[\"source\"] == \"body\":\n","                elem.clear()\n","                elem.append(opt_txt)\n","            elif b[\"source\"] == \"attribute\":\n","                attr_name = b[\"attr_name\"]\n","                elem[attr_name] = opt_txt\n","\n","        # Falls Title / Desc\n","        if new_title:\n","            title_tag = soup.find(\"title\")\n","            if not title_tag:\n","                head = soup.find(\"head\")\n","                if head:\n","                    title_tag = soup.new_tag(\"title\")\n","                    head.append(title_tag)\n","            if title_tag:\n","                title_tag.string = new_title\n","\n","        if new_desc:\n","            desc_tag = soup.find(\"meta\", attrs={\"name\":\"description\"})\n","            if not desc_tag:\n","                head = soup.find(\"head\")\n","                if head:\n","                    new_meta = soup.new_tag(\"meta\", attrs={\"name\":\"description\", \"content\": new_desc})\n","                    head.append(new_meta)\n","            else:\n","                desc_tag[\"content\"] = new_desc\n","\n","        # charset\n","        head = soup.find(\"head\")\n","        if head and not head.find(\"meta\", attrs={\"charset\": True}):\n","            meta_charset = soup.new_tag(\"meta\", charset=\"utf-8\")\n","            head.insert(0, meta_charset)\n","\n","        return soup\n","\n","    def save_html(self, soup, filepath):\n","        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","            f.write(str(soup))\n","        logging.info(f\"‚úÖ Datei gespeichert: {filepath}\")\n","\n","    # ================== 5) Hauptworkflow ====================\n","    def optimize_page(self, url: str, outfile: str):\n","        logging.info(f\"SEO-Optimierung startet => {url} => {outfile}\")\n","\n","        # 1) HTML + extrahieren\n","        html = self.fetch_html(url)\n","        blocks, old_title, old_desc, soup = self.extract_dom_texts(html)\n","\n","        # 2) Analyse Original\n","        analysis_text = self.do_pre_analysis(blocks)\n","\n","        # 3) SEO => block rewriting\n","        blocks = self.do_large_text_optimization(blocks)\n","\n","        # 4) Globale Beschreibung\n","        improvement_desc = self.describe_improvements_global(blocks)\n","\n","        # 5) Title & Desc => optional\n","        new_title = old_title\n","        new_desc  = old_desc\n","\n","        # 6) Re-Inject\n","        new_soup = self.inject_content(soup, blocks, new_title, new_desc)\n","        self.save_html(new_soup, outfile)\n","\n","        # 7) combined report\n","        combined_data = {\n","            \"analysis_of_original_text\": analysis_text,\n","            \"improvement_description\": improvement_desc,\n","            \"blocks\": []\n","        }\n","        for b in blocks:\n","            combined_data[\"blocks\"].append({\n","                \"id\": b[\"id\"],\n","                \"original_text\": b[\"original_text\"],\n","                \"optimized_text\": b[\"optimized_text\"]\n","            })\n","\n","        # Hier: Aus url einen kurzen Dateinamen bauen:\n","        # z.B. alle Slashes entfernen, oder du kannst parse_url\n","        sanitized_url = url.replace(\"/\", \"\")\n","        report_filename = f\"report_{sanitized_url}.json\"\n","\n","        json_report = os.path.join(self.output_dir, report_filename)\n","        with open(json_report, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(combined_data, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f\"Fertig! => {outfile}\\nReport => {json_report}\")\n","\n"],"metadata":{"id":"p_5-vxhH3LIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for url in list(original_texts.keys()):\n","    optimizer = SEOPageOptimizer(\n","        output_dir=OUTPUT_PATH + \"/final\",\n","        prompts_file=PROMPT_PATH + \"/seo_prompts_2.json\",\n","        google_ads_keywords=keywords_final\n","    )\n","\n","    url_to_optimize = url\n","\n","    # HTML-Filename wird z. B.\n","    html_filename = str(url).replace('/', '') + \"_optimized.html\"\n","    outfile = os.path.join(OUTPUT_PATH, \"final\", html_filename)\n","\n","    optimizer.optimize_page(url_to_optimize, outfile)\n","\n","    print(\"Done! HTML at:\", outfile)\n","    # => Jedem URL entspricht nun auch ein passender report_XXX.json\n"],"metadata":{"id":"5eqLQUME0ozZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","\n","def merge_reports_for_urls(url_list, output_dir=OUTPUT_PATH+\"/final\", final_merged=\"final_merged.json\"):\n","    \"\"\"\n","    - Liest f√ºr jede URL die jeweilige report_{url_sanitized}.json\n","    - Fasst analysis/improvement/Bloecke zusammen\n","    - Schreibt am Ende ein gro√ües JSON in `final_merged`\n","    \"\"\"\n","    # Dieses Dictionary wird alle zusammengefassten Infos enthalten\n","    big_merged_data = {}\n","\n","    for url in url_list:\n","        # Erzeuge den Dateinamen analog zur Optimierung\n","        # z. B.  \"report_https:example.compage.html.json\"\n","        sanitized_url = url.replace(\"/\", \"\")\n","        report_filename = f\"report_{sanitized_url}.json\"\n","        report_path = os.path.join(output_dir, report_filename)\n","\n","        # Pr√ºfen, ob Datei existiert\n","        if not os.path.isfile(report_path):\n","            print(f\"‚ö†Ô∏è Report-Datei fehlt: {report_path}\")\n","            continue\n","\n","        # JSON lesen\n","        with open(report_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","\n","        # analysis_of_original_text & improvement_description\n","        analysis = data.get(\"analysis_of_original_text\", \"\")\n","        improvement = data.get(\"improvement_description\", \"\")\n","\n","        # Blocks => original_text + optimized_text kumulieren\n","        blocks = data.get(\"blocks\", [])\n","        original_texts = []\n","        optimized_texts = []\n","\n","        for block in blocks:\n","            original_texts.append(block.get(\"original_text\",\"\"))\n","            optimized_texts.append(block.get(\"optimized_text\",\"\"))\n","\n","        # Zusammenfassen in je einen gro√üen String\n","        merged_original = \"\\n\".join(original_texts)\n","        merged_optimized = \"\\n\".join(optimized_texts)\n","\n","        # In big_merged_data ablegen\n","        big_merged_data[url] = {\n","            \"analysis\": analysis,\n","            \"improvement\": improvement,\n","            \"original_text\": merged_original,\n","            \"optimized_text\": merged_optimized\n","        }\n","\n","    # Alles als ein JSON speichern\n","    final_json_path = os.path.join(output_dir, final_merged)\n","    with open(final_json_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(big_merged_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"‚úÖ Zusammenfassung erstellt: {final_json_path}\")\n","\n","\n","# Beispiel-Aufruf:\n","if __name__ == \"__main__\":\n","    # Angenommen, hier deine URLs\n","    urls_to_process = list(original_texts.keys())\n","\n","    merge_reports_for_urls(urls_to_process, output_dir=OUTPUT_PATH+\"/final\", final_merged=\"allinone.json\")\n"],"metadata":{"id":"syYldS-NGSJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import pypandoc\n","\n","def create_html_report(merged_json_path, out_html=\"seo_report.html\", out_docx=\"seo_report.docx\"):\n","    \"\"\"\n","    Liest das zusammengefasste JSON (alle URLs, Analyse, Verbesserungen, Original & Optimiert)\n","    und erzeugt daraus ein HTML-Dokument. Anschlie√üend konvertiert es das HTML in eine DOCX-Datei.\n","    \"\"\"\n","\n","    # JSON laden\n","    with open(merged_json_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","        # => data = {\n","        #    \"https://example.com\" : {\n","        #       \"analysis\": \"...\",\n","        #       \"improvement\": \"...\",\n","        #       \"original_text\": \"...\",\n","        #       \"optimized_text\": \"...\",\n","        #    }, ...\n","        # }\n","\n","    # HTML Template zusammenbauen\n","    # Du kannst hier inline CSS nutzen oder <link> mit externem Stylesheet\n","    html_head = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","      <meta charset=\"UTF-8\"/>\n","      <title>SEO Zusammenfassung</title>\n","      <style>\n","        body {\n","          font-family: \"Helvetica Neue\", Arial, sans-serif;\n","          margin: 20px;\n","          line-height: 1.6;\n","          color: #333;\n","          background: #f2f2f2;\n","        }\n","        h1 {\n","          text-align: center;\n","          background: #007BFF;\n","          color: #fff;\n","          padding: 10px;\n","          border-radius: 4px;\n","        }\n","        .url-block {\n","          background: #fff;\n","          border-radius: 5px;\n","          padding: 20px;\n","          margin-bottom: 20px;\n","          box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n","        }\n","        .url-block h2 {\n","          margin-top: 0;\n","          color: #007BFF;\n","        }\n","        .subtitle {\n","          font-weight: bold;\n","          color: #555;\n","          margin-top: 1rem;\n","        }\n","        .text-box {\n","          background: #fafafa;\n","          border-left: 4px solid #007BFF;\n","          padding: 10px;\n","          margin: 10px 0;\n","          white-space: pre-wrap;  /* Damit \\n Zeilenumbr√ºche anzeigt */\n","        }\n","      </style>\n","    </head>\n","    <body>\n","      <h1>SEO Report &amp; Verbesserungs-√úbersicht</h1>\n","    \"\"\"\n","\n","    html_body = []\n","\n","    # F√ºr jede URL Sektion erstellen\n","    for idx, (url, content) in enumerate(data.items(), start=1):\n","        analysis = content.get(\"analysis\", \"\")\n","        improvement = content.get(\"improvement\", \"\")\n","        original_text = content.get(\"original_text\", \"\")\n","        optimized_text = content.get(\"optimized_text\", \"\")\n","\n","        section_html = f\"\"\"\n","        <div class=\"url-block\">\n","          <h2>{idx}. {url}</h2>\n","\n","          <div class=\"subtitle\">Analyse des Originaltexts:</div>\n","          <div class=\"text-box\">{analysis}</div>\n","\n","          <div class=\"subtitle\">Erkl√§rung der Verbesserungen:</div>\n","          <div class=\"text-box\">{improvement}</div>\n","        </div>\n","        \"\"\"\n","        html_body.append(section_html)\n","\n","    html_end = \"\"\"\n","    </body>\n","    </html>\n","    \"\"\"\n","\n","    # Finale HTML zusammensetzen\n","    final_html = html_head + \"\\n\".join(html_body) + html_end\n","\n","    # Speichern als HTML\n","    with open(out_html, \"w\", encoding=\"utf-8\") as f:\n","        f.write(final_html)\n","\n","    print(f\"‚úÖ HTML-Report erstellt: {out_html}\")\n","\n","    # Mit pypandoc in DOCX konvertieren\n","    # Daf√ºr muss pypandoc & pandoc installiert sein\n","    pypandoc.convert_file(\n","        source_file=out_html,\n","        to=\"docx\",\n","        outputfile=out_docx,\n","        extra_args=[\"--standalone\"]\n","    )\n","    print(f\"‚úÖ DOCX exportiert: {out_docx}\")\n","\n","\n","if __name__ == \"__main__\":\n","    merged_json = OUTPUT_PATH + \"/final/allinone.json\"  # Pfad zu deinem zusammengefassten JSON\n","    out_html = OUTPUT_PATH + \"/final/seo_report.html\"\n","    out_docx = OUTPUT_PATH + \"/final/seo_report.docx\"\n","\n","    create_html_report(merged_json, out_html, out_docx)\n"],"metadata":{"id":"I1SsHYtpkAWa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîé SEO Analyses + Statistics"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["# k√ºnstliche SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"],"metadata":{"id":"1lkqnkWkw5wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["json_path = OUTPUT_PATH + \"/final/allinone.json\"\n","with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","    seo_json = json.load(f)"],"metadata":{"id":"OibUqk7zvd5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","exclude_list = [\"leila\", \"graf\", \"koenig\", \"bjoern\", \"k√∂nig\", 'bj√∂rn', 'adolf', 'schmidt', 'strasse', 'stra√üe', 'tilo', 'remhof']\n","\n","keywords_final = google_ads_keywords\n","\n","seo_analyzer = SEOAnalyzer(\n","    seo_json=seo_json,\n","    keywords_final=keywords_final,\n","    output_dir=OUTPUT_PATH+\"/images\",\n","    historical_data=historical_data,\n","    wordcloud_exclude=exclude_list,\n","    shared_image_dict=FINAL_IMAGES\n",")\n","\n","analysis_paths = seo_analyzer.run_analysis()\n","print(\"Analysis Plot Pfade:\", analysis_paths)\n","\n","model_paths = seo_analyzer.run_models()\n","print(\"Model Plot Pfade:\", model_paths)\n","\n","all_paths = seo_analyzer.get_all_image_paths()\n","print(\"Alle Pfade gesammelt:\", all_paths)\n"],"metadata":{"id":"MtZrcL_wxKzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ü•ºLab"],"metadata":{"id":"MM7uu--42W4u"}}]}