{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["EnjP4Zhat_8t","xaxxhKAgyeQq","JtMpu3BMyh6-","Pns8QF8VWrHR","o38bsnD-EWcq","vNVT-lr5jejz","cgPAMHODtAuZ","SJVtNtjnqvJV","BatTg4ZKvpZV","AeTy1z3vxblD","1zqvsEJvx33W","4v6-jGMeO9Zt","HraJ3_5SJHSv","hdqvYkNsg7fx","dr3XCOHn7Nc_","tCeeWN-UjXaI","lNqdhhAit9q9","nb1dof25T_TI"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyP66JYhUhc3845/iF88W0W2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìî READ_ME und TO_DO"],"metadata":{"id":"EnjP4Zhat_8t"}},{"cell_type":"markdown","source":["#### README"],"metadata":{"id":"xaxxhKAgyeQq"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/README.md'\n","# üöÄ SEO Automation Pipeline mit OpenAI & Retrieval (RAG)\n","\n","Dieses Projekt bietet eine **komplette End-to-End-Pipeline f√ºr die SEO-Optimierung von Websites**, inklusive **Web-Scraping, SEO-Analyse, KI-gest√ºtzter Text-Optimierung und Qualit√§tskontrolle**.\n","\n","Kern des Projekts sind **automatisierte Abl√§ufe**, die von der **Datengewinnung bis zur SEO-optimierten Textgenerierung** reichen.\n","Mithilfe von **OpenAI (ChatGPT)** und einer **Retrieval Augmented Generation (RAG)-Architektur** wird sichergestellt, dass die finalen Texte nicht nur **SEO-freundlich**, sondern auch **grammatikalisch korrekt und hochwertig** sind.\n","\n","## üìö Inhaltsverzeichnis\n","\n","- Features\n","- Projektstruktur\n","- Ablauf & Module\n","- Technologien\n","- Installation\n","- Nutzung\n","- Ziele\n","- Roadmap\n","\n","## ‚úÖ Features\n","\n","- üåê **Automatisiertes Web Scraping** (inkl. Filter f√ºr relevante Inhalte)\n","- ‚úçÔ∏è **Generierung von SEO-optimierten Texten** mithilfe der OpenAI API\n","- üß† **RAG-gest√ºtzte Fehlererkennung & Textkorrektur** mit Vektordatenbank (FAISS)\n","- üìä **Analyse der Optimierungsergebnisse** (Statistiken, √Ñhnlichkeiten, Visualisierungen)\n","- üìà **Keyword-Analyse und Keyword-Optimierung**\n","- üì¶ Ausgabe in **HTML und PDF** f√ºr Kunden\n","- üìä Umfangreiche **Datenvisualisierungen** (Wordclouds, Cosine Similarity, Keyword-Verteilung)\n","\n","## üóÇÔ∏è Projektstruktur\n","\n","```\n","SEO-Project/\n","‚îú‚îÄ‚îÄ data/                # Prompts, Fehler-Korrektur-Daten, weitere JSON Dateien\n","‚îú‚îÄ‚îÄ notebooks/           # Colab/Notebooks zum Starten und Entwickeln\n","‚îú‚îÄ‚îÄ src/                # Source Code (Python-Klassen und Module)\n","‚îÇ   ‚îú‚îÄ‚îÄ webscraper.py    # Webscraping und Text-Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ llm_processor.py # Anbindung an OpenAI API, Keyword Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py       # Zentrale Chatbot-Klasse zur Kommunikation mit GPT\n","‚îÇ   ‚îú‚îÄ‚îÄ seoanalyzer.py   # Analyse und Auswertung der Texte\n","‚îÇ   ‚îú‚îÄ‚îÄ github.py        # Automatischer Upload ins GitHub Repo\n","‚îÇ   ‚îú‚îÄ‚îÄ rag_checker.py   # RAG-Modul f√ºr Fehlerkorrektur via FAISS\n","‚îÇ   ‚îú‚îÄ‚îÄ utils.py         # Hilfsmodule (z.B. f√ºr Prompt-Management)\n","‚îÇ   ‚îî‚îÄ‚îÄ embedding_demo.py# 3D Embedding- und Cosine Similarity Visualisierungen\n","‚îî‚îÄ‚îÄ requirements.txt    # Python-Abh√§ngigkeiten\n","```\n","\n","## ‚öôÔ∏è Ablauf & Module\n","\n","### 1. **Web Scraping**\n","- **src/webscraper.py**: Holt Inhalte von Webseiten, filtert irrelevante Seiten (z.B. Impressum, AGB).\n","\n","### 2. **SEO-Optimierung mit OpenAI**\n","- **src/llmprocessor.py**:\n","  - Extrahiert Keywords aus den Inhalten.\n","  - Optimiert die Texte f√ºr SEO mit gezielten Prompts.\n","\n","### 3. **Analyse & Visualisierung**\n","- **src/seoanalyzer.py**: Verarbeitet und analysiert die Original- und optimierten Texte.\n","\n","### 4. **GitHub Automation**\n","- **src/github.py**: L√§dt finale Ergebnisse in ein GitHub-Repo hoch.\n","\n","## üß∞ Technologien\n","\n","| Technologie                  | Beschreibung                                       |\n","|-----------------------------|---------------------------------------------------|\n","| Python                      | Hauptsprache                                       |\n","| OpenAI API (ChatGPT, GPT-4)  | Generative KI f√ºr SEO-Texte                       |\n","| FAISS                      | Vektorsuche f√ºr RAG und Text-Fehler                |\n","| Pandas, NumPy               | Datenanalyse und Verarbeitung                      |\n","| Matplotlib, Seaborn         | Visualisierungen                                   |\n","| Sentence Transformers       | Embedding-Erstellung f√ºr Vektordatenbank          |\n","| BeautifulSoup, Requests     | Webscraping                                        |\n","| Google Colab                | Entwicklung und Ausf√ºhrung                        |\n","\n","## üöÄ Installation\n","\n","```bash\n","pip install -r requirements.txt\n","python -m spacy download de_core_news_sm\n","pip install faiss-cpu sentence-transformers openai wordcloud matplotlib seaborn\n","```\n","\n","## üíª Nutzung\n","\n","```python\n","scraper = WebsiteScraper(start_url=\"https://www.example.com\")\n","scraper.scrape_website()\n","\n","llm_processor = LLMProcessor(PROJECT_ROOT, scraper.get_scraped_data())\n","llm_processor.run_all()\n","\n","seo_checker = SEORAGChecker(quality_checker, chatbot_system_prompt)\n","final_text = seo_checker.check_text(optimized_text)\n","\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n","```\n","\n","## üéØ Ziele\n","\n","- ‚úÖ Vollst√§ndige Automatisierung der SEO-Optimierung\n","- ‚úÖ RAG f√ºr sprachliche Qualit√§tskontrolle\n","- ‚úÖ Kundenfertige PDF/HTML-Reports\n","\n","## üöß Roadmap\n","\n","- [ ] Automatische SEO Scores (z.B. Google Ads API)\n","- [ ] Automatische Keyword-Erweiterung\n","- [ ] Mehrsprachigkeit (aktuell Deutsch)\n","- [ ] WordPress-Integration\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"443XYilDN9NA","executionInfo":{"status":"ok","timestamp":1742371708562,"user_tz":-60,"elapsed":355,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"a8190647-e5b7-4745-9715-cdb898589c89"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/README.md\n"]}]},{"cell_type":"markdown","source":["####TODO"],"metadata":{"id":"JtMpu3BMyh6-"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/TODO.md'\n","# To-Do Liste: SEO Automation & KI-Projekt\n","\n","Diese Liste fasst alle anstehenden Aufgaben im Projekt zusammen\n","\n","---\n","\n","## 0. **Aktuelles und dringendes**\n","- [ ] 18.3.2025 **Version missmatch**: numpy 2.2.3 und pandas 2.2.4 **side effects on**: dependencies.py, excelimporter.py, Installation.ipynb\n","\n","---\n","\n","## 1. **Allgemeine Projektorganisation**\n","- [ ] **Projektstruktur verbessern**: Ordner √ºbersichtlich gestalten (z.B. `src/`, `data/`, `tests/`, `notebooks/`, dependencies.py).\n","- [ ] **Dokumentation erweitern**: READ_ME und Wiki (bzw. GitHub Pages) zu jedem Modul anlegen.\n","- [ ] **Automatisierte Tests** Pytest f√ºr Kernfunktionen ausbauen.\n","- [ ] **Produkt f√ºr Kunden finalisieren**\n","- [ ] **FAISS DB**: automatisierte Erweiterung bei neu gefundenen Fehlern\n","- [ ] **Template GitHub**: issues\n","- [ ] Funktionalit√§ten aus **utils.py** √ºberdenken\n","- [ ] langfristig Umstieg auf **langchain**\n","- [ ] textprocessor durch openai **function calling** ersetzen\n","- [ ] **dependencies** und versionen robuster machen\n","\n","---\n","\n","## 2. **Vector-Datenbank (FAISS) & Retrieval**\n","- [ ] **VectorDB-Klasse finalisieren**:\n","  - [ ] Kleinere Bugs beheben\n","  - [ ] Userfreundliche Methoden f√ºr neue Eintr√§ge\n","- [ ] **Einrichtung der DB** bei Projektstart (Neubau vs. Laden) vereinheitlichen\n","- [ ] **Konfigurierbare √Ñhnlichkeits-Schwelle** (z.B. `threshold=0.6`) besser dokumentieren\n","- [ ] **Dynamische Filter** f√ºr bestimmte Fehlerkategorien (z.B. Stil vs. Grammatik) √ºberlegen\n","\n","---\n","\n","## 3. **SEO-Optimierungs-Pipeline (LangChain)**\n","- [ ] **LangChain-Workflow** debuggen und lauff√§hig machen\n","  - [ ] Placeholder & Prompt-Mapping f√ºr `format_messages(...)` beheben\n","  - [ ] `.role` vs. `_role`-Konflikt l√∂sen (Debug-Statements anpassen)\n","- [ ] **Prompts in JSON-Dateien** verlagern (z.B. `/data/prompts/`) und sauber verlinken\n","- [ ] **Google Ads Keywords** Integration:\n","  - [ ] Nur LLM-Keyword-Extraktion aufrufen, wenn keine Google-Keywords vorliegen\n","- [ ] **Supervisor-Feedback** integrieren (optional) & QA-Schritte definieren\n","\n","---\n","\n","## 4. **SEOGrammarChecker & PromptManager**\n","- [ ] Klassenrefactoring:\n","  - [ ] **`VectorDB`** vs. **`PromptManager`** vs. **`SEOGrammarChecker`** sauber trennen\n","  - [ ] M√∂glichst wenig Code-Duplikate, mehr modulare Testbarkeit\n","- [ ] **Konfigurationsdatei** (z.B. YAML) f√ºr Pfade, wie `FAISS_PATH` & Promptordner\n","- [ ] **Erweiterbare Prompt-Templates**:\n","  - [ ] Z.B. `seo_optimization.json`, `grammar_check.json`, `supervisor.json`, etc.\n","\n","---\n","\n","## 5. **Abschluss & Integration**\n","- [ ] **Dokumentation** aller Pipelines & Klassen in der README (oder in separater Doku)\n","- [ ] **Optionale WordPress-Integration** in der Zukunft (Ideenspeicher)\n","  - [ ] Upload via REST API\n","  - [ ] Metadaten (Title, Slug, Tags etc.)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBtOaSe-CHyO","executionInfo":{"status":"ok","timestamp":1742371708868,"user_tz":-60,"elapsed":307,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"1418132a-077c-42eb-86da-afbb32fcf681"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/TODO.md\n"]}]},{"cell_type":"markdown","source":["#üåãHYPERPARAMETER\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","\n","SRC_PATH, DATA_PATH, TEST_PATH = PROJECT_ROOT + \"/src\", PROJECT_ROOT + \"/data\", PROJECT_ROOT + '/tests'\n","FAISS_PATH = DATA_PATH + '/faiss_db'\n","KEYWORD_PATH = DATA_PATH + '/keywords'\n","PROMPT_PATH = DATA_PATH + '/prompts'\n","\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","# START_URL = 'https://www.malerarbeiten-koenig.de/'\n","EXCLUDED_WEBSITES = [\"impressum\", \"datenschutz\", \"datenschutzerkl√§rung\", \"agb\"]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1742371733643,"user_tz":-60,"elapsed":24774,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"80a8c8ca-109f-4fae-905e-bfd5b0d70b52"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements + dependencies"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["import importlib\n","import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"],"metadata":{"id":"do4Bf0uw-y8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_WEBSITES)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì∫ google ads seo keywords"],"metadata":{"id":"eHxAg4fuIc2T"}},{"cell_type":"code","source":["import excelimporter\n","importlib.reload(excelimporter)\n","from excelimporter import ExcelImporter\n","\n","importer = ExcelImporter(\n","    project_folder=PROJECT_ROOT,\n","    header=2\n",")\n","keyword_df = importer.import_all()\n","\n","excluded_seo_keywords = ['spange de', 'kfo zentrum essen', 'gerade zahne', 'zahn spange']\n","\n","keyword_df = keyword_df[(keyword_df['Avg. monthly searches'] > 10) &\n"," (~keyword_df['Keyword'].isin(excluded_seo_keywords))\n"," ].sort_values(by='Avg. monthly searches', ascending=False).reset_index(drop=True).copy()\n","\n","google_ads_keywords = list(keyword_df['Keyword'])"],"metadata":{"id":"REeveZ8rwEys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆwebtext analysis + SEO"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["import llmprocessor\n","importlib.reload(llmprocessor)\n","from llmprocessor import LLMProcessor\n","\n","llm_processor = LLMProcessor(PROMPT_PATH, original_texts, google_ads_keywords=google_ads_keywords)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"q3isJXZoebv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìÅ Textprozessor"],"metadata":{"id":"SJVtNtjnqvJV"}},{"cell_type":"code","source":["import json\n","import textprocessor\n","importlib.reload(textprocessor)\n","from textprocessor import TextProcessor\n","\n","# JSON mit den SEO-Abschnitten extrahieren\n","json_output = TextProcessor.extract_sections_to_json(list(optimized_texts.keys()), list(optimized_texts.values()))\n","seo_json = json.loads(json_output)\n","\n","# texte bereinigen und hinzuf√ºgen\n","seo_json = TextProcessor.add_cleaned_text(seo_json, original_texts)\n","\n","# Ergebnis anzeigen\n","print(json.dumps(seo_json, indent=4, ensure_ascii=False))\n"],"metadata":{"id":"SS1zb2sAWKIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîé SEO Analyses + Statistics"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["llm_processor.get_keywords()['keywords_final']"],"metadata":{"id":"_NV0yuINglEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","keywords_final = json.loads(llm_processor.get_keywords()['keywords_final']) if not google_ads_keywords else google_ads_keywords\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n"],"metadata":{"id":"MtZrcL_wxKzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂‚óÄ conversion rates, modelierungen"],"metadata":{"id":"AeTy1z3vxblD"}},{"cell_type":"code","source":["# Historische SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"],"metadata":{"id":"1lkqnkWkw5wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final, historical_data)\n","seo_analyzer.run_models()"],"metadata":{"id":"ob_ct0NfVagR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üõè embedding demo"],"metadata":{"id":"1zqvsEJvx33W"}},{"cell_type":"code","source":["import embeddingdemo\n","importlib.reload(embeddingdemo)\n","from embeddingdemo import EmbeddingDemo\n","\n","demo = EmbeddingDemo()\n","demo.run_all_visualizations()"],"metadata":{"id":"toL_RwWuO72N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["%%capture\n","from jinja2 import Template\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout f√ºr \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox f√ºr zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um Zeilenumbr√ºche aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und Erkl√§rung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">{{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def json_to_html(json_data):\n","    # HTML-Template mit EINER Spalte f√ºr \"SEO\" (die \"alt\"-Spalte entf√§llt)\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            /* Zeilenumbr√ºche aus dem JSON in <br> wandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- \"Analyse\" normal untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">\n","                {{ sections.Analyse | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- \"Erkl√§rung\" normal untereinander -->\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">\n","                {{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- NUR noch die \"SEO\"-Spalte -->\n","            <div class=\"column\">\n","                <p class=\"header\">SEO</p>\n","                <p class=\"content\">\n","                    {{ sections.SEO | replace('\\\\n','<br>') | safe }}\n","                </p>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    template = Template(html_template)\n","    return template.render(data=json_data)\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"Cx6qfyVfJzI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import asyncio\n","from playwright.async_api import async_playwright\n","\n","async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschlie√üend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, R√§nder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pypandoc\n","\n","input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• Tests"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"markdown","source":["#### üìÉ Doku"],"metadata":{"id":"hdqvYkNsg7fx"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/tests/DOKU_TESTS.md'\n","# ‚úÖ Pytest Template: Chatbot Klasse\n","\n","## 1. Klassen & Methoden die getestet werden sollen\n","\n","- **Chatbot**\n","  - `chat()`\n","  - `chat_with_streaming()`\n","\n","---\n","\n","## 2. Beispielhafte Inputs + erwartete Outputs pro Methode\n","\n","| Methode                      | Beispiel Input                                                           | Erwartete Ausgabe    |\n","|-----------------------------|-------------------------------------------------------------------------|----------------------|\n","| `Chatbot.chat()`             | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'           | \"Test erfolgreich\"   |\n","| `Chatbot.chat_with_streaming()` | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'         | \"Test erfolgreich\"   |\n","\n","---\n","\n","## 3. Return-Typen der Methoden\n","\n","| Methode                      | R√ºckgabe-Typ |\n","|-----------------------------|--------------|\n","| `Chatbot.chat()`             | `str`        |\n","| `Chatbot.chat_with_streaming()` | `str`     |\n","\n","---\n","\n","## 4. Externe Services mocken?\n","\n","| Service         |  Mocken?                           |\n","|-----------------|-------------------------------------|\n","| OpenAI API      |  Nein                                |\n","| FAISS Index     |  Ja (kleine Test-Datenbank f√ºr FAISS) |\n","\n","---\n","\n","## 5. Ordnerstruktur f√ºr Tests\n","\n","```bash\n","/project-root/\n","    /src/\n","        chatbot.py\n","    /tests/\n","        test_chatbot.py\n","    /logs/\n","        test_report.log\n","    ...\n","```\n","\n","---\n"],"metadata":{"id":"V1xV8mVljdkL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### üë∑ code"],"metadata":{"id":"dr3XCOHn7Nc_"}},{"cell_type":"code","source":["import pytest\n","pytest.main(['-v', TEST_PATH+'/chatbot_test/chatbot_test.py'])"],"metadata":{"id":"n68k1npffFsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® Error Correction Collection"],"metadata":{"id":"tCeeWN-UjXaI"}},{"cell_type":"code","source":["error_corrections = {\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\":\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, sowie Kau- und Sprechprobleme effektiv behandeln.\",\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\":\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch und Swahili. Dazu hat sie Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\",\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf abgeschlossen.\":\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf promoviert.\",\n","    \"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln.\":\n","    \"Sie ist hoch qualifiziert und hat eine besondere Hingabe zu einem √§sthetischen L√§cheln.\",\n","    \"behandlungsorientierte Zahnberatung\": \"patientenorientierte Beratung\",\n","    \"√§stehthetisches L√§cheln\": \"√§sthetisches L√§cheln\",\n","    \"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\": \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\",\n","    \"Der aktuelle Text zur Zahnspangenpraxis\": \"Der aktuelle Text zur kieferorthop√§dischen Praxis\"\n","}"],"metadata":{"id":"Y_KjsdzFHBpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_error_corrections = {\"Das ist ein neuer Fehler.\": \"Das ist ein korrigierter Fehler.\"}"],"metadata":{"id":"AQJFBU_9-T9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® WIP RAG"],"metadata":{"id":"lNqdhhAit9q9"}},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 1) VectorDB\n","# -------------------------\n","\n","test_text = seo_json[list(seo_json.keys())[0]][\"SEO\"]\n","\n","class VectorDB:\n","    \"\"\"\n","    Eine Klasse f√ºr alles rund um die Vektordatenbank:\n","    - Aufbauen & Laden (FAISS)\n","    - Neue Eintr√§ge hinzuf√ºgen\n","    - Querying f√ºr Context Retrieval\n","    \"\"\"\n","\n","    def __init__(self, db_folder):\n","        \"\"\"\n","        :param db_folder: Pfad zum Datenbank-Ordner\n","        \"\"\"\n","        self.db_folder = db_folder\n","        self.index_file = os.path.join(db_folder, \"faiss_index.bin\")\n","        self.json_file  = os.path.join(db_folder, \"faiss_index.json\")\n","\n","        self.index = None\n","        self.error_dict = {}\n","\n","        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    def build_index(self, error_corrections: dict):\n","        \"\"\"\n","        Baut einen neuen FAISS-Index aus den √ºbergebenen Fehler-Korrektur-Paaren.\n","        \"\"\"\n","        print(\"üî® Baue neuen FAISS-Index...\")\n","        os.makedirs(self.db_folder, exist_ok=True)\n","\n","        self.error_dict = error_corrections\n","        errors = list(self.error_dict.keys())\n","\n","        # Embeddings\n","        embeddings = np.array([self.model.encode(e) for e in errors], dtype=\"float32\")\n","\n","        # FAISS-Index anlegen\n","        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n","        self.index.add(embeddings)\n","\n","        # Daten auf Festplatte schreiben\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ Neuer Index + JSON in '{self.db_folder}' erstellt.\")\n","\n","    def load_index(self):\n","        \"\"\"\n","        L√§dt einen bereits existierenden FAISS-Index und die Fehler-Daten.\n","        \"\"\"\n","        if not (os.path.exists(self.index_file) and os.path.exists(self.json_file)):\n","            raise FileNotFoundError(\"‚ùå Kein FAISS-Index gefunden. Bitte build_index() aufrufen.\")\n","\n","        print(\"üîé Lade vorhandenen FAISS-Index...\")\n","        self.index = faiss.read_index(self.index_file)\n","\n","        with open(self.json_file, \"r\", encoding=\"utf-8\") as f:\n","            self.error_dict = json.load(f)\n","\n","        print(\"‚úÖ Index & Fehler-Korrekturen geladen.\")\n","\n","    def add_entries(self, new_error_corrections: dict):\n","        \"\"\"\n","        F√ºgt weitere Fehler-Korrektur-Paare hinzu, ohne alles neu zu bauen.\n","        \"\"\"\n","        if self.index is None:\n","            # Versuch zu laden, falls vorhanden\n","            if os.path.exists(self.index_file) and os.path.exists(self.json_file):\n","                self.load_index()\n","            else:\n","                raise FileNotFoundError(\"‚ùå Kein Index vorhanden. Bitte erst build_index() nutzen.\")\n","\n","        # Merge in self.error_dict\n","        for fehler, korrektur in new_error_corrections.items():\n","            self.error_dict[fehler] = korrektur\n","\n","        # embeddings nur f√ºr die neuen keys\n","        new_keys = list(new_error_corrections.keys())\n","        new_embeds = np.array([self.model.encode(k) for k in new_keys], dtype=\"float32\")\n","\n","        # An Index anh√§ngen\n","        self.index.add(new_embeds)\n","\n","        # Speichern\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ {len(new_keys)} neue Eintr√§ge hinzugef√ºgt und Index aktualisiert.\")\n","\n","    def query(self, text: str, top_k=3, threshold=0.6):\n","        \"\"\"\n","        Sucht in der DB nach √§hnlichen fehlerhaften Formulierungen.\n","\n","        :param text: Der zu pr√ºfende Satz/Abschnitt\n","        :param top_k: Anzahl der gesuchten √Ñhnlichkeiten\n","        :param threshold: Distanzschwelle\n","        :return: Liste [(fehler, korrektur), ...]\n","        \"\"\"\n","        if self.index is None:\n","            self.load_index()\n","\n","        embed = np.array([self.model.encode(text)], dtype=\"float32\")\n","        distances, indices = self.index.search(embed, top_k)\n","\n","        all_errors = list(self.error_dict.keys())\n","\n","        results = []\n","        for i in range(top_k):\n","          idx = indices[0][i]\n","          # Sicherstellen, dass idx in den Bereich von all_errors passt\n","          if idx < len(all_errors):\n","              if distances[0][i] < threshold:\n","                  fehler_key = all_errors[idx]\n","                  korrektur = self.error_dict[fehler_key]\n","                  results.append((fehler_key, korrektur))\n","        return results\n","\n","\n","    def retrieve_context(self, seo_text: str) -> str:\n","        \"\"\"\n","        Durchsucht den seo_text Satz f√ºr Satz, holt ggf. Korrekturvorschl√§ge\n","        und baut einen Kontextstring.\n","        \"\"\"\n","        lines = []\n","        for s in seo_text.split(\". \"):\n","            suggestions = self.query(s)\n","            for old, new in suggestions:\n","                lines.append(f\"- Fehler: {old} ‚ûù Verbesserung: {new}\")\n","\n","        if lines:\n","            return \"Bekannte Fehler/Korrekturen:\\n\" + \"\\n\".join(lines)\n","        else:\n","            return \"Keine bekannten Fehler gefunden.\"\n","\n","\n","\n","db = VectorDB(db_folder=FAISS_PATH)\n","db.build_index(error_corrections)\n","db.add_entries(new_error_corrections)\n","db.retrieve_context(test_text)"],"metadata":{"id":"alRDUdT4I7o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 2) PromptManager\n","# -------------------------\n","\n","class PromptManager:\n","    \"\"\"\n","    L√§dt Prompts aus dem /data/prompts Ordner und kombiniert sie mit\n","    dem Context aus der VectorDB, um einen finalen Prompt zu erstellen.\n","    \"\"\"\n","\n","    def __init__(self, prompts_folder=\"./data/prompts\"):\n","        \"\"\"\n","        :param prompts_folder: Ordner, in dem .json (oder .txt) Prompts liegen\n","        \"\"\"\n","        self.prompts_folder = prompts_folder\n","\n","    def load_prompt(self, filename: str) -> dict:\n","        \"\"\"\n","        L√§dt einen JSON-Prompt aus dem Ordner, z.B. 'grammar_prompt.json'.\n","        \"\"\"\n","        path = os.path.join(self.prompts_folder, filename)\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                return json.load(f)\n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è Prompt-Datei {path} nicht gefunden!\")\n","            return {}\n","        except json.JSONDecodeError:\n","            print(f\"‚ö†Ô∏è Ung√ºltiges JSON in {path}\")\n","            return {}\n","\n","    def build_final_prompt(self, base_prompt_file: str, context: str, user_text: str) -> (str, str):\n","        \"\"\"\n","        Kombiniert:\n","         - base_prompt_file (System-/User-Prompts)\n","         - den 'context' aus der VectorDB\n","         - den 'user_text' (SEO-Text)\n","        und gibt final (system_prompt, user_prompt) zur√ºck.\n","        \"\"\"\n","        prompt_data = self.load_prompt(base_prompt_file)\n","\n","        system_prompt = prompt_data.get(\"system_prompt\", \"\")\n","        user_prompt   = prompt_data.get(\"user_prompt\", \"\")\n","\n","        # Kontext an system_prompt anh√§ngen\n","        system_prompt_full = system_prompt\n","\n","        # SEO-Text an user_prompt anh√§ngen\n","        user_prompt_full = user_prompt.format(context=context,optimized_text=user_text)\n","\n","        return (system_prompt_full, user_prompt_full)\n","\n","pm = PromptManager(prompts_folder=PROMPT_PATH)\n","context = db.retrieve_context(test_text)\n","final_prompts = pm.build_final_prompt(\"grammar_check.json\", context, test_text)"],"metadata":{"id":"-TWN0JGYI7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from chatbot import Chatbot\n","\n","# -------------------------\n","# 3) SEOGrammarChecker\n","# -------------------------\n","\n","cb = Chatbot(systemprompt=final_prompts[0], userprompt=final_prompts[1])\n","final_text = cb.chat()\n","final_text"],"metadata":{"id":"ahrShDjzI7iz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õì WIP Langchain"],"metadata":{"id":"nb1dof25T_TI"}},{"cell_type":"code","source":["import subprocess\n","from langchain_openai import ChatOpenAI\n","from google.colab import userdata\n","import os\n","\n","subprocess.run([\"pip\", \"install\", \"--upgrade\", \"pydantic\"])\n","\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","llm = ChatOpenAI(temperature=0,\n","    model = \"gpt-4o-mini-2024-07-18\",\n","    openai_api_key=os.environ['OPENAI_API_KEY'],\n",")"],"metadata":{"id":"VTzMyOlEUh6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing_extensions import assert_type\n","from utils import load_prompts\n","\n","prompts = load_prompts(PROMPT_PATH + '/optimize_seo.json')\n","\n","\n","system_prompt = prompts[\"system_prompt\"]\n","user_prompt = prompts[\"user_prompt\"]\n","\n","test_prompt = f\"\"\"{prompts[\"user_prompt\"]}\"\"\"\n","#test_prompt = test_prompt.replace('{keywords}', google_ads_keywords.__str__())\n","\n","user_prompt = user_prompt.replace('{keywords}', google_ads_keywords.__str__())\n","user_prompt = user_prompt.replace('{original_text}', seo_json['https://www.rue-zahnspange.de/']['SEO'])\n","\n","\n","print(test_prompt)"],"metadata":{"id":"-kjb474j49MV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n","\n","def extract_keywords(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein SEO-Experte, spezialisiert auf Keyword-Recherche.\"),\n","        HumanMessage(content=f\"\"\"\n","        Analysiere den folgenden Unternehmens-Text und finde die besten SEO-Keywords.\n","        Ber√ºcksichtige lokale Infos, falls vorhanden.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir eine Liste von Keywords.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"4tpet7NjXCgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_text_for_seo(text, keywords):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein professioneller SEO-Texter.\"),\n","        HumanMessage(content=f\"\"\"\n","        Optimiere den folgenden Text f√ºr SEO, indem du diese Keywords sinnvoll integrierst:\n","\n","        Keywords: {keywords}\n","\n","        Achte auf nat√ºrliche Sprache, gute Lesbarkeit und Vermeidung von Keyword-Stuffing.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir den optimierten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text, keywords=keywords)  # Pass keywords here\n","    response = llm(messages)\n","    return response.content\n"],"metadata":{"id":"6kpgSaXrY240"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grammar_and_style_check(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein erfahrener Lektor und Sprachexperte.\"),\n","        HumanMessage(content=f\"\"\"\n","        Pr√ºfe den folgenden Text auf Grammatik, Rechtschreibung und Stil.\n","        Mache den Text fl√ºssig, professionell und fehlerfrei.\n","\n","        Text:\n","        {text}\n","\n","        Gib den verbesserten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)  # Format with text\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"BJ-Q38kFY5lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def supervisor_check(original_text, keywords, optimized_text, final_text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein Supervisor, der SEO- und Textqualit√§t √ºberpr√ºft.\"),\n","        HumanMessage(content=f\"\"\"\n","        Hier sind die Arbeitsschritte:\n","\n","        Urspr√ºnglicher Text:\n","        {original_text}\n","\n","        Gefundene Keywords:\n","        {keywords}\n","\n","        SEO-optimierter Text:\n","        {optimized_text}\n","\n","        Finaler Text (nach Lektorat):\n","        {final_text}\n","\n","        Beantworte:\n","        1. Sind alle wichtigen Keywords sinnvoll eingebaut?\n","        2. Ist der Text professionell und lesbar?\n","        3. Verbesserungsvorschl√§ge?\n","        Wenn alles gut ist, antworte: 'Finaler Text akzeptiert.'\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(\n","        original_text=original_text,\n","        keywords=keywords,\n","        optimized_text=optimized_text,\n","        final_text=final_text\n","    )  # Format with all variables\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"Um1Q9boRY8rY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seo_pipeline(original_text):\n","    # Keywords finden\n","    print(\"Schritt 1: Keywords finden...\")\n","    keywords = extract_keywords(original_text)\n","    print(\"Gefundene Keywords:\", keywords)\n","\n","    # SEO-Optimierung\n","    print(\"\\nSchritt 2: SEO-Optimierung...\")\n","    optimized_text = optimize_text_for_seo(original_text, keywords)\n","    print(\"SEO-optimierter Text:\\n\", optimized_text)\n","\n","    # Grammatikpr√ºfung\n","    print(\"\\nSchritt 3: Grammatikpr√ºfung...\")\n","    final_text = grammar_and_style_check(optimized_text)\n","    print(\"Finaler Text nach Lektorat:\\n\", final_text)\n","\n","    # Supervisor\n","    print(\"\\nSupervisor pr√ºft...\")\n","    supervisor_feedback = supervisor_check(original_text, keywords, optimized_text, final_text)\n","    print(\"Supervisor Feedback:\\n\", supervisor_feedback)\n","\n","    return final_text\n"],"metadata":{"id":"_T9EbqsEZBJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    unternehmens_text = seo_json['https://www.rue-zahnspange.de/']['alt']\n","\n","    final_output = seo_pipeline(unternehmens_text)\n","    print(\"\\n--- Finaler SEO-optimierter Text ---\\n\")\n","    print(final_output)"],"metadata":{"id":"VlJfLxlpO9ec"},"execution_count":null,"outputs":[]}]}