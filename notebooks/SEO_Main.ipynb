{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["xaxxhKAgyeQq","Pns8QF8VWrHR","TPz7fgRaRAUv","o38bsnD-EWcq","vNVT-lr5jejz","eHxAg4fuIc2T","cgPAMHODtAuZ","SJVtNtjnqvJV","BatTg4ZKvpZV","1zqvsEJvx33W","4v6-jGMeO9Zt","HraJ3_5SJHSv","hdqvYkNsg7fx","dr3XCOHn7Nc_","tCeeWN-UjXaI","lNqdhhAit9q9","nb1dof25T_TI"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyMqGDZrIz0qwsSvgFsROFx+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìî READ_ME und TO_DO"],"metadata":{"id":"EnjP4Zhat_8t"}},{"cell_type":"markdown","source":["#### README"],"metadata":{"id":"xaxxhKAgyeQq"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/README.md'\n","# üöÄ SEO Automation Pipeline mit OpenAI & Retrieval (RAG)\n","\n","Dieses Projekt bietet eine **komplette End-to-End-Pipeline f√ºr die SEO-Optimierung von Websites**, inklusive **Web-Crawling, SEO-Analyse, KI-gest√ºtzter Text-Optimierung und Qualit√§tskontrolle**.\n","\n","Kern des Projekts sind **automatisierte Abl√§ufe**, die von der **Datengewinnung bis zur SEO-optimierten Textgenerierung** reichen.\n","Mithilfe von **OpenAI (ChatGPT)** und einer **Retrieval Augmented Generation (RAG)-Architektur** wird sichergestellt, dass die finalen Texte nicht nur **SEO-freundlich**, sondern auch **grammatikalisch korrekt und hochwertig** sind.\n","\n","## üìö Inhaltsverzeichnis\n","\n","- Features\n","- Projektstruktur\n","- Ablauf & Module\n","- Technologien\n","- Installation\n","- Nutzung\n","- Ziele\n","- Roadmap\n","\n","## ‚úÖ Features\n","\n","- üåê **Automatisiertes Web Crawling** (inkl. Filter f√ºr relevante Inhalte)\n","- ‚úçÔ∏è **Generierung von SEO-optimierten Texten** mithilfe der OpenAI API\n","- üß† **RAG-gest√ºtzte Fehlererkennung & Textkorrektur** mit Vektordatenbank (FAISS)\n","- üìä **Analyse der Optimierungsergebnisse** (Statistiken, √Ñhnlichkeiten, Visualisierungen)\n","- üìà **Keyword-Analyse und Keyword-Optimierung**\n","- üì¶ Ausgabe in **HTML und PDF** f√ºr Kunden\n","- üìä Umfangreiche **Datenvisualisierungen** (Wordclouds, Cosine Similarity, Keyword-Verteilung)\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10oR2bcugvN2MClp14ia7gnzMGX5b896t\" alt=\"SEO Heatmap\" width=\"600\">\n","\n","\n","\n","\n","## üóÇÔ∏è Projektstruktur\n","\n","```\n","SEO-Project/\n","‚îú‚îÄ‚îÄ data/                # Prompts, Fehler-Korrektur-Daten, weitere JSON Dateien\n","‚îú‚îÄ‚îÄ notebooks/           # Colab/Notebooks zum Starten und Entwickeln\n","‚îú‚îÄ‚îÄ output/              # Erzeugte Dateien (HTML, PDF, Bilder)\n","‚îÇ   ‚îú‚îÄ‚îÄ final           # Dokumente f√ºr Kunden (HTML, PDF)\n","‚îÇ   ‚îî‚îÄ‚îÄ images          # Visualisierungen\n","‚îú‚îÄ‚îÄ src/                # Source Code (Python-Klassen und Module)\n","‚îÇ   ‚îú‚îÄ‚îÄ webscraper.py    # Webscrawling und Text-Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ llmprocessor.py # Anbindung an OpenAI API, Keyword Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py       # Zentrale Chatbot-Klasse zur Kommunikation mit GPT\n","‚îÇ   ‚îú‚îÄ‚îÄ seoanalyzer.py   # Analyse und Auswertung der Texte\n","‚îÇ   ‚îú‚îÄ‚îÄ github.py        # Automatischer Upload ins GitHub Repo\n","‚îÇ   ‚îú‚îÄ‚îÄ utils.py         # Hilfsmodule (z.B. f√ºr Prompt-Management)\n","‚îÇ   ‚îî‚îÄ‚îÄ embeddingdemo.py# 3D Embedding- und Cosine Similarity Visualisierungen\n","‚îú‚îÄ‚îÄ tests/              # pytest der Hauptfunktionalit√§ten\n","‚îî‚îÄ‚îÄ requirements.txt    # Python-Abh√§ngigkeiten\n","```\n","\n","## ‚öôÔ∏è Ablauf & Module\n","\n","### 1. **Web Crawling**\n","- **src/webscraper.py**: Holt Inhalte von Webseiten, filtert irrelevante Seiten (z.B. Impressum, AGB).\n","\n","### 2. **SEO-Optimierung mit OpenAI**\n","- **src/llmprocessor.py**:\n","  - Extrahiert Keywords aus den Inhalten.\n","  - Optimiert die Texte f√ºr SEO mit gezielten Prompts.\n","\n","### 3. **Analyse & Visualisierung**\n","- **src/seoanalyzer.py**: Verarbeitet und analysiert die Original- und optimierten Texte.\n","\n","### 4. **GitHub Automation**\n","- **src/github.py**: L√§dt finale Ergebnisse in ein GitHub-Repo hoch.\n","\n","## üß∞ Technologien\n","\n","| Technologie                  | Beschreibung                                       |\n","|-----------------------------|---------------------------------------------------|\n","| Python                      | Hauptsprache                                       |\n","| OpenAI API (ChatGPT, GPT-4)  | Generative KI f√ºr SEO-Texte                       |\n","| FAISS                      | Vektorsuche f√ºr RAG und Text-Fehler                |\n","| Pandas, NumPy               | Datenanalyse und Verarbeitung                      |\n","| Matplotlib, Seaborn         | Visualisierungen                                   |\n","| Sentence Transformers       | Embedding-Erstellung f√ºr Vektordatenbank          |\n","| BeautifulSoup, Requests     | Webcrawling                                        |\n","| Google Colab                | Entwicklung und Ausf√ºhrung                        |\n","\n","## üöÄ Installation\n","\n","```bash\n","pip install -r requirements.txt\n","python -m spacy download de_core_news_sm\n","pip install faiss-cpu sentence-transformers openai wordcloud matplotlib seaborn\n","```\n","\n","## üíª Nutzung\n","\n","```python\n","scraper = WebsiteScraper(start_url=\"https://www.example.com\")\n","scraper.scrape_website()\n","\n","llm_processor = LLMProcessor(prompts_folder, get_filtered_texts, google_ads_keywords)\n","llm_processor.run_all()\n","\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n","```\n","\n","## üéØ Ziele\n","\n","- ‚úÖ Vollst√§ndige Automatisierung der SEO-Optimierung\n","- ‚úÖ RAG f√ºr sprachliche Qualit√§tskontrolle\n","- ‚úÖ Kundenfertige PDF/HTML-Reports\n","\n","## üöß Roadmap\n","\n","- [ ] **Produkt f√ºr Kunden finalisieren:** all-in-one solution f√ºr webcrawl + SEO + optimierten html code\n","- [ ] Automatische SEO Scores (z.B. Google Ads API)\n","- [ ] Automatische Keyword-Erweiterung\n","- [ ] Mehrsprachigkeit\n","- [ ] WordPress-Integration\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"443XYilDN9NA","executionInfo":{"status":"ok","timestamp":1743263489771,"user_tz":-60,"elapsed":119,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"c71ffc42-041e-4d6e-d1fd-fe27d41b1106"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/README.md\n"]}]},{"cell_type":"markdown","source":["####TODO"],"metadata":{"id":"JtMpu3BMyh6-"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/TODO.md'\n","# To-Do Liste: SEO Automation & KI-Projekt\n","\n","Diese Liste fasst alle anstehenden Aufgaben im Projekt zusammen\n","\n","---\n","\n","## 0. **Aktuelles und dringendes**\n","- [ ] 18.3.2025 **Version missmatch**: numpy 2.2.3 und spacy 3.5 **side effects on**: dependencies.py, excelimporter.py, Installation.ipynb **ursachen**: spacy version 3.5 verlangt numpy 1.26.4 -> version missmatch\n","      -> 23.5. class SEOAnlyzer refaktoriert und spacy ersetzt\n","- [ ] 28.3.25 lokale SEO keywords liefern manchmal falsche Stadt\n","\n","---\n","\n","## 1. **Allgemeine Projektorganisation**\n","- [ ] **Projektstruktur verbessern**: Ordner √ºbersichtlich gestalten (z.B. `src/`, `data/`, `tests/`, `notebooks/`, dependencies.py).\n","- [ ] **Dokumentation erweitern**: READ_ME und Wiki (bzw. GitHub Pages) zu jedem Modul anlegen.\n","- [ ] **Automatisierte Tests** Pytest f√ºr Kernfunktionen ausbauen.\n","- [ ] **Produkt f√ºr Kunden finalisieren**\n","- [ ] **FAISS DB**: automatisierte Erweiterung bei neu gefundenen Fehlern\n","- [ ] **Template GitHub**: issues\n","- [ ] Funktionalit√§ten aus **utils.py** √ºberdenken\n","- [ ] langfristig Umstieg auf **langchain**\n","- [ ] textprocessor durch openai **function calling** ersetzen\n","- [ ] **dependencies** und versionen robuster machen\n","- [ ] **bug reporting system** einrichten\n","\n","---\n","\n","## 2. **Vector-Datenbank (FAISS) & Retrieval**\n","- [ ] **VectorDB-Klasse finalisieren**:\n","  - [ ] Kleinere Bugs beheben\n","  - [ ] Userfreundliche Methoden f√ºr neue Eintr√§ge\n","- [ ] **Einrichtung der DB** bei Projektstart (Neubau vs. Laden) vereinheitlichen\n","- [ ] **Konfigurierbare √Ñhnlichkeits-Schwelle** (z.B. `threshold=0.6`) besser dokumentieren\n","- [ ] **Dynamische Filter** f√ºr bestimmte Fehlerkategorien (z.B. Stil vs. Grammatik) √ºberlegen\n","- [ ] **hybrides system mit knowledge tree und RAG** etablieren\n","\n","---\n","\n","## 3. **SEO-Optimierungs-Pipeline**\n","- [ ] **Prompts in JSON-Dateien** verlagern (z.B. `/data/prompts/`) und sauber verlinken\n","- [ ] **Supervisor-Feedback** integrieren & QA-Schritte definieren\n","- [ ] Langchain und SEOPageOptimizer verbinden\n","\n","---\n","\n","## 4. **SEOGrammarChecker & PromptManager**\n","- [ ] Klassenrefactoring:\n","  - [ ] **`VectorDB`** vs. **`PromptManager`** vs. **`SEOGrammarChecker`** sauber trennen\n","  - [ ] M√∂glichst wenig Code-Duplikate, mehr modulare Testbarkeit\n","- [ ] **Konfigurationsdatei** (z.B. YAML) f√ºr Pfade, wie `FAISS_PATH` & Promptordner\n","- [ ] **Erweiterbare Prompt-Templates**:\n","  - [ ] Z.B. `seo_optimization.json`, `grammar_check.json`, `supervisor.json`, etc.\n","\n","---\n","\n","## 5. **Abschluss & Integration**\n","- [ ] **Dokumentation** aller Pipelines & Klassen in der README (oder in separater Doku)\n","- [ ] **Optionale WordPress-Integration** in der Zukunft (Ideenspeicher)\n","  - [ ] Upload via REST API\n","  - [ ] Metadaten (Title, Slug, Tags etc.)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBtOaSe-CHyO","executionInfo":{"status":"ok","timestamp":1743263489802,"user_tz":-60,"elapsed":27,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"bd8c4433-f946-4892-a498-9826b4da00c4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/TODO.md\n"]}]},{"cell_type":"markdown","source":["#globale Parameter\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","# START_URL = 'https://www.ewms-tech.com/'\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","# START_URL = 'https://www.malerarbeiten-koenig.de/'\n","\n","\n","EXCLUDED_WEBSITES = [\"impressum\", \"datenschutz\", \"datenschutzerkl√§rung\", \"agb\"]\n","\n","\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","\n","SRC_PATH, DATA_PATH, TEST_PATH, OUTPUT_PATH = PROJECT_ROOT + \"/src\", PROJECT_ROOT + \"/data\", PROJECT_ROOT + '/tests', PROJECT_ROOT + '/output'\n","FAISS_PATH = DATA_PATH + '/faiss_db'\n","KEYWORD_PATH = DATA_PATH + '/keywords'\n","PROMPT_PATH = DATA_PATH + '/prompts'\n","\n","\n","FINAL_IMAGES = {}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1743263493799,"user_tz":-60,"elapsed":3998,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"517edb21-e77b-4fc7-c341-a24ea5547a63"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["import importlib\n","import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"],"metadata":{"id":"do4Bf0uw-y8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ crawl"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import importlib\n","import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_WEBSITES)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì∫ google ads seo keywords"],"metadata":{"id":"eHxAg4fuIc2T"}},{"cell_type":"code","source":["import excelimporter\n","importlib.reload(excelimporter)\n","from excelimporter import ExcelImporter\n","\n","importer = ExcelImporter(project_folder=PROJECT_ROOT, header=2)\n","keyword_df = importer.import_all()\n","\n","if keyword_df is not None:\n","    excluded_seo_keywords = ['spange de', 'kfo zentrum essen', 'gerade zahne', 'zahn spange']\n","\n","    keyword_df = keyword_df[(keyword_df['Avg. monthly searches'] > 10) &\n","    (~keyword_df['Keyword'].isin(excluded_seo_keywords))\n","    ].sort_values(by='Avg. monthly searches', ascending=False).reset_index(drop=True).copy()\n","\n","    google_ads_keywords = list(keyword_df['Keyword'])\n","\n","    keyword_df.set_index(keyword_df.columns[0], inplace=True)\n","    searches_df = keyword_df[[col for col in keyword_df.columns if col.startswith('Searches')]]\n","\n","else:\n","    print(\"Error: importer.import_all() returned None. Check your data file and ExcelImporter class.\")\n","    google_ads_keywords = None"],"metadata":{"id":"REeveZ8rwEys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keywordvisualizer\n","importlib.reload(keywordvisualizer)\n","from keywordvisualizer import KeywordVisualizer\n","\n","\n","visualizer = KeywordVisualizer(\n","    df=searches_df,\n","    output_dir=OUTPUT_PATH+'/images',\n","    image_paths=FINAL_IMAGES\n",")\n","visualizer.heatmap()"],"metadata":{"id":"LnJ5j0VZ_JWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"31sgkNSF_JTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XXzSL_SR_JRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sXLiqhti_JOm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆwebtext analysis + SEO"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["import llmprocessor\n","importlib.reload(llmprocessor)\n","from llmprocessor import LLMProcessor\n","\n","google_ads_keywords = None if not google_ads_keywords else google_ads_keywords\n","\n","llm_processor = LLMProcessor(PROMPT_PATH, original_texts, google_ads_keywords=google_ads_keywords)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"q3isJXZoebv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìÅ Textprozessor"],"metadata":{"id":"SJVtNtjnqvJV"}},{"cell_type":"code","source":["import json\n","import textprocessor\n","importlib.reload(textprocessor)\n","from textprocessor import TextProcessor\n","\n","# JSON mit den SEO-Abschnitten extrahieren\n","json_output = TextProcessor.extract_sections_to_json(list(optimized_texts.keys()), list(optimized_texts.values()))\n","seo_json = json.loads(json_output)\n","\n","# texte bereinigen und hinzuf√ºgen\n","seo_json = TextProcessor.add_cleaned_text(seo_json, original_texts)\n","\n","# Ergebnis anzeigen\n","print(json.dumps(seo_json, indent=4, ensure_ascii=False))\n"],"metadata":{"id":"SS1zb2sAWKIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîé SEO Analyses + Statistics"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["# k√ºnstliche SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"],"metadata":{"id":"1lkqnkWkw5wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","exclude_list = [\"leila\", \"graf\", \"koenig\", \"bjoern\", \"k√∂nig\", 'bj√∂rn', 'adolf', 'schmidt', 'strasse', 'stra√üe', 'tilo', 'remhof']\n","\n","keywords_final = json.loads(llm_processor.get_keywords()['keywords_final']) if not google_ads_keywords else google_ads_keywords\n","\n","seo_analyzer = SEOAnalyzer(\n","    seo_json=seo_json,\n","    keywords_final=keywords_final,\n","    output_dir=OUTPUT_PATH+\"/images\",\n","    historical_data=historical_data,\n","    wordcloud_exclude=exclude_list,\n","    shared_image_dict=FINAL_IMAGES\n",")\n","\n","analysis_paths = seo_analyzer.run_analysis()\n","print(\"Analysis Plot Pfade:\", analysis_paths)\n","\n","model_paths = seo_analyzer.run_models()\n","print(\"Model Plot Pfade:\", model_paths)\n","\n","all_paths = seo_analyzer.get_all_image_paths()\n","print(\"Alle Pfade gesammelt:\", all_paths)\n"],"metadata":{"id":"MtZrcL_wxKzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üõè embedding demo"],"metadata":{"id":"1zqvsEJvx33W"}},{"cell_type":"code","source":["import os\n","import embeddingdemo\n","importlib.reload(embeddingdemo)\n","from embeddingdemo import EmbeddingDemo\n","\n","demo = EmbeddingDemo(output_dir=OUTPUT_PATH+'/images', final_images=FINAL_IMAGES)\n","demo.run_all_visualizations()\n","\n","print(\"Alle Embedding-Bilder:\", demo.get_image_paths())\n","print(\"Gemeinsame Bilder (shared dict):\", FINAL_IMAGES)"],"metadata":{"id":"toL_RwWuO72N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["import seoreportexporter\n","importlib.reload(seoreportexporter)\n","from seoreportexporter import SEOReportExporter\n","exporter = SEOReportExporter(seo_json, OUTPUT_PATH, DATA_PATH+\"/intro_texts/sections_intro.json\", FINAL_IMAGES)\n","exporter.run_all_exports()"],"metadata":{"id":"ByLZAW6rSbwF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• Tests"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"markdown","source":["#### üìÉ Doku"],"metadata":{"id":"hdqvYkNsg7fx"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/tests/DOKU_TESTS.md'\n","# ‚úÖ Pytest Template: Chatbot Klasse\n","\n","## 1. Klassen & Methoden die getestet werden sollen\n","\n","- **Chatbot**\n","  - `chat()`\n","  - `chat_with_streaming()`\n","\n","---\n","\n","## 2. Beispielhafte Inputs + erwartete Outputs pro Methode\n","\n","| Methode                      | Beispiel Input                                                           | Erwartete Ausgabe    |\n","|-----------------------------|-------------------------------------------------------------------------|----------------------|\n","| `Chatbot.chat()`             | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'           | \"Test erfolgreich\"   |\n","| `Chatbot.chat_with_streaming()` | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'         | \"Test erfolgreich\"   |\n","\n","---\n","\n","## 3. Return-Typen der Methoden\n","\n","| Methode                      | R√ºckgabe-Typ |\n","|-----------------------------|--------------|\n","| `Chatbot.chat()`             | `str`        |\n","| `Chatbot.chat_with_streaming()` | `str`     |\n","\n","---\n","\n","## 4. Externe Services mocken?\n","\n","| Service         |  Mocken?                           |\n","|-----------------|-------------------------------------|\n","| OpenAI API      |  Nein                                |\n","| FAISS Index     |  Ja (kleine Test-Datenbank f√ºr FAISS) |\n","\n","---\n","\n","## 5. Ordnerstruktur f√ºr Tests\n","\n","```bash\n","/project-root/\n","    /src/\n","        chatbot.py\n","    /tests/\n","        test_chatbot.py\n","    /logs/\n","        test_report.log\n","    ...\n","```\n","\n","---\n"],"metadata":{"id":"V1xV8mVljdkL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### üë∑ code"],"metadata":{"id":"dr3XCOHn7Nc_"}},{"cell_type":"code","source":["import pytest\n","pytest.main(['-v', TEST_PATH+'/chatbot_test/chatbot_test.py'])"],"metadata":{"id":"n68k1npffFsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® Error Collection"],"metadata":{"id":"tCeeWN-UjXaI"}},{"cell_type":"code","source":["error_corrections = {\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\":\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, sowie Kau- und Sprechprobleme effektiv behandeln.\",\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\":\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch und Swahili. Dazu hat sie Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\",\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf abgeschlossen.\":\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf promoviert.\",\n","    \"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln.\":\n","    \"Sie ist hoch qualifiziert und hat eine besondere Hingabe zu einem √§sthetischen L√§cheln.\",\n","    \"behandlungsorientierte Zahnberatung\": \"patientenorientierte Beratung\",\n","    \"√§stehthetisches L√§cheln\": \"√§sthetisches L√§cheln\",\n","    \"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\": \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\",\n","    \"Der aktuelle Text zur Zahnspangenpraxis\": \"Der aktuelle Text zur kieferorthop√§dischen Praxis\"\n","}"],"metadata":{"id":"Y_KjsdzFHBpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_error_corrections = {\"Das ist ein neuer Fehler.\": \"Das ist ein korrigierter Fehler.\"}"],"metadata":{"id":"AQJFBU_9-T9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® RAG"],"metadata":{"id":"lNqdhhAit9q9"}},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 1) VectorDB\n","# -------------------------\n","\n","test_text = seo_json[list(seo_json.keys())[0]][\"SEO\"]\n","\n","class VectorDB:\n","    \"\"\"\n","    Eine Klasse f√ºr alles rund um die Vektordatenbank:\n","    - Aufbauen & Laden (FAISS)\n","    - Neue Eintr√§ge hinzuf√ºgen\n","    - Querying f√ºr Context Retrieval\n","    \"\"\"\n","\n","    def __init__(self, db_folder):\n","        \"\"\"\n","        :param db_folder: Pfad zum Datenbank-Ordner\n","        \"\"\"\n","        self.db_folder = db_folder\n","        self.index_file = os.path.join(db_folder, \"faiss_index.bin\")\n","        self.json_file  = os.path.join(db_folder, \"faiss_index.json\")\n","\n","        self.index = None\n","        self.error_dict = {}\n","\n","        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    def build_index(self, error_corrections: dict):\n","        \"\"\"\n","        Baut einen neuen FAISS-Index aus den √ºbergebenen Fehler-Korrektur-Paaren.\n","        \"\"\"\n","        print(\"üî® Baue neuen FAISS-Index...\")\n","        os.makedirs(self.db_folder, exist_ok=True)\n","\n","        self.error_dict = error_corrections\n","        errors = list(self.error_dict.keys())\n","\n","        # Embeddings\n","        embeddings = np.array([self.model.encode(e) for e in errors], dtype=\"float32\")\n","\n","        # FAISS-Index anlegen\n","        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n","        self.index.add(embeddings)\n","\n","        # Daten auf Festplatte schreiben\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ Neuer Index + JSON in '{self.db_folder}' erstellt.\")\n","\n","    def load_index(self):\n","        \"\"\"\n","        L√§dt einen bereits existierenden FAISS-Index und die Fehler-Daten.\n","        \"\"\"\n","        if not (os.path.exists(self.index_file) and os.path.exists(self.json_file)):\n","            raise FileNotFoundError(\"‚ùå Kein FAISS-Index gefunden. Bitte build_index() aufrufen.\")\n","\n","        print(\"üîé Lade vorhandenen FAISS-Index...\")\n","        self.index = faiss.read_index(self.index_file)\n","\n","        with open(self.json_file, \"r\", encoding=\"utf-8\") as f:\n","            self.error_dict = json.load(f)\n","\n","        print(\"‚úÖ Index & Fehler-Korrekturen geladen.\")\n","\n","    def add_entries(self, new_error_corrections: dict):\n","        \"\"\"\n","        F√ºgt weitere Fehler-Korrektur-Paare hinzu, ohne alles neu zu bauen.\n","        \"\"\"\n","        if self.index is None:\n","            # Versuch zu laden, falls vorhanden\n","            if os.path.exists(self.index_file) and os.path.exists(self.json_file):\n","                self.load_index()\n","            else:\n","                raise FileNotFoundError(\"‚ùå Kein Index vorhanden. Bitte erst build_index() nutzen.\")\n","\n","        # Merge in self.error_dict\n","        for fehler, korrektur in new_error_corrections.items():\n","            self.error_dict[fehler] = korrektur\n","\n","        # embeddings nur f√ºr die neuen keys\n","        new_keys = list(new_error_corrections.keys())\n","        new_embeds = np.array([self.model.encode(k) for k in new_keys], dtype=\"float32\")\n","\n","        # An Index anh√§ngen\n","        self.index.add(new_embeds)\n","\n","        # Speichern\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ {len(new_keys)} neue Eintr√§ge hinzugef√ºgt und Index aktualisiert.\")\n","\n","    def query(self, text: str, top_k=3, threshold=0.6):\n","        \"\"\"\n","        Sucht in der DB nach √§hnlichen fehlerhaften Formulierungen.\n","\n","        :param text: Der zu pr√ºfende Satz/Abschnitt\n","        :param top_k: Anzahl der gesuchten √Ñhnlichkeiten\n","        :param threshold: Distanzschwelle\n","        :return: Liste [(fehler, korrektur), ...]\n","        \"\"\"\n","        if self.index is None:\n","            self.load_index()\n","\n","        embed = np.array([self.model.encode(text)], dtype=\"float32\")\n","        distances, indices = self.index.search(embed, top_k)\n","\n","        all_errors = list(self.error_dict.keys())\n","\n","        results = []\n","        for i in range(top_k):\n","          idx = indices[0][i]\n","          # Sicherstellen, dass idx in den Bereich von all_errors passt\n","          if idx < len(all_errors):\n","              if distances[0][i] < threshold:\n","                  fehler_key = all_errors[idx]\n","                  korrektur = self.error_dict[fehler_key]\n","                  results.append((fehler_key, korrektur))\n","        return results\n","\n","\n","    def retrieve_context(self, seo_text: str) -> str:\n","        \"\"\"\n","        Durchsucht den seo_text Satz f√ºr Satz, holt ggf. Korrekturvorschl√§ge\n","        und baut einen Kontextstring.\n","        \"\"\"\n","        lines = []\n","        for s in seo_text.split(\". \"):\n","            suggestions = self.query(s)\n","            for old, new in suggestions:\n","                lines.append(f\"- Fehler: {old} ‚ûù Verbesserung: {new}\")\n","\n","        if lines:\n","            return \"Bekannte Fehler/Korrekturen:\\n\" + \"\\n\".join(lines)\n","        else:\n","            return \"Keine bekannten Fehler gefunden.\"\n","\n","\n","\n","db = VectorDB(db_folder=FAISS_PATH)\n","db.build_index(error_corrections)\n","db.add_entries(new_error_corrections)\n","db.retrieve_context(test_text)"],"metadata":{"id":"alRDUdT4I7o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 2) PromptManager\n","# -------------------------\n","\n","class PromptManager:\n","    \"\"\"\n","    L√§dt Prompts aus dem /data/prompts Ordner und kombiniert sie mit\n","    dem Context aus der VectorDB, um einen finalen Prompt zu erstellen.\n","    \"\"\"\n","\n","    def __init__(self, prompts_folder=\"./data/prompts\"):\n","        \"\"\"\n","        :param prompts_folder: Ordner, in dem .json (oder .txt) Prompts liegen\n","        \"\"\"\n","        self.prompts_folder = prompts_folder\n","\n","    def load_prompt(self, filename: str) -> dict:\n","        \"\"\"\n","        L√§dt einen JSON-Prompt aus dem Ordner, z.B. 'grammar_prompt.json'.\n","        \"\"\"\n","        path = os.path.join(self.prompts_folder, filename)\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                return json.load(f)\n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è Prompt-Datei {path} nicht gefunden!\")\n","            return {}\n","        except json.JSONDecodeError:\n","            print(f\"‚ö†Ô∏è Ung√ºltiges JSON in {path}\")\n","            return {}\n","\n","    def build_final_prompt(self, base_prompt_file: str, context: str, user_text: str) -> (str, str):\n","        \"\"\"\n","        Kombiniert:\n","         - base_prompt_file (System-/User-Prompts)\n","         - den 'context' aus der VectorDB\n","         - den 'user_text' (SEO-Text)\n","        und gibt final (system_prompt, user_prompt) zur√ºck.\n","        \"\"\"\n","        prompt_data = self.load_prompt(base_prompt_file)\n","\n","        system_prompt = prompt_data.get(\"system_prompt\", \"\")\n","        user_prompt   = prompt_data.get(\"user_prompt\", \"\")\n","\n","        # Kontext an system_prompt anh√§ngen\n","        system_prompt_full = system_prompt\n","\n","        # SEO-Text an user_prompt anh√§ngen\n","        user_prompt_full = user_prompt.format(context=context,optimized_text=user_text)\n","\n","        return (system_prompt_full, user_prompt_full)\n","\n","pm = PromptManager(prompts_folder=PROMPT_PATH)\n","context = db.retrieve_context(test_text)\n","final_prompts = pm.build_final_prompt(\"grammar_check.json\", context, test_text)"],"metadata":{"id":"-TWN0JGYI7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from chatbot import Chatbot\n","\n","# -------------------------\n","# 3) SEOGrammarChecker\n","# -------------------------\n","\n","cb = Chatbot(systemprompt=final_prompts[0], userprompt=final_prompts[1])\n","final_text = cb.chat()\n","final_text"],"metadata":{"id":"ahrShDjzI7iz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õì Langchain"],"metadata":{"id":"nb1dof25T_TI"}},{"cell_type":"code","source":["import subprocess\n","from langchain_openai import ChatOpenAI\n","from google.colab import userdata\n","import os\n","\n","subprocess.run([\"pip\", \"install\", \"--upgrade\", \"pydantic\"])\n","\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","llm = ChatOpenAI(temperature=0,\n","    model = \"gpt-4o-mini-2024-07-18\",\n","    openai_api_key=os.environ['OPENAI_API_KEY'],\n",")"],"metadata":{"id":"VTzMyOlEUh6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing_extensions import assert_type\n","from utils import load_prompts\n","\n","prompts = load_prompts(PROMPT_PATH + '/optimize_seo.json')\n","\n","\n","system_prompt = prompts[\"system_prompt\"]\n","user_prompt = prompts[\"user_prompt\"]\n","\n","test_prompt = f\"\"\"{prompts[\"user_prompt\"]}\"\"\"\n","#test_prompt = test_prompt.replace('{keywords}', google_ads_keywords.__str__())\n","\n","user_prompt = user_prompt.replace('{keywords}', google_ads_keywords.__str__())\n","user_prompt = user_prompt.replace('{original_text}', seo_json[START_URL]['SEO'])\n","\n","\n","print(test_prompt)"],"metadata":{"id":"-kjb474j49MV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n","\n","def extract_keywords(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein SEO-Experte, spezialisiert auf Keyword-Recherche.\"),\n","        HumanMessage(content=f\"\"\"\n","        Analysiere den folgenden Unternehmens-Text und finde die besten SEO-Keywords.\n","        Ber√ºcksichtige lokale Infos, falls vorhanden.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir eine Liste von Keywords.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"4tpet7NjXCgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_text_for_seo(text, keywords):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein professioneller SEO-Texter.\"),\n","        HumanMessage(content=f\"\"\"\n","        Optimiere den folgenden Text f√ºr SEO, indem du diese Keywords sinnvoll integrierst:\n","\n","        Keywords: {keywords}\n","\n","        Achte auf nat√ºrliche Sprache, gute Lesbarkeit und Vermeidung von Keyword-Stuffing.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir den optimierten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text, keywords=keywords)  # Pass keywords here\n","    response = llm(messages)\n","    return response.content\n"],"metadata":{"id":"6kpgSaXrY240"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grammar_and_style_check(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein erfahrener Lektor und Sprachexperte.\"),\n","        HumanMessage(content=f\"\"\"\n","        Pr√ºfe den folgenden Text auf Grammatik, Rechtschreibung und Stil.\n","        Mache den Text fl√ºssig, professionell und fehlerfrei.\n","\n","        Text:\n","        {text}\n","\n","        Gib den verbesserten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)  # Format with text\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"BJ-Q38kFY5lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def supervisor_check(original_text, keywords, optimized_text, final_text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein Supervisor, der SEO- und Textqualit√§t √ºberpr√ºft.\"),\n","        HumanMessage(content=f\"\"\"\n","        Hier sind die Arbeitsschritte:\n","\n","        Urspr√ºnglicher Text:\n","        {original_text}\n","\n","        Gefundene Keywords:\n","        {keywords}\n","\n","        SEO-optimierter Text:\n","        {optimized_text}\n","\n","        Finaler Text (nach Lektorat):\n","        {final_text}\n","\n","        Beantworte:\n","        1. Sind alle wichtigen Keywords sinnvoll eingebaut?\n","        2. Ist der Text professionell und lesbar?\n","        3. Verbesserungsvorschl√§ge?\n","        Wenn alles gut ist, antworte: 'Finaler Text akzeptiert.'\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(\n","        original_text=original_text,\n","        keywords=keywords,\n","        optimized_text=optimized_text,\n","        final_text=final_text\n","    )  # Format with all variables\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"Um1Q9boRY8rY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seo_pipeline(original_text):\n","    # Keywords finden\n","    print(\"Schritt 1: Keywords finden...\")\n","    keywords = extract_keywords(original_text)\n","    print(\"Gefundene Keywords:\", keywords)\n","\n","    # SEO-Optimierung\n","    print(\"\\nSchritt 2: SEO-Optimierung...\")\n","    optimized_text = optimize_text_for_seo(original_text, keywords)\n","    print(\"SEO-optimierter Text:\\n\", optimized_text)\n","\n","    # Grammatikpr√ºfung\n","    print(\"\\nSchritt 3: Grammatikpr√ºfung...\")\n","    final_text = grammar_and_style_check(optimized_text)\n","    print(\"Finaler Text nach Lektorat:\\n\", final_text)\n","\n","    # Supervisor\n","    print(\"\\nSupervisor pr√ºft...\")\n","    supervisor_feedback = supervisor_check(original_text, keywords, optimized_text, final_text)\n","    print(\"Supervisor Feedback:\\n\", supervisor_feedback)\n","\n","    return final_text\n"],"metadata":{"id":"_T9EbqsEZBJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    unternehmens_text = seo_json[START_URL]['alt']\n","\n","    final_output = seo_pipeline(unternehmens_text)\n","    print(\"\\n--- Finaler SEO-optimierter Text ---\\n\")\n","    print(final_output)"],"metadata":{"id":"VlJfLxlpO9ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ü•ºLab"],"metadata":{"id":"MM7uu--42W4u"}},{"cell_type":"code","source":["import os\n","import re\n","import requests\n","import logging\n","import openai\n","from bs4 import BeautifulSoup\n","from bs4 import Comment\n","from urllib.parse import urljoin, urlparse\n","from datetime import datetime\n","\n","from chatbot import Chatbot\n","\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","\n","\n","\n","class SEOPageOptimizer:\n","    \"\"\"\n","    Diese Klasse l√§dt den HTML-Code einer Webseite herunter, extrahiert alle sichtbaren Texte und Meta-Tags,\n","    optimiert diese Inhalte mithilfe von ChatGPT (√ºber die Chatbot-Klasse) und speichert den modifizierten HTML-Code lokal.\n","    \"\"\"\n","\n","    def __init__(self, max_pages=1, output_dir=\"output/final\"):\n","        \"\"\"\n","        :param max_pages: Wie viele Seiten h√∂chstens gecrawlt werden (f√ºr Start-Seite meist 1 ausreichend).\n","        :param output_dir: Ausgabeordner f√ºr die optimierte HTML-Datei.\n","        \"\"\"\n","        logging.basicConfig(level=logging.INFO)\n","        self.max_pages = max_pages\n","        self.output_dir = output_dir\n","        os.makedirs(self.output_dir, exist_ok=True)\n","\n","    def fetch_html(self, url: str) -> str:\n","        \"\"\"L√§dt den HTML-Quelltext der angegebenen URL herunter.\"\"\"\n","        logging.info(f\"Lade HTML von {url}...\")\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()  # wirft Exception bei HTTP-Fehler\n","        logging.info(\"HTML erfolgreich heruntergeladen.\")\n","        return response.text\n","\n","    def extract_content(self, html: str):\n","        \"\"\"\n","        Extrahiert sichtbare Texte (p, h1-h6, li) und Meta-Tags (<title>, <meta name='description'>).\n","        Gibt (text_elements, title, meta_desc, soup) zur√ºck.\n","          - text_elements: Liste von (element, original_text)\n","          - title/meta_desc: Originale Strings\n","          - soup: Das BS-Objekt zum sp√§teren Re-Inject\n","        \"\"\"\n","        logging.info(\"Extrahiere sichtbare Texte und Meta-Informationen...\")\n","        soup = BeautifulSoup(html, \"html.parser\")\n","\n","        # Meta-Tags\n","        title_tag = soup.find(\"title\")\n","        meta_desc_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n","\n","        title_text = title_tag.get_text(strip=True) if title_tag else \"\"\n","        meta_desc_text = meta_desc_tag.get(\"content\", \"\").strip() if meta_desc_tag else \"\"\n","\n","        # Sichtbare Texte => p, h1-h6, li\n","        text_elements = []\n","        for tag_name in [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\"]:\n","            for elem in soup.find_all(tag_name):\n","                text = elem.get_text(separator=\" \", strip=True)\n","                if text:\n","                    text_elements.append((elem, text))\n","\n","        logging.info(f\"Title: '{title_text}', Meta-Desc: '{meta_desc_text}', \"\n","                     f\"Text-Elemente gesamt: {len(text_elements)}.\")\n","        return text_elements, title_text, meta_desc_text, soup\n","\n","    def optimize_content(self, text_elements, title_text, meta_desc_text):\n","        \"\"\"\n","        Nutzt ChatGPT, um alle Texte SEO-optimiert umzuschreiben. Generiert au√üerdem\n","        neuen Title und neue Meta-Description.\n","        \"\"\"\n","        logging.info(\"Starte SEO-Optimierung der Inhalte via ChatGPT...\")\n","\n","        # System-Prompt f√ºrs ChatGPT\n","        # Evtl. anpassen, wie formell, Sprachniveau etc.\n","        system_prompt = (\n","            \"Du bist ein professioneller SEO-Texter. \"\n","            \"Deine Aufgabe ist es, Texte suchmaschinenfreundlicher zu gestalten, \"\n","            \"ohne deren inhaltliche Aussage zu ver√§ndern.\"\n","        )\n","\n","        optimized_texts = []\n","\n","        # 1) Alle sichtbaren Texte optimieren\n","        for elem, original_text in text_elements:\n","            user_prompt = (\n","                f\"Bitte √ºberarbeite folgenden Text, damit er SEO-freundlicher wird:\\n\"\n","                f\"Original: \\\"{original_text}\\\"\\n\\n\"\n","                \"Achte darauf:\\n\"\n","                \"- Sinn und Inhalt bleiben erhalten\\n\"\n","                \"- Verwende eine klar verst√§ndliche Sprache\\n\"\n","                \"- Integriere relevante Keywords, ohne Keyword-Stuffing\\n\"\n","                \"- Nutze ggf. Synonyme\\n\"\n","                \"Gib mir nur den umgeschriebenen Text zur√ºck.\"\n","            )\n","\n","            cb = Chatbot(systemprompt=system_prompt, userprompt=user_prompt)\n","            new_text = cb.chat()\n","            if not new_text.strip():\n","                # Falls leer, nimm original\n","                new_text = original_text\n","            optimized_texts.append((elem, new_text))\n","\n","        # 2) Meta-Titel & -Description neu generieren\n","        # Aus allen Texten ggf. einen Kontext bauen\n","        full_context = \" \".join([t for _, t in text_elements])[:1000]  # Ggf. beschr√§nkt auf 1000 Zeichen\n","\n","        # Title prompt\n","        title_user_prompt = (\n","            f\"Erstelle einen SEO-optimierten Titel (max. 60 Zeichen) f√ºr folgenden Seiteninhalt:\\n\"\n","            f\"\\\"{full_context}\\\"\\n\\n\"\n","            \"Anforderungen:\\n\"\n","            \"- Hauptkeyword an Anfang\\n\"\n","            \"- Maximal 60 Zeichen\\n\"\n","            \"- Klare, kurze Formulierung\\n\"\n","            \"Gib mir nur den finalen Titel zur√ºck.\"\n","        )\n","        cb_title = Chatbot(systemprompt=system_prompt, userprompt=title_user_prompt)\n","        new_title = cb_title.chat().strip()\n","        if not new_title:\n","            new_title = title_text  # Fallback\n","\n","        # Description prompt\n","        desc_user_prompt = (\n","            f\"Erstelle eine SEO-optimierte Meta-Description (max. 155 Zeichen) \"\n","            f\"f√ºr folgenden Inhalt:\\n\"\n","            f\"\\\"{full_context}\\\"\\n\\n\"\n","            \"Anforderungen:\\n\"\n","            \"- Hauptkeyword einbauen\\n\"\n","            \"- Ca. 155 Zeichen\\n\"\n","            \"- Mit Call-to-Action enden\\n\"\n","            \"Gib mir nur die finalen S√§tze zur√ºck.\"\n","        )\n","        cb_desc = Chatbot(systemprompt=system_prompt, userprompt=desc_user_prompt)\n","        new_desc = cb_desc.chat().strip()\n","        if not new_desc:\n","            new_desc = meta_desc_text  # Fallback\n","\n","        logging.info(\"SEO-Optimierung abgeschlossen.\")\n","        return optimized_texts, new_title, new_desc\n","\n","    def inject_content(self, soup, optimized_texts, new_title, new_description):\n","        \"\"\"\n","        Ersetzt Originaltexte in soup durch die neuen Texte + Title + Meta Desc\n","        \"\"\"\n","        logging.info(\"F√ºge optimierte Inhalte in das HTML ein...\")\n","\n","        # 1) Alle sichtbaren Texte neu einf√ºgen\n","        for elem, text in optimized_texts:\n","            elem.clear()\n","            elem.append(text)\n","\n","        # 2) Title / Meta\n","        title_tag = soup.find(\"title\")\n","        if title_tag:\n","            title_tag.string = new_title\n","        else:\n","            head = soup.find(\"head\")\n","            if head:\n","                new_tag = soup.new_tag(\"title\")\n","                new_tag.string = new_title\n","                head.append(new_tag)\n","\n","        desc_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n","        if desc_tag:\n","            desc_tag[\"content\"] = new_description\n","        else:\n","            head = soup.find(\"head\")\n","            if head:\n","                new_meta = soup.new_tag(\"meta\", attrs={\"name\": \"description\", \"content\": new_description})\n","                head.append(new_meta)\n","\n","        return soup\n","\n","    def save_html(self, soup, filepath: str):\n","        \"\"\"\n","        Speichert den aktualisierten HTML-Code in einer lokalen Datei (UTF-8).\n","        \"\"\"\n","        logging.info(f\"Speichere optimiertes HTML unter {filepath}\")\n","        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","            f.write(str(soup))\n","        logging.info(\"Datei gespeichert.\")\n","\n","    def optimize_page(self, url: str, output_path: str):\n","        \"\"\"\n","        F√ºhrt alle Schritte aus:\n","         1) HTML laden\n","         2) Texte extrahieren\n","         3) Optimieren\n","         4) Re-Inject\n","         5) Speichern\n","        \"\"\"\n","        # 1) HTML laden\n","        html = self.fetch_html(url)\n","        # 2) Inhalte extrahieren\n","        text_elems, old_title, old_desc, soup = self.extract_content(html)\n","        # 3) ChatGPT-Optimierung\n","        opt_texts, new_title, new_desc = self.optimize_content(text_elems, old_title, old_desc)\n","        # 4) Re-Inject\n","        new_soup = self.inject_content(soup, opt_texts, new_title, new_desc)\n","        # 5) Speichern\n","        self.save_html(new_soup, output_path)\n","        logging.info(\"SEO-Optimierung vollst√§ndig abgeschlossen.\")\n"],"metadata":{"id":"uH0s9orO2e7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = SEOPageOptimizer(max_pages=1, output_dir=OUTPUT_PATH+\"/final\")\n","url_to_optimize = START_URL\n","output_html_path = OUTPUT_PATH+\"/final/optimized_page.html\"\n","\n","optimizer.optimize_page(url_to_optimize, output_html_path)\n","\n","print(\"Fertig! Schau dir die Datei an:\", output_html_path)\n"],"metadata":{"id":"TgDpNjaU2525"},"execution_count":null,"outputs":[]}]}