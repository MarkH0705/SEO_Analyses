{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Pns8QF8VWrHR","TPz7fgRaRAUv","o38bsnD-EWcq","vNVT-lr5jejz","eHxAg4fuIc2T","SJVtNtjnqvJV","BatTg4ZKvpZV","AeTy1z3vxblD","1zqvsEJvx33W","4v6-jGMeO9Zt","HraJ3_5SJHSv","tCeeWN-UjXaI"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyOYPky21BdlN1e+viq/fQpr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#üåãHYPERPARAMETER\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","\n","SRC_PATH, DATA_PATH = PROJECT_ROOT + \"/src\", PROJECT_ROOT + \"/data\"\n","FAISS_PATH = DATA_PATH + '/faiss_db'\n","KEYWORD_PATH = DATA_PATH + '/keywords'\n","\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","EXCLUDED_WEBSITES = [\"impressum\", \"datenschutz\", \"agb\"]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1741650260123,"user_tz":-60,"elapsed":2484,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"b1c1f783-8eae-43aa-c4bf-0076d189bb40"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements + dependencies"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo","executionInfo":{"status":"ok","timestamp":1741650309592,"user_tz":-60,"elapsed":49467,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de3200ff-5b43-4c53-aea3-4e1c6f199ca7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"],"metadata":{"id":"do4Bf0uw-y8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_WEBSITES)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì∫ google ads seo keywords"],"metadata":{"id":"eHxAg4fuIc2T"}},{"cell_type":"code","source":["import excelimporter\n","importlib.reload(excelimporter)\n","from excelimporter import ExcelImporter\n","\n","importer = ExcelImporter(\n","    project_folder=PROJECT_ROOT,\n","    header=2  # <-- 1 = \"nehme die zweite Zeile als Spaltennamen\"\n",")\n","keyword_df = importer.import_all()\n","\n","excluded_seo_keywords = ['spange de', 'kfo zentrum essen']\n","keyword_df = keyword_df[(keyword_df['Avg. monthly searches'] > 10) &\n"," (~keyword_df['Keyword'].isin(excluded_seo_keywords))\n"," ].sort_values(by='Avg. monthly searches', ascending=False).reset_index(drop=True).copy()\n","\n","google_ads_keywords = list(keyword_df['Keyword'])"],"metadata":{"id":"REeveZ8rwEys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fEn-t4ZvwEwH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vz2iJMFa5QSv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆwebtext analysis + SEO"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["import llmprocessor\n","importlib.reload(llmprocessor)\n","from llmprocessor import LLMProcessor\n","\n","llm_processor = LLMProcessor(PROJECT_ROOT, original_texts, google_ads_keywords=google_ads_keywords)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"q3isJXZoebv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìÅ Textprozessor"],"metadata":{"id":"SJVtNtjnqvJV"}},{"cell_type":"code","source":["import textprocessor\n","importlib.reload(textprocessor)\n","from textprocessor import TextProcessor\n","\n","# JSON mit den SEO-Abschnitten extrahieren\n","json_output = TextProcessor.extract_sections_to_json(list(optimized_texts.keys()), list(optimized_texts.values()))\n","seo_json = json.loads(json_output)\n","\n","# texte bereinigen und hinzuf√ºgen\n","seo_json = TextProcessor.add_cleaned_text(seo_json, original_texts)\n","\n","# Ergebnis anzeigen\n","print(json.dumps(seo_json, indent=4, ensure_ascii=False))\n"],"metadata":{"id":"SS1zb2sAWKIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìäSEO Analysis"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["llm_processor.get_keywords()['keywords_final']"],"metadata":{"id":"_NV0yuINglEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","keywords_final = json.loads(llm_processor.get_keywords()['keywords_final']) if not google_ads_keywords else google_ads_keywords\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n"],"metadata":{"id":"MtZrcL_wxKzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂ conversion rates, modelierungen"],"metadata":{"id":"AeTy1z3vxblD"}},{"cell_type":"code","source":["# Historische SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"],"metadata":{"id":"1lkqnkWkw5wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final, historical_data)\n","seo_analyzer.run_models()"],"metadata":{"id":"ob_ct0NfVagR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üõè embedding demo"],"metadata":{"id":"1zqvsEJvx33W"}},{"cell_type":"code","source":["import embeddingdemo\n","importlib.reload(embeddingdemo)\n","from embeddingdemo import EmbeddingDemo\n","\n","demo = EmbeddingDemo()\n","demo.run_all_visualizations()"],"metadata":{"id":"toL_RwWuO72N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["%%capture\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout f√ºr \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox f√ºr zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um Zeilenumbr√ºche aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und Erkl√§rung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">{{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from jinja2 import Template\n","\n","def json_to_html(json_data):\n","    # HTML-Template mit EINER Spalte f√ºr \"SEO\" (die \"alt\"-Spalte entf√§llt)\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            /* Zeilenumbr√ºche aus dem JSON in <br> wandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- \"Analyse\" normal untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">\n","                {{ sections.Analyse | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- \"Erkl√§rung\" normal untereinander -->\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">\n","                {{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- NUR noch die \"SEO\"-Spalte -->\n","            <div class=\"column\">\n","                <p class=\"header\">SEO</p>\n","                <p class=\"content\">\n","                    {{ sections.SEO | replace('\\\\n','<br>') | safe }}\n","                </p>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    template = Template(html_template)\n","    return template.render(data=json_data)\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"Cx6qfyVfJzI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschlie√üend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, R√§nder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• RAG Sammlung Grammatikfehler"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthop√§din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\"\n","\n","\"Sie hat ihren Master of Science in Kieferorthop√§die von der Danube Private University, Krems, √ñsterreich, und hat an der Heinrich-Heine-Universit√§t D√ºsseldorf abgeschlossen.\"\n","\n","\"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln. \"\n","\n","\"behandlungsorientierte Zahnberatung\"\n","\n","\"√§stehthetisches L√§cheln\"\n","\n","\"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\" \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\"\n","\n","\"Der aktuelle Text zur Zahnspangenpraxis\"\n","\n","\"Kieferorthop√§de in [Ihre Stadt]\"\n"],"metadata":{"id":"fH4mRdPXceeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V1xV8mVljdkL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® RAG"],"metadata":{"id":"tCeeWN-UjXaI"}},{"cell_type":"code","source":["chatbot_system_prompt = (\n","    \"Du bist ein hochqualifizierter Korrekturleser und optimierst Texte f√ºr SEO-Zwecke. \"\n","    \"Du bist Teil eines RAG Systems und deine Aufgabe ist es, grammatikalische Fehler zu erkennen. Du bist die finale Korrektur f√ºr die Texte.\"\n","    \"Du wirst context retrieval zu den Texten erhalten. Du wirst die Texte nur korrigieren, wenn grammatikalische Fehler auftreten\"\n","    \" oder wenn es grobe Fehler in der Formulierung gibt. Denke daran, dass die Texte bereits otpimiert sind und nur die \"\n","    \"letzten Fehler erkannt werden m√ºssen! Deswegen wirst du die Texte nur ver√§ndern, wenn du grammatikalische Fehler findest.\"\n","    \"Wenn du Fehler findest, gebe den Text korrigiert zur√ºck.\"\n","    \"Wenn du keinen Fehler findest, gebe einfach nur den Text zur√ºck.\"\n","    \"Hier ist der context retrieval:\"\n",")\n","\n","chatbot_user_prompt = (\n","    \"Bitte verbessere den folgenden Text:\"\n",")"],"metadata":{"id":"6G1Jy7f1HBms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import rag\n","importlib.reload(rag)\n","from rag import RAG\n","\n","\n","# 1. RAG-Objekt anlegen\n","rag = RAG(FAISS_PATH, chatbot_system_prompt=chatbot_system_prompt,chatbot_user_prompt=chatbot_user_prompt)\n","\n","# 2. Nur beim ersten Mal:\n","error_corrections = {\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\":\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, sowie Kau- und Sprechprobleme effektiv behandeln.\",\n","    \"Als in Kenia geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\":\n","    \"Als in Kenia geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch und Swahili. Dazu hat sie Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\",\n","    \"Sie hat ihren Master of Science in Kieferorthop√§die von der Danube Private University, Krems, √ñsterreich, und hat an der Heinrich-Heine-Universit√§t D√ºsseldorf abgeschlossen.\":\n","    \"Sie hat ihren Master of Science in Kieferorthop√§die von der Danube Private University, Krems, √ñsterreich, und hat an der Heinrich-Heine-Universit√§t D√ºsseldorf promoviert.\",\n","    \"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln.\":\n","    \"Sie ist hoch qualifiziert und hat eine besondere Hingabe zu einem √§sthetischen L√§cheln. \",\n","    \"behandlungsorientierte Zahnberatung\": \"patientenorientierte Beratung\",\n","    \"√§stehthetisches L√§cheln\": \"√§sthetisches L√§cheln\",\n","    \"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\": \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\",\n","    \"Der aktuelle Text zur Zahnspangenpraxis\": \"Der aktuelle Text zur kieferorthop√§dischen Praxis\"\n","}\n","rag.initialize_db(error_corrections=error_corrections)\n","\n","# 3. Weitere Eintr√§ge\n","# new_entries = {\n","#     \"Unsere Firma hat viele zufriedene Kunde\": \"Unsere Firma hat viele zufriedene Kunden\"\n","# }\n","# rag.add_entries(new_entries)\n","\n","# 4. Text pr√ºfen\n","seo_text = seo_json['https://www.rue-zahnspange.de/']['SEO']\n","improved = rag.check_text(seo_text)\n","print(\"Verbesserter Text:\\n\", improved)\n"],"metadata":{"id":"Y_KjsdzFHBpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QNo5WkK8I569"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"686NDBbgI7r1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 1) VectorDB\n","# -------------------------\n","\n","class VectorDB:\n","    \"\"\"\n","    Eine Klasse f√ºr alles rund um deine Vektordatenbank:\n","    - Aufbauen & Laden (FAISS)\n","    - Neue Eintr√§ge hinzuf√ºgen\n","    - Querying f√ºr Context Retrieval\n","    \"\"\"\n","\n","    def __init__(self, db_folder):\n","        \"\"\"\n","        :param db_folder: Pfad zu deinem Datenbank-Ordner, z.B. \"/content/drive/MyDrive/SEO/data\"\n","        \"\"\"\n","        self.db_folder = db_folder\n","        self.index_file = os.path.join(db_folder, \"faiss_index.bin\")\n","        self.json_file  = os.path.join(db_folder, \"faiss_index.json\")\n","\n","        self.index = None\n","        self.error_dict = {}  # z.B. {fehler: korrektur}\n","\n","        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    def build_index(self, error_corrections: dict):\n","        \"\"\"\n","        Baut einen neuen FAISS-Index aus den √ºbergebenen Fehler-Korrektur-Paaren.\n","        \"\"\"\n","        print(\"üî® Baue neuen FAISS-Index...\")\n","        os.makedirs(self.db_folder, exist_ok=True)\n","\n","        self.error_dict = error_corrections\n","        errors = list(self.error_dict.keys())\n","\n","        # Embeddings\n","        embeddings = np.array([self.model.encode(e) for e in errors], dtype=\"float32\")\n","\n","        # FAISS-Index anlegen\n","        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n","        self.index.add(embeddings)\n","\n","        # Daten auf Festplatte schreiben\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ Neuer Index + JSON in '{self.db_folder}' erstellt.\")\n","\n","    def load_index(self):\n","        \"\"\"\n","        L√§dt einen bereits existierenden FAISS-Index und die Fehler-Daten.\n","        \"\"\"\n","        if not (os.path.exists(self.index_file) and os.path.exists(self.json_file)):\n","            raise FileNotFoundError(\"‚ùå Kein FAISS-Index gefunden. Bitte build_index() aufrufen.\")\n","\n","        print(\"üîé Lade vorhandenen FAISS-Index...\")\n","        self.index = faiss.read_index(self.index_file)\n","\n","        with open(self.json_file, \"r\", encoding=\"utf-8\") as f:\n","            self.error_dict = json.load(f)\n","\n","        print(\"‚úÖ Index & Fehler-Korrekturen geladen.\")\n","\n","    def add_entries(self, new_error_corrections: dict):\n","        \"\"\"\n","        F√ºgt weitere Fehler-Korrektur-Paare hinzu, ohne alles neu zu bauen.\n","        \"\"\"\n","        if self.index is None:\n","            # Versuch zu laden, falls vorhanden\n","            if os.path.exists(self.index_file) and os.path.exists(self.json_file):\n","                self.load_index()\n","            else:\n","                raise FileNotFoundError(\"‚ùå Kein Index vorhanden. Bitte erst build_index() nutzen.\")\n","\n","        # Merge in self.error_dict\n","        for fehler, korrektur in new_error_corrections.items():\n","            self.error_dict[fehler] = korrektur\n","\n","        # embeddings nur f√ºr die neuen keys\n","        new_keys = list(new_error_corrections.keys())\n","        new_embeds = np.array([self.model.encode(k) for k in new_keys], dtype=\"float32\")\n","\n","        # An Index anh√§ngen\n","        self.index.add(new_embeds)\n","\n","        # Speichern\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ {len(new_keys)} neue Eintr√§ge hinzugef√ºgt und Index aktualisiert.\")\n","\n","    def query(self, text: str, top_k=3, threshold=0.6):\n","        \"\"\"\n","        Sucht in der DB nach √§hnlichen fehlerhaften Formulierungen.\n","\n","        :param text: Der zu pr√ºfende Satz/Abschnitt\n","        :param top_k: Anzahl der gesuchten √Ñhnlichkeiten\n","        :param threshold: Distanzschwelle\n","        :return: Liste [(fehler, korrektur), ...]\n","        \"\"\"\n","        if self.index is None:\n","            self.load_index()\n","\n","        embed = np.array([self.model.encode(text)], dtype=\"float32\")\n","        distances, indices = self.index.search(embed, top_k)\n","\n","        all_errors = list(self.error_dict.keys())\n","\n","        results = []\n","        for i in range(top_k):\n","            if distances[0][i] < threshold:\n","                fehler_key = all_errors[indices[0][i]]\n","                korrektur = self.error_dict[fehler_key]\n","                results.append((fehler_key, korrektur))\n","        return results\n","\n","    def retrieve_context(self, seo_text: str) -> str:\n","        \"\"\"\n","        Durchsucht den seo_text Satz f√ºr Satz, holt ggf. Korrekturvorschl√§ge\n","        und baut einen Kontextstring.\n","        \"\"\"\n","        lines = []\n","        for s in seo_text.split(\". \"):\n","            suggestions = self.query(s)\n","            for old, new in suggestions:\n","                lines.append(f\"- Fehler: {old} ‚ûù Verbesserung: {new}\")\n","\n","        if lines:\n","            return \"Bekannte Fehler/Korrekturen:\\n\" + \"\\n\".join(lines)\n","        else:\n","            return \"Keine bekannten Fehler gefunden.\"\n","\n"],"metadata":{"id":"alRDUdT4I7o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","# -------------------------\n","# 2) PromptManager\n","# -------------------------\n","\n","class PromptManager:\n","    \"\"\"\n","    L√§dt Prompts aus dem /data/prompts Ordner und kombiniert sie mit\n","    dem Context aus der VectorDB, um einen finalen Prompt zu erstellen.\n","    \"\"\"\n","\n","    def __init__(self, prompts_folder=\"./data/prompts\"):\n","        \"\"\"\n","        :param prompts_folder: Ordner, in dem .json (oder .txt) Prompts liegen\n","        \"\"\"\n","        self.prompts_folder = prompts_folder\n","        # Du k√∂nntest hier diverse Prompts laden oder\n","        # sie dynamisch in build_final_prompt() nach Dateiname laden\n","\n","    def load_prompt(self, filename: str) -> dict:\n","        \"\"\"\n","        L√§dt einen JSON-Prompt aus dem Ordner, z.B. 'grammar_prompt.json'.\n","        \"\"\"\n","        path = os.path.join(self.prompts_folder, filename)\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                return json.load(f)\n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è Prompt-Datei {path} nicht gefunden!\")\n","            return {}\n","        except json.JSONDecodeError:\n","            print(f\"‚ö†Ô∏è Ung√ºltiges JSON in {path}\")\n","            return {}\n","\n","    def build_final_prompt(self, base_prompt_file: str, context: str, user_text: str) -> (str, str):\n","        \"\"\"\n","        Kombiniert:\n","         - base_prompt_file (System-/User-Prompts)\n","         - den 'context' aus der VectorDB\n","         - den 'user_text' (SEO-Text)\n","        und gibt final (system_prompt, user_prompt) zur√ºck.\n","        \"\"\"\n","        prompt_data = self.load_prompt(base_prompt_file)\n","\n","        # Annahme: JSON enth√§lt z.B. { \"system_prompt\": \"...\", \"user_prompt\": \"...\" }\n","        system_prompt = prompt_data.get(\"system_prompt\", \"\")\n","        user_prompt   = prompt_data.get(\"user_prompt\", \"\")\n","\n","        # Kontext an system_prompt anh√§ngen\n","        system_prompt_full = f\"{system_prompt}\\n{context}\"\n","\n","        # SEO-Text an user_prompt anh√§ngen\n","        user_prompt_full = f\"{user_prompt}\\n\\n{user_text}\"\n","\n","        return (system_prompt_full, user_prompt_full)\n","\n","\n"],"metadata":{"id":"-TWN0JGYI7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","# -------------------------\n","# 3) SEOGrammarChecker\n","# -------------------------\n","\n","class SEOGrammarChecker:\n","    \"\"\"\n","    Nutzt VectorDB, um Kontext zu holen,\n","    PromptManager, um finalen Prompt zu bauen,\n","    und Chatbot, um den Text zu optimieren.\n","    \"\"\"\n","\n","    def __init__(self, vector_db: VectorDB, prompt_manager: PromptManager):\n","        \"\"\"\n","        :param vector_db: Instanz von VectorDB\n","        :param prompt_manager: Instanz von PromptManager\n","        \"\"\"\n","        self.vector_db = vector_db\n","        self.prompt_manager = prompt_manager\n","\n","    def check_text(self, seo_text: str, prompt_file: str = \"grammar_prompt.json\") -> str:\n","        \"\"\"\n","        1) Hole Context aus VectorDB\n","        2) Lade base_prompt aus 'prompt_file'\n","        3) Baue finalen Prompt\n","        4) Chatbot-Aufruf\n","        :return: Grammatikalisch verbesserter Text\n","        \"\"\"\n","        # 1) Kontext f√ºr den Text\n","        context_str = self.vector_db.retrieve_context(seo_text)\n","\n","        # 2) Prompts aus JSON\n","        # z.B. { \"system_prompt\": \"Du bist ein Korrektor...\", \"user_prompt\": \"Bitte verbessere...\" }\n","        (system_prompt, user_prompt) = self.prompt_manager.build_final_prompt(\n","            base_prompt_file=prompt_file,\n","            context=context_str,\n","            user_text=seo_text\n","        )\n","\n","        # 3) Chatbot-Aufruf\n","        cb = Chatbot(systemprompt=system_prompt, userprompt=user_prompt)\n","        final_text = cb.chat()\n","        return final_text\n"],"metadata":{"id":"ahrShDjzI7iz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õì Langchain"],"metadata":{"id":"nb1dof25T_TI"}},{"cell_type":"code","source":["import subprocess\n","subprocess.run([\"pip\", \"install\", \"--upgrade\", \"pydantic\"], check=True)"],"metadata":{"id":"kg2CDmOEbScK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from google.colab import userdata\n","import os\n","\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","llm = ChatOpenAI(temperature=0,\n","    model=\"gpt-4o-mini-2024-07-18\",\n","    openai_api_key=os.environ['OPENAI_API_KEY']\n",")"],"metadata":{"id":"VTzMyOlEUh6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n","\n","def extract_keywords(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein SEO-Experte, spezialisiert auf Keyword-Recherche.\"),\n","        HumanMessage(content=f\"\"\"\n","        Analysiere den folgenden Unternehmens-Text und finde die besten SEO-Keywords.\n","        Ber√ºcksichtige lokale Infos, falls vorhanden.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir eine Liste von Keywords.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"4tpet7NjXCgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_text_for_seo(text, keywords):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein professioneller SEO-Texter.\"),\n","        HumanMessage(content=f\"\"\"\n","        Optimiere den folgenden Text f√ºr SEO, indem du diese Keywords sinnvoll integrierst:\n","\n","        Keywords: {keywords}\n","\n","        Achte auf nat√ºrliche Sprache, gute Lesbarkeit und Vermeidung von Keyword-Stuffing.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir den optimierten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text, keywords=keywords)  # Pass keywords here\n","    response = llm(messages)\n","    return response.content\n"],"metadata":{"id":"6kpgSaXrY240"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grammar_and_style_check(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein erfahrener Lektor und Sprachexperte.\"),\n","        HumanMessage(content=f\"\"\"\n","        Pr√ºfe den folgenden Text auf Grammatik, Rechtschreibung und Stil.\n","        Mache den Text fl√ºssig, professionell und fehlerfrei.\n","\n","        Text:\n","        {text}\n","\n","        Gib den verbesserten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)  # Format with text\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"BJ-Q38kFY5lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def supervisor_check(original_text, keywords, optimized_text, final_text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein Supervisor, der SEO- und Textqualit√§t √ºberpr√ºft.\"),\n","        HumanMessage(content=f\"\"\"\n","        Hier sind die Arbeitsschritte:\n","\n","        Urspr√ºnglicher Text:\n","        {original_text}\n","\n","        Gefundene Keywords:\n","        {keywords}\n","\n","        SEO-optimierter Text:\n","        {optimized_text}\n","\n","        Finaler Text (nach Lektorat):\n","        {final_text}\n","\n","        Beantworte:\n","        1. Sind alle wichtigen Keywords sinnvoll eingebaut?\n","        2. Ist der Text professionell und lesbar?\n","        3. Verbesserungsvorschl√§ge?\n","        Wenn alles gut ist, antworte: 'Finaler Text akzeptiert.'\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(\n","        original_text=original_text,\n","        keywords=keywords,\n","        optimized_text=optimized_text,\n","        final_text=final_text\n","    )  # Format with all variables\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"Um1Q9boRY8rY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seo_pipeline(original_text):\n","    # Schritt 1: Keywords finden\n","    print(\"Schritt 1: Keywords finden...\")\n","    keywords = extract_keywords(original_text)\n","    print(\"Gefundene Keywords:\", keywords)\n","\n","    # Schritt 2: SEO-Optimierung\n","    print(\"\\nSchritt 2: SEO-Optimierung...\")\n","    optimized_text = optimize_text_for_seo(original_text, keywords)\n","    print(\"SEO-optimierter Text:\\n\", optimized_text)\n","\n","    # Schritt 3: Grammatikpr√ºfung\n","    print(\"\\nSchritt 3: Grammatikpr√ºfung...\")\n","    final_text = grammar_and_style_check(optimized_text)\n","    print(\"Finaler Text nach Lektorat:\\n\", final_text)\n","\n","    # Optional: Supervisor\n","    print(\"\\nSupervisor pr√ºft...\")\n","    supervisor_feedback = supervisor_check(original_text, keywords, optimized_text, final_text)\n","    print(\"Supervisor Feedback:\\n\", supervisor_feedback)\n","\n","    return final_text\n"],"metadata":{"id":"_T9EbqsEZBJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    unternehmens_text = \"\"\"\n","    Unser Unternehmen bietet hochwertige Gartenger√§te in M√ºnchen. Von Rasenm√§hern bis Heckenscheren, wir beraten pers√∂nlich.\n","    \"\"\"\n","\n","    final_output = seo_pipeline(unternehmens_text)\n","    print(\"\\n--- Finaler SEO-optimierter Text ---\\n\")\n","    print(final_output)\n"],"metadata":{"id":"NrZBvcpwZFq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["google_ads_keywords"],"metadata":{"id":"gnShucwBZMfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EqcE7XBKcc3B"},"execution_count":null,"outputs":[]}]}