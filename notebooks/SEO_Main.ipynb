{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Pns8QF8VWrHR","TPz7fgRaRAUv","o38bsnD-EWcq","vNVT-lr5jejz","cgPAMHODtAuZ","SJVtNtjnqvJV","4v6-jGMeO9Zt"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyOO1FOyp0ecrfKqNzOuMpG0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#üåãHYPERPARAMETER\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","SRC_PATH = PROJECT_ROOT + \"/src\"\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1741108015013,"user_tz":-60,"elapsed":5846,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"85941bfb-c1f4-4b63-8091-a1cd8ec3adbc"},"execution_count":138,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements + dependencies"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo","executionInfo":{"status":"ok","timestamp":1741108081286,"user_tz":-60,"elapsed":66271,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":139,"outputs":[]},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5","executionInfo":{"status":"ok","timestamp":1741108081830,"user_tz":-60,"elapsed":529,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"4c6fd6e6-e4e5-4ccb-b6bc-9fac057fde14"},"execution_count":140,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"],"metadata":{"id":"do4Bf0uw-y8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_KEYWORDS)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆwebtext analysis + SEO"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["import llmprocessor\n","importlib.reload(llmprocessor)\n","from llmprocessor import LLMProcessor\n","\n","llm_processor = LLMProcessor(PROJECT_ROOT, original_texts)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"q3isJXZoebv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìÅ Textprozessor"],"metadata":{"id":"SJVtNtjnqvJV"}},{"cell_type":"code","source":["import textprocessor\n","importlib.reload(textprocessor)\n","from textprocessor import TextProcessor\n","\n","# JSON mit den SEO-Abschnitten extrahieren\n","json_output = TextProcessor.extract_sections_to_json(list(optimized_texts.keys()), list(optimized_texts.values()))\n","seo_json = json.loads(json_output)\n","\n","# texte bereinigen und hinzuf√ºgen\n","seo_json = TextProcessor.add_cleaned_text(seo_json, original_texts)\n","\n","# Ergebnis anzeigen\n","print(json.dumps(seo_json, indent=4, ensure_ascii=False))\n"],"metadata":{"id":"SS1zb2sAWKIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìä SEO Analysis"],"metadata":{"id":"V50QPoW3F6wW"}},{"cell_type":"code","source":["original_texts_list_clean = []\n","for key in seo_json:\n","  original_texts_list_clean.append(seo_json[key]['alt'])\n","\n","optimized_texts_list_clean = []\n","for key in seo_json:\n","  optimized_texts_list_clean.append(seo_json[key]['SEO'])"],"metadata":{"id":"6fvi3_scYei3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple stats\n","\n","def text_stats(text):\n","    words = text.split()\n","    return {\n","        \"Zeichenanzahl\": len(text),\n","        \"Wortanzahl\": len(words),\n","        \"Satzanzahl\": text.count('.') + text.count('!') + text.count('?')\n","    }\n","\n","\n","vn = [original_texts_list_clean, optimized_texts_list_clean]\n","\n","for _, listlist in enumerate(vn):\n","\n","  print(f'{[\"original\", \"SEO\"][_]}')\n","\n","  for text in listlist:\n","    print(text_stats(text))\n","\n","\n","\n"],"metadata":{"id":"GDBZiKVnH9dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_word_frequencies(text):\n","    words = text.lower().split()\n","    return Counter(words)\n","\n","\n","for _, listlist in enumerate(original_texts_list_clean):\n","\n","  # Wortfrequenzen berechnen\n","  original_freq = get_word_frequencies(original_texts_list_clean[_])\n","  optimized_freq = get_word_frequencies(optimized_texts_list_clean[_])\n","\n","  # Unterschied berechnen\n","  diff = {word: optimized_freq[word] - original_freq[word] for word in set(original_freq) | set(optimized_freq)}\n","\n","  # Sortiert ausgeben (absteigend nach √Ñnderung)\n","  sorted_diff = sorted(diff.items(), key=lambda x: x[1], reverse=True)\n","  for word, change in sorted_diff:\n","      print(f\"{word}: {change}\")\n"],"metadata":{"id":"qGiMiXwIH9ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_used = llm_processor.get_keywords()\n","keywords_final = json.loads(keywords_used['keywords_final'])"],"metadata":{"id":"yFAfe_--E8os"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, listlist in enumerate(optimized_texts_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([\", \".join(keywords_final), optimized_texts_list_clean[_]])\n","\n","  # Cosinus-√Ñhnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"√Ñhnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"W5bd8b1yH9YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, listlist in enumerate(original_texts_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([\", \".join(keywords_final), original_texts_list_clean[_]])\n","\n","  # Cosinus-√Ñhnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"√Ñhnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"z1-eRoarZ-9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìäSEO Analysis"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n"],"metadata":{"id":"MtZrcL_wxKzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂ conversion rates, modelierungen"],"metadata":{"id":"AeTy1z3vxblD"}},{"cell_type":"code","source":["# Historische SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"],"metadata":{"id":"1lkqnkWkw5wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final, historical_data)\n","seo_analyzer.run_models()"],"metadata":{"id":"ob_ct0NfVagR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üõè embedding demo"],"metadata":{"id":"1zqvsEJvx33W"}},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","\n","# Function to visualize keyword similarity in 3D using spheres to represent keyword clouds\n","def plot_3d_keyword_similarity(title=\"Keyword Cloud Similarity Visualization\"):\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Generate data points for the keyword cloud\n","    np.random.seed(42)\n","    keywords = np.random.rand(50, 3) - 0.5  # Random points for keyword representation\n","    old_text = keywords + np.array([0.4, 0.4, 0.4])  # Shifted to represent the old text\n","    optimized_text = keywords + np.array([-0.4, -0.3, -0.2])  # Shifted to represent the optimized text\n","\n","    # Plot the keyword cloud\n","    ax.scatter(keywords[:, 0], keywords[:, 1], keywords[:, 2], color='g', label='Keywords', alpha=0.6, s=40)\n","\n","    # Plot old text keyword distribution\n","    ax.scatter(old_text[:, 0], old_text[:, 1], old_text[:, 2], color='r', label='Old Text', alpha=0.7, s=50)\n","\n","    # Plot optimized text keyword distribution\n","    ax.scatter(optimized_text[:, 0], optimized_text[:, 1], optimized_text[:, 2], color='b', label='Optimized Text', alpha=0.7, s=50)\n","\n","    # Plot settings\n","    ax.set_xlim([-1, 1])\n","    ax.set_ylim([-1, 1])\n","    ax.set_zlim([-1, 1])\n","    ax.set_title(title)\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","# Visualizing keyword distribution in 3D for Old Text, Optimized Text, and Keywords\n","plot_3d_keyword_similarity(\"3D Keyword Cloud Similarity: Old Text vs Optimized Text\")\n"],"metadata":{"id":"rTEF2o98kcXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visual explanation of cosine similarity with embeddings, dot product, and how the result turns into a probability\n","\n","def plot_cosine_similarity_steps():\n","    fig = plt.figure(figsize=(18, 6))\n","\n","    # Step 1: Plot the embeddings\n","    ax1 = fig.add_subplot(131, projection='3d')\n","    vec1 = np.array([0.8, 0.6, 0.3])\n","    vec2 = np.array([0.9, 0.4, 0.5])\n","\n","    ax1.quiver(0, 0, 0, vec1[0], vec1[1], vec1[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax1.quiver(0, 0, 0, vec2[0], vec2[1], vec2[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","    ax1.set_xlim([0, 1])\n","    ax1.set_ylim([0, 1])\n","    ax1.set_zlim([0, 1])\n","    ax1.set_title('Step 1: Embeddings (Keyword Vectors)')\n","    ax1.set_xlabel('X')\n","    ax1.set_ylabel('Y')\n","    ax1.set_zlabel('Z')\n","    ax1.legend()\n","\n","    # Step 2: Calculate and visualize the dot product\n","    ax2 = fig.add_subplot(132)\n","    dot_product = np.dot(vec1, vec2)\n","    ax2.bar(['Dot Product'], [dot_product], color='orange')\n","    ax2.set_ylim(0, 1.5)\n","    ax2.set_title('Step 2: Dot Product Calculation')\n","    ax2.text(0, dot_product + 0.05, f'{dot_product:.2f}', ha='center', fontsize=12)\n","\n","    # Step 3: Convert to cosine similarity and interpret as probability\n","    ax3 = fig.add_subplot(133)\n","    cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n","    ax3.bar(['Cosine Similarity'], [cos_sim], color='green')\n","    ax3.set_ylim(0, 1.1)\n","    ax3.set_title('Step 3: Cosine Similarity as Probability')\n","    ax3.text(0, cos_sim + 0.05, f'{cos_sim:.2f}', ha='center', fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_cosine_similarity_steps()\n"],"metadata":{"id":"-J9rBQbokcUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization with vector names instead of cosine similarity values\n","def plot_3d_cosine_comparison_with_labels():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.4, 0.3, 0.2])\n","    vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Add labels to the vector tips\n","    ax.text(vec_keywords[0], vec_keywords[1], vec_keywords[2], 'SEO Keywords', color='#4CAF50', fontsize=12, fontweight='bold')\n","    ax.text(vec_old_text[0], vec_old_text[1], vec_old_text[2], 'Old Text', color='#FF5733', fontsize=12, fontweight='bold')\n","    ax.text(vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], 'Optimized Text', color='#1E90FF', fontsize=12, fontweight='bold')\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Labels\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_labels()\n"],"metadata":{"id":"zAQSqDAXxD03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Function to get BERT embeddings for a word\n","def get_bert_embedding(word):\n","    inputs = tokenizer(word, return_tensors='pt')\n","    outputs = model(**inputs)\n","    # Take the mean of the last hidden state as the word embedding\n","    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n","\n","# List of words for embeddings\n","words = [\"king\", \"queen\", \"onion\", \"man\",\"feather\",\"scale\", \"lioness\", \"bull\", \"cow\", \"slither\",\"skin\", \"wing\", \"fin\",\n","         \"green\", \"fly\", \"run\", \"snake\", \"sneak\", \"wolf\", \"fur\", \"swim\", \"fish\", \"air\", \"water\", \"swimming\", \"bird\",\n","         \"cheetah\", \"woman\", \"apple\", \"orange\", \"car\", \"train\", \"dog\", \"running\",\"cat\", \"red\", \"yellow\", \"banana\", \"lion\", \"wolf\", \"dog\"\n","         ,\"day\", \"sun\", \"night\", \"moon\"]\n","\n","# Generate BERT embeddings for all words\n","bert_embeddings = np.array([get_bert_embedding(word) for word in words])\n","\n","# Function to calculate pairwise distances for word relationships\n","def calculate_pairwise_distances(word_pairs):\n","    distance_results = []\n","    for (word1, word2, word3, word4) in word_pairs:\n","        idx1, idx2, idx3, idx4 = words.index(word1), words.index(word2), words.index(word3), words.index(word4)\n","        distance1 = np.linalg.norm(bert_embeddings[idx1] - bert_embeddings[idx2])  # Distance between word1 and word2\n","        distance2 = np.linalg.norm(bert_embeddings[idx3] - bert_embeddings[idx4])  # Distance between word3 and word4\n","        distance_results.append({\n","            \"Word Pair 1\": f\"{word1} - {word2}\",\n","            \"Distance 1\": round(distance1, 2),\n","            \"Word Pair 2\": f\"{word3} - {word4}\",\n","            \"Distance 2\": round(distance2, 2)\n","        })\n","\n","    return pd.DataFrame(distance_results)\n","\n","# Define specific word pairs to compare distances\n","word_pairs_for_distances = [\n","    (\"king\", \"man\", \"queen\", \"woman\"),\n","    (\"apple\", \"red\", \"banana\", \"yellow\"),\n","    (\"day\", \"sun\", \"night\", \"moon\")\n","]\n","\n","# Calculate pairwise distances using BERT embeddings\n","pairwise_distance_df = calculate_pairwise_distances(word_pairs_for_distances)\n","\n","# Display the resulting table\n","import IPython.display as display\n","display.display(pairwise_distance_df)"],"metadata":{"id":"ROYD3s4SGC9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Function to get BERT embeddings for a word\n","def get_bert_embedding(word):\n","    inputs = tokenizer(word, return_tensors='pt')\n","    outputs = model(**inputs)\n","    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n","\n","# List of words and their embeddings\n","words = [\"king\", \"queen\", \"man\", \"woman\",\"red\", \"yellow\", \"banana\", \"apple\", \"day\", \"sun\", \"night\", \"moon\"]\n","bert_embeddings = np.array([get_bert_embedding(word) for word in words])\n","\n","# Reduce dimensionality to 3D using PCA\n","pca = PCA(n_components=3)\n","bert_embeddings_3d = pca.fit_transform(bert_embeddings)\n","\n","# Create Plotly figure\n","fig = go.Figure()\n","\n","\n","# Add 3D scatter points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[bert_embeddings_3d[i, 0]], y=[bert_embeddings_3d[i, 1]], z=[bert_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=10, color='skyblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Customize layout with initial camera settings\n","ticktext = [\"-4\", \"-2\", \"0\", \"2\", \"4\"]\n","tickvals = [-5, -2, 0, 2, 5]\n","prange=[-5, 5]\n","\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of BERT Word Embeddings\",\n","    scene=dict(\n","        xaxis=dict(\n","            range=prange,\n","            title=\"PCA Component 1\",\n","            tickmode=\"array\",\n","            tickvals=tickvals,  # Custom tick positions\n","            ticktext=ticktext # Custom tick labels (optional)\n","        ),\n","        yaxis=dict(\n","            range=[-4,3],\n","            title=\"PCA Component 2\",\n","            tickvals=tickvals,  # Custom tick positions\n","            ticktext=ticktext\n","        ),\n","        zaxis=dict(\n","            range=prange,\n","            title=\"PCA Component 3\",\n","            tickmode=\"array\",\n","            tickvals=tickvals,  # Custom tick positions\n","            ticktext=ticktext  # Example custom labels\n","        ),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=100, b=100)\n",")\n","\n","# Highlight the relationships with arrows\n","def add_arrow(word1, word2, color):\n","    idx1, idx2 = words.index(word1), words.index(word2)\n","    fig.add_trace(go.Scatter3d(\n","        x=[bert_embeddings_3d[idx1, 0], bert_embeddings_3d[idx2, 0]],\n","        y=[bert_embeddings_3d[idx1, 1], bert_embeddings_3d[idx2, 1]],\n","        z=[bert_embeddings_3d[idx1, 2], bert_embeddings_3d[idx2, 2]],\n","        mode='lines',\n","        line=dict(color=color, width=4),\n","        showlegend=False\n","    ))\n","\n","# Add arrows for specific relationships\n","add_arrow(\"king\", \"man\", \"crimson\")\n","add_arrow(\"queen\", \"woman\", \"crimson\")\n","add_arrow(\"apple\", \"red\", \"olivedrab\")\n","add_arrow(\"banana\", \"yellow\", \"olivedrab\")\n","add_arrow(\"day\", \"sun\", \"orange\")\n","add_arrow(\"night\", \"moon\", \"orange\")\n","\n","\n","\n","fig.show()\n"],"metadata":{"id":"hgf2jzILNLrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","aliceblue, antiquewhite, aqua, aquamarine, azure,\n","beige, bisque, black, blanchedalmond, blue,\n","blueviolet, brown, burlywood, cadetblue,\n","chartreuse, chocolate, coral, cornflowerblue,\n","cornsilk, crimson, cyan, darkblue, darkcyan,\n","darkgoldenrod, darkgray, darkgrey, darkgreen,\n","darkkhaki, darkmagenta, darkolivegreen, darkorange,\n","darkorchid, darkred, darksalmon, darkseagreen,\n","darkslateblue, darkslategray, darkslategrey,\n","darkturquoise, darkviolet, deeppink, deepskyblue,\n","dimgray, dimgrey, dodgerblue, firebrick,\n","floralwhite, forestgreen, fuchsia, gainsboro,\n","ghostwhite, gold, goldenrod, gray, grey, green,\n","greenyellow, honeydew, hotpink, indianred, indigo,\n","ivory, khaki, lavender, lavenderblush, lawngreen,\n","lemonchiffon, lightblue, lightcoral, lightcyan,\n","lightgoldenrodyellow, lightgray, lightgrey,\n","lightgreen, lightpink, lightsalmon, lightseagreen,\n","lightskyblue, lightslategray, lightslategrey,\n","lightsteelblue, lightyellow, lime, limegreen,\n","linen, magenta, maroon, mediumaquamarine,\n","mediumblue, mediumorchid, mediumpurple,\n","mediumseagreen, mediumslateblue, mediumspringgreen,\n","mediumturquoise, mediumvioletred, midnightblue,\n","mintcream, mistyrose, moccasin, navajowhite, navy,\n","oldlace, olive, olivedrab, orange, orangered,\n","orchid, palegoldenrod, palegreen, paleturquoise,\n","palevioletred, papayawhip, peachpuff, peru, pink,\n","plum, powderblue, purple, red, rosybrown,\n","royalblue, rebeccapurple, saddlebrown, salmon,\n","sandybrown, seagreen, seashell, sienna, silver,\n","skyblue, slateblue, slategray, slategrey, snow,\n","springgreen, steelblue, tan, teal, thistle, tomato,\n","turquoise, violet, wheat, white, whitesmoke,\n","yellow, yellowgreen\n","\n","\"\"\""],"metadata":{"id":"3SolLFOgNLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["%%capture\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout f√ºr \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox f√ºr zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um Zeilenumbr√ºche aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und Erkl√§rung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">{{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from jinja2 import Template\n","\n","def json_to_html(json_data):\n","    # HTML-Template mit EINER Spalte f√ºr \"SEO\" (die \"alt\"-Spalte entf√§llt)\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            /* Zeilenumbr√ºche aus dem JSON in <br> wandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- \"Analyse\" normal untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">\n","                {{ sections.Analyse | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- \"Erkl√§rung\" normal untereinander -->\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">\n","                {{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- NUR noch die \"SEO\"-Spalte -->\n","            <div class=\"column\">\n","                <p class=\"header\">SEO</p>\n","                <p class=\"content\">\n","                    {{ sections.SEO | replace('\\\\n','<br>') | safe }}\n","                </p>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    template = Template(html_template)\n","    return template.render(data=json_data)\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"Cx6qfyVfJzI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschlie√üend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, R√§nder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthop√§din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\"\n","\n","\"Sie hat ihren Master of Science in Kieferorthop√§die von der Danube Private University, Krems, √ñsterreich, und hat an der Heinrich-Heine-Universit√§t D√ºsseldorf abgeschlossen.\"\n","\n","\"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln. \"\n","\n","\"behandlungsorientierte Zahnberatung\"\n","\n","\"√§stehthetisches L√§cheln\"\n","\n","\"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\" \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\"\n","\n","\"Der aktuelle Text zur Zahnspangenpraxis\"\n","\n","\"Kieferorthop√§de in [Ihre Stadt]\""],"metadata":{"id":"fH4mRdPXceeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u"},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def chunk_text_2(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    F√ºr echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text_2(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk √§hneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenf√ºgen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","# test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","# retrieved_context = get_context_from_vector_store(test_chunk)\n","# print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu pr√ºfen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu ber√ºcksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu pr√ºfenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu pr√ºfen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur Pr√ºfung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text gr√ºndlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu ver√§ndern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zus√§tzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein g√ºltiges JSON zur√ºckgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine g√ºltige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","all_corrected_texts = []\n","all_new_mistakes = []\n","\n","#for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","#    corrected_text_chunks = []\n","\n","#    for chunk in text_chunks:\n","#        # 3a) Kontext abfragen\n","#        context = get_context_from_vector_store(chunk)\n","#\n","#\n","#       # 4a) Prompt ChatGPT (Korrektur)\n","#        result = proofread_text_with_context(chunk, context)\n","#\n","#        corrected_text = result[\"corrected_text\"]\n","#        new_mistakes = result[\"new_mistakes_found\"]\n","#\n","#        # Sammeln\n","#        corrected_text_chunks.append(corrected_text)\n","#        all_new_mistakes.extend(new_mistakes)\n","#\n","#    # Pro SEO-Text f√ºgen wir die korrigierten Chunks zusammen.\n","#    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","#    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# for _ in all_corrected_texts:\n","#   print(_)"],"metadata":{"id":"qIUE9Dfl5YzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3"},"execution_count":null,"outputs":[]}]}