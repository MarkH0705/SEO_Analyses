{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["EnjP4Zhat_8t","xaxxhKAgyeQq","JtMpu3BMyh6-","Pns8QF8VWrHR","TPz7fgRaRAUv","o38bsnD-EWcq","vNVT-lr5jejz","eHxAg4fuIc2T","cgPAMHODtAuZ","SJVtNtjnqvJV","BatTg4ZKvpZV","1zqvsEJvx33W","4v6-jGMeO9Zt","HraJ3_5SJHSv","hdqvYkNsg7fx","dr3XCOHn7Nc_","tCeeWN-UjXaI","lNqdhhAit9q9","nb1dof25T_TI","rRMI7Rf5Wsi_","MM7uu--42W4u"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyMfKx321FIDhbAxk/X/UKJP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìî READ_ME und TO_DO"],"metadata":{"id":"EnjP4Zhat_8t"}},{"cell_type":"markdown","source":["#### README"],"metadata":{"id":"xaxxhKAgyeQq"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/README.md'\n","# üöÄ SEO Automation Pipeline mit OpenAI & Retrieval (RAG)\n","\n","Dieses Projekt bietet eine **komplette End-to-End-Pipeline f√ºr die SEO-Optimierung von Websites**, inklusive **Web-Crawling, SEO-Analyse, KI-gest√ºtzter Text-Optimierung und Qualit√§tskontrolle**.\n","\n","Kern des Projekts sind **automatisierte Abl√§ufe**, die von der **Datengewinnung bis zur SEO-optimierten Textgenerierung** reichen.\n","Mithilfe von **OpenAI (ChatGPT)** und einer **Retrieval Augmented Generation (RAG)-Architektur** wird sichergestellt, dass die finalen Texte nicht nur **SEO-freundlich**, sondern auch **grammatikalisch korrekt und hochwertig** sind.\n","\n","## üìö Inhaltsverzeichnis\n","\n","- Features\n","- Projektstruktur\n","- Ablauf & Module\n","- Technologien\n","- Installation\n","- Nutzung\n","- Ziele\n","- Roadmap\n","\n","## ‚úÖ Features\n","\n","- üåê **Automatisiertes Web Crawling** (inkl. Filter f√ºr relevante Inhalte)\n","- ‚úçÔ∏è **Generierung von SEO-optimierten Texten** mithilfe der OpenAI API\n","- üß† **RAG-gest√ºtzte Fehlererkennung & Textkorrektur** mit Vektordatenbank (FAISS)\n","- üìä **Analyse der Optimierungsergebnisse** (Statistiken, √Ñhnlichkeiten, Visualisierungen)\n","- üìà **Keyword-Analyse und Keyword-Optimierung**\n","- üì¶ Ausgabe in **HTML und PDF** f√ºr Kunden\n","- üìä Umfangreiche **Datenvisualisierungen** (Wordclouds, Cosine Similarity, Keyword-Verteilung)\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10oR2bcugvN2MClp14ia7gnzMGX5b896t\" alt=\"SEO Heatmap\" width=\"600\">\n","\n","\n","\n","\n","## üóÇÔ∏è Projektstruktur\n","\n","```\n","SEO-Project/\n","‚îú‚îÄ‚îÄ data/                # Prompts, Fehler-Korrektur-Daten, weitere JSON Dateien\n","‚îú‚îÄ‚îÄ notebooks/           # Colab/Notebooks zum Starten und Entwickeln\n","‚îú‚îÄ‚îÄ output/              # Erzeugte Dateien (HTML, PDF, Bilder)\n","‚îÇ   ‚îú‚îÄ‚îÄ final           # Dokumente f√ºr Kunden (HTML, PDF)\n","‚îÇ   ‚îî‚îÄ‚îÄ images          # Visualisierungen\n","‚îú‚îÄ‚îÄ src/                # Source Code (Python-Klassen und Module)\n","‚îÇ   ‚îú‚îÄ‚îÄ webscraper.py    # Webscrawling und Text-Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ llmprocessor.py # Anbindung an OpenAI API, Keyword Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py       # Zentrale Chatbot-Klasse zur Kommunikation mit GPT\n","‚îÇ   ‚îú‚îÄ‚îÄ seoanalyzer.py   # Analyse und Auswertung der Texte\n","‚îÇ   ‚îú‚îÄ‚îÄ github.py        # Automatischer Upload ins GitHub Repo\n","‚îÇ   ‚îú‚îÄ‚îÄ utils.py         # Hilfsmodule (z.B. f√ºr Prompt-Management)\n","‚îÇ   ‚îî‚îÄ‚îÄ embeddingdemo.py# 3D Embedding- und Cosine Similarity Visualisierungen\n","‚îú‚îÄ‚îÄ tests/              # pytest der Hauptfunktionalit√§ten\n","‚îî‚îÄ‚îÄ requirements.txt    # Python-Abh√§ngigkeiten\n","```\n","\n","## ‚öôÔ∏è Ablauf & Module\n","\n","### 1. **Web Crawling**\n","- **src/webscraper.py**: Holt Inhalte von Webseiten, filtert irrelevante Seiten (z.B. Impressum, AGB).\n","\n","### 2. **SEO-Optimierung mit OpenAI**\n","- **src/llmprocessor.py**:\n","  - Extrahiert Keywords aus den Inhalten.\n","  - Optimiert die Texte f√ºr SEO mit gezielten Prompts.\n","\n","### 3. **Analyse & Visualisierung**\n","- **src/seoanalyzer.py**: Verarbeitet und analysiert die Original- und optimierten Texte.\n","\n","### 4. **GitHub Automation**\n","- **src/github.py**: L√§dt finale Ergebnisse in ein GitHub-Repo hoch.\n","\n","## üß∞ Technologien\n","\n","| Technologie                  | Beschreibung                                       |\n","|-----------------------------|---------------------------------------------------|\n","| Python                      | Hauptsprache                                       |\n","| OpenAI API (ChatGPT, GPT-4)  | Generative KI f√ºr SEO-Texte                       |\n","| FAISS                      | Vektorsuche f√ºr RAG und Text-Fehler                |\n","| Pandas, NumPy               | Datenanalyse und Verarbeitung                      |\n","| Matplotlib, Seaborn         | Visualisierungen                                   |\n","| Sentence Transformers       | Embedding-Erstellung f√ºr Vektordatenbank          |\n","| BeautifulSoup, Requests     | Webcrawling                                        |\n","| Google Colab                | Entwicklung und Ausf√ºhrung                        |\n","\n","## üöÄ Installation\n","\n","```bash\n","pip install -r requirements.txt\n","python -m spacy download de_core_news_sm\n","pip install faiss-cpu sentence-transformers openai wordcloud matplotlib seaborn\n","```\n","\n","## üíª Nutzung\n","\n","```python\n","scraper = WebsiteScraper(start_url=\"https://www.example.com\")\n","scraper.scrape_website()\n","\n","llm_processor = LLMProcessor(prompts_folder, get_filtered_texts, google_ads_keywords)\n","llm_processor.run_all()\n","\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n","```\n","\n","## üéØ Ziele\n","\n","- ‚úÖ Vollst√§ndige Automatisierung der SEO-Optimierung\n","- ‚úÖ RAG f√ºr sprachliche Qualit√§tskontrolle\n","- ‚úÖ Kundenfertige PDF/HTML-Reports\n","\n","## üöß Roadmap\n","\n","- [ ] **Produkt f√ºr Kunden finalisieren:** all-in-one solution f√ºr webcrawl + SEO + optimierten html code\n","- [ ] Automatische SEO Scores (z.B. Google Ads API)\n","- [ ] Automatische Keyword-Erweiterung\n","- [ ] Mehrsprachigkeit\n","- [ ] WordPress-Integration\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"443XYilDN9NA","executionInfo":{"status":"ok","timestamp":1743331371424,"user_tz":-120,"elapsed":734,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"f39e0e50-cef7-4696-907a-62566f8635b2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/README.md\n"]}]},{"cell_type":"markdown","source":["####TODO"],"metadata":{"id":"JtMpu3BMyh6-"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/TODO.md'\n","# To-Do Liste: SEO Automation & KI-Projekt\n","\n","Diese Liste fasst alle anstehenden Aufgaben im Projekt zusammen\n","\n","---\n","\n","## 0. **Aktuelles und dringendes**\n","- [ ] 18.3.2025 **Version missmatch**: numpy 2.2.3 und spacy 3.5 **side effects on**: dependencies.py, excelimporter.py, Installation.ipynb **ursachen**: spacy version 3.5 verlangt numpy 1.26.4 -> version missmatch\n","      -> 23.5. class SEOAnlyzer refaktoriert und spacy ersetzt\n","- [ ] 28.3.25 lokale SEO keywords liefern manchmal falsche Stadt\n","\n","---\n","\n","## 1. **Allgemeine Projektorganisation**\n","- [ ] **Projektstruktur verbessern**: Ordner √ºbersichtlich gestalten (z.B. `src/`, `data/`, `tests/`, `notebooks/`, dependencies.py).\n","- [ ] **Dokumentation erweitern**: READ_ME und Wiki (bzw. GitHub Pages) zu jedem Modul anlegen.\n","- [ ] **Automatisierte Tests** Pytest f√ºr Kernfunktionen ausbauen.\n","- [ ] **Produkt f√ºr Kunden finalisieren**\n","- [ ] **FAISS DB**: automatisierte Erweiterung bei neu gefundenen Fehlern\n","- [ ] **Template GitHub**: issues\n","- [ ] Funktionalit√§ten aus **utils.py** √ºberdenken\n","- [ ] langfristig Umstieg auf **langchain**\n","- [ ] textprocessor durch openai **function calling** ersetzen\n","- [ ] **dependencies** und versionen robuster machen\n","- [ ] **bug reporting system** einrichten\n","\n","---\n","\n","## 2. **Vector-Datenbank (FAISS) & Retrieval**\n","- [ ] **VectorDB-Klasse finalisieren**:\n","  - [ ] Kleinere Bugs beheben\n","  - [ ] Userfreundliche Methoden f√ºr neue Eintr√§ge\n","- [ ] **Einrichtung der DB** bei Projektstart (Neubau vs. Laden) vereinheitlichen\n","- [ ] **Konfigurierbare √Ñhnlichkeits-Schwelle** (z.B. `threshold=0.6`) besser dokumentieren\n","- [ ] **Dynamische Filter** f√ºr bestimmte Fehlerkategorien (z.B. Stil vs. Grammatik) √ºberlegen\n","- [ ] **hybrides system mit knowledge tree und RAG** etablieren\n","\n","---\n","\n","## 3. **SEO-Optimierungs-Pipeline**\n","- [ ] **Prompts in JSON-Dateien** verlagern (z.B. `/data/prompts/`) und sauber verlinken\n","- [ ] **Supervisor-Feedback** integrieren & QA-Schritte definieren\n","- [ ] Langchain und SEOPageOptimizer verbinden\n","\n","---\n","\n","## 4. **SEOGrammarChecker & PromptManager**\n","- [ ] Klassenrefactoring:\n","  - [ ] **`VectorDB`** vs. **`PromptManager`** vs. **`SEOGrammarChecker`** sauber trennen\n","  - [ ] M√∂glichst wenig Code-Duplikate, mehr modulare Testbarkeit\n","- [ ] **Konfigurationsdatei** (z.B. YAML) f√ºr Pfade, wie `FAISS_PATH` & Promptordner\n","- [ ] **Erweiterbare Prompt-Templates**:\n","  - [ ] Z.B. `seo_optimization.json`, `grammar_check.json`, `supervisor.json`, etc.\n","\n","---\n","\n","## 5. **Abschluss & Integration**\n","- [ ] **Dokumentation** aller Pipelines & Klassen in der README (oder in separater Doku)\n","- [ ] **Optionale WordPress-Integration** in der Zukunft (Ideenspeicher)\n","  - [ ] Upload via REST API\n","  - [ ] Metadaten (Title, Slug, Tags etc.)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBtOaSe-CHyO","executionInfo":{"status":"ok","timestamp":1743331371867,"user_tz":-120,"elapsed":431,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"6dd1b2ca-4813-4366-95cd-70d718c600f6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/TODO.md\n"]}]},{"cell_type":"markdown","source":["#globale Parameter\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","# START_URL = 'https://www.ewms-tech.com/'\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","# START_URL = 'https://www.malerarbeiten-koenig.de/'\n","\n","\n","EXCLUDED_WEBSITES = [\"impressum\", \"datenschutz\", \"datenschutzerkl√§rung\", \"agb\"]\n","\n","\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","\n","SRC_PATH, DATA_PATH, TEST_PATH, OUTPUT_PATH = PROJECT_ROOT + \"/src\", PROJECT_ROOT + \"/data\", PROJECT_ROOT + '/tests', PROJECT_ROOT + '/output'\n","FAISS_PATH = DATA_PATH + '/faiss_db'\n","KEYWORD_PATH = DATA_PATH + '/keywords'\n","PROMPT_PATH = DATA_PATH + '/prompts'\n","\n","\n","FINAL_IMAGES = {}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1743331388514,"user_tz":-120,"elapsed":16634,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"623d0142-3b0b-4ed3-c71d-d2afd42d5afb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo","executionInfo":{"status":"ok","timestamp":1743331639743,"user_tz":-120,"elapsed":251226,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# %run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5","executionInfo":{"status":"ok","timestamp":1743331639753,"user_tz":-120,"elapsed":5,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["import importlib\n","import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"],"metadata":{"id":"do4Bf0uw-y8U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"240d583a-c764-4f7d-b293-a79741aba6e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Klonen des GitHub-Repositories...\n"]}]},{"cell_type":"markdown","source":["# üï∏ crawl"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import importlib\n","import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_WEBSITES)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì∫ google ads seo keywords"],"metadata":{"id":"eHxAg4fuIc2T"}},{"cell_type":"code","source":["import excelimporter\n","importlib.reload(excelimporter)\n","from excelimporter import ExcelImporter\n","\n","importer = ExcelImporter(project_folder=PROJECT_ROOT, header=2)\n","keyword_df = importer.import_all()\n","\n","if keyword_df is not None:\n","    excluded_seo_keywords = ['spange de', 'kfo zentrum essen', 'gerade zahne', 'zahn spange']\n","\n","    keyword_df = keyword_df[(keyword_df['Avg. monthly searches'] > 10) &\n","    (~keyword_df['Keyword'].isin(excluded_seo_keywords))\n","    ].sort_values(by='Avg. monthly searches', ascending=False).reset_index(drop=True).copy()\n","\n","    google_ads_keywords = list(keyword_df['Keyword'])\n","\n","    keyword_df.set_index(keyword_df.columns[0], inplace=True)\n","    searches_df = keyword_df[[col for col in keyword_df.columns if col.startswith('Searches')]]\n","\n","else:\n","    print(\"Error: importer.import_all() returned None. Check your data file and ExcelImporter class.\")\n","    google_ads_keywords = None"],"metadata":{"id":"REeveZ8rwEys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keywordvisualizer\n","importlib.reload(keywordvisualizer)\n","from keywordvisualizer import KeywordVisualizer\n","\n","\n","visualizer = KeywordVisualizer(\n","    df=searches_df,\n","    output_dir=OUTPUT_PATH+'/images',\n","    image_paths=FINAL_IMAGES\n",")\n","visualizer.heatmap()"],"metadata":{"id":"LnJ5j0VZ_JWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"31sgkNSF_JTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XXzSL_SR_JRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sXLiqhti_JOm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆwebtext analysis + SEO"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["import llmprocessor\n","importlib.reload(llmprocessor)\n","from llmprocessor import LLMProcessor\n","\n","google_ads_keywords = None if not google_ads_keywords else google_ads_keywords\n","\n","llm_processor = LLMProcessor(PROMPT_PATH, original_texts, google_ads_keywords=google_ads_keywords)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"q3isJXZoebv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìÅ Textprozessor"],"metadata":{"id":"SJVtNtjnqvJV"}},{"cell_type":"code","source":["import json\n","import textprocessor\n","importlib.reload(textprocessor)\n","from textprocessor import TextProcessor\n","\n","# JSON mit den SEO-Abschnitten extrahieren\n","json_output = TextProcessor.extract_sections_to_json(list(optimized_texts.keys()), list(optimized_texts.values()))\n","seo_json = json.loads(json_output)\n","\n","# texte bereinigen und hinzuf√ºgen\n","seo_json = TextProcessor.add_cleaned_text(seo_json, original_texts)\n","\n","# Ergebnis anzeigen\n","print(json.dumps(seo_json, indent=4, ensure_ascii=False))\n"],"metadata":{"id":"SS1zb2sAWKIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîé SEO Analyses + Statistics"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["# k√ºnstliche SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"],"metadata":{"id":"1lkqnkWkw5wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","exclude_list = [\"leila\", \"graf\", \"koenig\", \"bjoern\", \"k√∂nig\", 'bj√∂rn', 'adolf', 'schmidt', 'strasse', 'stra√üe', 'tilo', 'remhof']\n","\n","keywords_final = json.loads(llm_processor.get_keywords()['keywords_final']) if not google_ads_keywords else google_ads_keywords\n","\n","seo_analyzer = SEOAnalyzer(\n","    seo_json=seo_json,\n","    keywords_final=keywords_final,\n","    output_dir=OUTPUT_PATH+\"/images\",\n","    historical_data=historical_data,\n","    wordcloud_exclude=exclude_list,\n","    shared_image_dict=FINAL_IMAGES\n",")\n","\n","analysis_paths = seo_analyzer.run_analysis()\n","print(\"Analysis Plot Pfade:\", analysis_paths)\n","\n","model_paths = seo_analyzer.run_models()\n","print(\"Model Plot Pfade:\", model_paths)\n","\n","all_paths = seo_analyzer.get_all_image_paths()\n","print(\"Alle Pfade gesammelt:\", all_paths)\n"],"metadata":{"id":"MtZrcL_wxKzk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üõè embedding demo"],"metadata":{"id":"1zqvsEJvx33W"}},{"cell_type":"code","source":["import os\n","import embeddingdemo\n","importlib.reload(embeddingdemo)\n","from embeddingdemo import EmbeddingDemo\n","\n","demo = EmbeddingDemo(output_dir=OUTPUT_PATH+'/images', final_images=FINAL_IMAGES)\n","demo.run_all_visualizations()\n","\n","print(\"Alle Embedding-Bilder:\", demo.get_image_paths())\n","print(\"Gemeinsame Bilder (shared dict):\", FINAL_IMAGES)"],"metadata":{"id":"toL_RwWuO72N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["%%capture\n","import seoreportexporter\n","importlib.reload(seoreportexporter)\n","from seoreportexporter import SEOReportExporter\n","exporter = SEOReportExporter(seo_json, OUTPUT_PATH, DATA_PATH+\"/intro_texts/sections_intro.json\", FINAL_IMAGES)\n","exporter.run_all_exports()"],"metadata":{"id":"ByLZAW6rSbwF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• Tests"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"markdown","source":["#### üìÉ Doku"],"metadata":{"id":"hdqvYkNsg7fx"}},{"cell_type":"code","source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/tests/DOKU_TESTS.md'\n","# ‚úÖ Pytest Template: Chatbot Klasse\n","\n","## 1. Klassen & Methoden die getestet werden sollen\n","\n","- **Chatbot**\n","  - `chat()`\n","  - `chat_with_streaming()`\n","\n","---\n","\n","## 2. Beispielhafte Inputs + erwartete Outputs pro Methode\n","\n","| Methode                      | Beispiel Input                                                           | Erwartete Ausgabe    |\n","|-----------------------------|-------------------------------------------------------------------------|----------------------|\n","| `Chatbot.chat()`             | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'           | \"Test erfolgreich\"   |\n","| `Chatbot.chat_with_streaming()` | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'         | \"Test erfolgreich\"   |\n","\n","---\n","\n","## 3. Return-Typen der Methoden\n","\n","| Methode                      | R√ºckgabe-Typ |\n","|-----------------------------|--------------|\n","| `Chatbot.chat()`             | `str`        |\n","| `Chatbot.chat_with_streaming()` | `str`     |\n","\n","---\n","\n","## 4. Externe Services mocken?\n","\n","| Service         |  Mocken?                           |\n","|-----------------|-------------------------------------|\n","| OpenAI API      |  Nein                                |\n","| FAISS Index     |  Ja (kleine Test-Datenbank f√ºr FAISS) |\n","\n","---\n","\n","## 5. Ordnerstruktur f√ºr Tests\n","\n","```bash\n","/project-root/\n","    /src/\n","        chatbot.py\n","    /tests/\n","        test_chatbot.py\n","    /logs/\n","        test_report.log\n","    ...\n","```\n","\n","---\n"],"metadata":{"id":"V1xV8mVljdkL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### üë∑ code"],"metadata":{"id":"dr3XCOHn7Nc_"}},{"cell_type":"code","source":["import pytest\n","pytest.main(['-v', TEST_PATH+'/chatbot_test/chatbot_test.py'])"],"metadata":{"id":"n68k1npffFsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® Error Collection"],"metadata":{"id":"tCeeWN-UjXaI"}},{"cell_type":"code","source":["error_corrections = {\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\":\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, sowie Kau- und Sprechprobleme effektiv behandeln.\",\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\":\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch und Swahili. Dazu hat sie Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\",\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf abgeschlossen.\":\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf promoviert.\",\n","    \"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln.\":\n","    \"Sie ist hoch qualifiziert und hat eine besondere Hingabe zu einem √§sthetischen L√§cheln.\",\n","    \"behandlungsorientierte Zahnberatung\": \"patientenorientierte Beratung\",\n","    \"√§stehthetisches L√§cheln\": \"√§sthetisches L√§cheln\",\n","    \"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\": \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\",\n","    \"Der aktuelle Text zur Zahnspangenpraxis\": \"Der aktuelle Text zur kieferorthop√§dischen Praxis\"\n","}"],"metadata":{"id":"Y_KjsdzFHBpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_error_corrections = {\"Das ist ein neuer Fehler.\": \"Das ist ein korrigierter Fehler.\"}"],"metadata":{"id":"AQJFBU_9-T9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üé® RAG"],"metadata":{"id":"lNqdhhAit9q9"}},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 1) VectorDB\n","# -------------------------\n","\n","test_text = seo_json[list(seo_json.keys())[0]][\"SEO\"]\n","\n","class VectorDB:\n","    \"\"\"\n","    Eine Klasse f√ºr alles rund um die Vektordatenbank:\n","    - Aufbauen & Laden (FAISS)\n","    - Neue Eintr√§ge hinzuf√ºgen\n","    - Querying f√ºr Context Retrieval\n","    \"\"\"\n","\n","    def __init__(self, db_folder):\n","        \"\"\"\n","        :param db_folder: Pfad zum Datenbank-Ordner\n","        \"\"\"\n","        self.db_folder = db_folder\n","        self.index_file = os.path.join(db_folder, \"faiss_index.bin\")\n","        self.json_file  = os.path.join(db_folder, \"faiss_index.json\")\n","\n","        self.index = None\n","        self.error_dict = {}\n","\n","        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    def build_index(self, error_corrections: dict):\n","        \"\"\"\n","        Baut einen neuen FAISS-Index aus den √ºbergebenen Fehler-Korrektur-Paaren.\n","        \"\"\"\n","        print(\"üî® Baue neuen FAISS-Index...\")\n","        os.makedirs(self.db_folder, exist_ok=True)\n","\n","        self.error_dict = error_corrections\n","        errors = list(self.error_dict.keys())\n","\n","        # Embeddings\n","        embeddings = np.array([self.model.encode(e) for e in errors], dtype=\"float32\")\n","\n","        # FAISS-Index anlegen\n","        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n","        self.index.add(embeddings)\n","\n","        # Daten auf Festplatte schreiben\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ Neuer Index + JSON in '{self.db_folder}' erstellt.\")\n","\n","    def load_index(self):\n","        \"\"\"\n","        L√§dt einen bereits existierenden FAISS-Index und die Fehler-Daten.\n","        \"\"\"\n","        if not (os.path.exists(self.index_file) and os.path.exists(self.json_file)):\n","            raise FileNotFoundError(\"‚ùå Kein FAISS-Index gefunden. Bitte build_index() aufrufen.\")\n","\n","        print(\"üîé Lade vorhandenen FAISS-Index...\")\n","        self.index = faiss.read_index(self.index_file)\n","\n","        with open(self.json_file, \"r\", encoding=\"utf-8\") as f:\n","            self.error_dict = json.load(f)\n","\n","        print(\"‚úÖ Index & Fehler-Korrekturen geladen.\")\n","\n","    def add_entries(self, new_error_corrections: dict):\n","        \"\"\"\n","        F√ºgt weitere Fehler-Korrektur-Paare hinzu, ohne alles neu zu bauen.\n","        \"\"\"\n","        if self.index is None:\n","            # Versuch zu laden, falls vorhanden\n","            if os.path.exists(self.index_file) and os.path.exists(self.json_file):\n","                self.load_index()\n","            else:\n","                raise FileNotFoundError(\"‚ùå Kein Index vorhanden. Bitte erst build_index() nutzen.\")\n","\n","        # Merge in self.error_dict\n","        for fehler, korrektur in new_error_corrections.items():\n","            self.error_dict[fehler] = korrektur\n","\n","        # embeddings nur f√ºr die neuen keys\n","        new_keys = list(new_error_corrections.keys())\n","        new_embeds = np.array([self.model.encode(k) for k in new_keys], dtype=\"float32\")\n","\n","        # An Index anh√§ngen\n","        self.index.add(new_embeds)\n","\n","        # Speichern\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ {len(new_keys)} neue Eintr√§ge hinzugef√ºgt und Index aktualisiert.\")\n","\n","    def query(self, text: str, top_k=3, threshold=0.6):\n","        \"\"\"\n","        Sucht in der DB nach √§hnlichen fehlerhaften Formulierungen.\n","\n","        :param text: Der zu pr√ºfende Satz/Abschnitt\n","        :param top_k: Anzahl der gesuchten √Ñhnlichkeiten\n","        :param threshold: Distanzschwelle\n","        :return: Liste [(fehler, korrektur), ...]\n","        \"\"\"\n","        if self.index is None:\n","            self.load_index()\n","\n","        embed = np.array([self.model.encode(text)], dtype=\"float32\")\n","        distances, indices = self.index.search(embed, top_k)\n","\n","        all_errors = list(self.error_dict.keys())\n","\n","        results = []\n","        for i in range(top_k):\n","          idx = indices[0][i]\n","          # Sicherstellen, dass idx in den Bereich von all_errors passt\n","          if idx < len(all_errors):\n","              if distances[0][i] < threshold:\n","                  fehler_key = all_errors[idx]\n","                  korrektur = self.error_dict[fehler_key]\n","                  results.append((fehler_key, korrektur))\n","        return results\n","\n","\n","    def retrieve_context(self, seo_text: str) -> str:\n","        \"\"\"\n","        Durchsucht den seo_text Satz f√ºr Satz, holt ggf. Korrekturvorschl√§ge\n","        und baut einen Kontextstring.\n","        \"\"\"\n","        lines = []\n","        for s in seo_text.split(\". \"):\n","            suggestions = self.query(s)\n","            for old, new in suggestions:\n","                lines.append(f\"- Fehler: {old} ‚ûù Verbesserung: {new}\")\n","\n","        if lines:\n","            return \"Bekannte Fehler/Korrekturen:\\n\" + \"\\n\".join(lines)\n","        else:\n","            return \"Keine bekannten Fehler gefunden.\"\n","\n","\n","\n","db = VectorDB(db_folder=FAISS_PATH)\n","db.build_index(error_corrections)\n","db.add_entries(new_error_corrections)\n","db.retrieve_context(test_text)"],"metadata":{"id":"alRDUdT4I7o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 2) PromptManager\n","# -------------------------\n","\n","class PromptManager:\n","    \"\"\"\n","    L√§dt Prompts aus dem /data/prompts Ordner und kombiniert sie mit\n","    dem Context aus der VectorDB, um einen finalen Prompt zu erstellen.\n","    \"\"\"\n","\n","    def __init__(self, prompts_folder=\"./data/prompts\"):\n","        \"\"\"\n","        :param prompts_folder: Ordner, in dem .json (oder .txt) Prompts liegen\n","        \"\"\"\n","        self.prompts_folder = prompts_folder\n","\n","    def load_prompt(self, filename: str) -> dict:\n","        \"\"\"\n","        L√§dt einen JSON-Prompt aus dem Ordner, z.B. 'grammar_prompt.json'.\n","        \"\"\"\n","        path = os.path.join(self.prompts_folder, filename)\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                return json.load(f)\n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è Prompt-Datei {path} nicht gefunden!\")\n","            return {}\n","        except json.JSONDecodeError:\n","            print(f\"‚ö†Ô∏è Ung√ºltiges JSON in {path}\")\n","            return {}\n","\n","    def build_final_prompt(self, base_prompt_file: str, context: str, user_text: str) -> (str, str):\n","        \"\"\"\n","        Kombiniert:\n","         - base_prompt_file (System-/User-Prompts)\n","         - den 'context' aus der VectorDB\n","         - den 'user_text' (SEO-Text)\n","        und gibt final (system_prompt, user_prompt) zur√ºck.\n","        \"\"\"\n","        prompt_data = self.load_prompt(base_prompt_file)\n","\n","        system_prompt = prompt_data.get(\"system_prompt\", \"\")\n","        user_prompt   = prompt_data.get(\"user_prompt\", \"\")\n","\n","        # Kontext an system_prompt anh√§ngen\n","        system_prompt_full = system_prompt\n","\n","        # SEO-Text an user_prompt anh√§ngen\n","        user_prompt_full = user_prompt.format(context=context,optimized_text=user_text)\n","\n","        return (system_prompt_full, user_prompt_full)\n","\n","pm = PromptManager(prompts_folder=PROMPT_PATH)\n","context = db.retrieve_context(test_text)\n","final_prompts = pm.build_final_prompt(\"grammar_check.json\", context, test_text)"],"metadata":{"id":"-TWN0JGYI7l0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from chatbot import Chatbot\n","\n","# -------------------------\n","# 3) SEOGrammarChecker\n","# -------------------------\n","\n","cb = Chatbot(systemprompt=final_prompts[0], userprompt=final_prompts[1])\n","final_text = cb.chat()\n","final_text"],"metadata":{"id":"ahrShDjzI7iz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õì Langchain"],"metadata":{"id":"nb1dof25T_TI"}},{"cell_type":"code","source":["import subprocess\n","from langchain_openai import ChatOpenAI\n","from google.colab import userdata\n","import os\n","\n","subprocess.run([\"pip\", \"install\", \"--upgrade\", \"pydantic\"])\n","\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","llm = ChatOpenAI(temperature=0,\n","    model = \"gpt-4o-mini-2024-07-18\",\n","    openai_api_key=os.environ['OPENAI_API_KEY'],\n",")"],"metadata":{"id":"VTzMyOlEUh6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing_extensions import assert_type\n","from utils import load_prompts\n","\n","prompts = load_prompts(PROMPT_PATH + '/optimize_seo.json')\n","\n","\n","system_prompt = prompts[\"system_prompt\"]\n","user_prompt = prompts[\"user_prompt\"]\n","\n","test_prompt = f\"\"\"{prompts[\"user_prompt\"]}\"\"\"\n","#test_prompt = test_prompt.replace('{keywords}', google_ads_keywords.__str__())\n","\n","user_prompt = user_prompt.replace('{keywords}', google_ads_keywords.__str__())\n","user_prompt = user_prompt.replace('{original_text}', seo_json[START_URL]['SEO'])\n","\n","\n","print(test_prompt)"],"metadata":{"id":"-kjb474j49MV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n","\n","def extract_keywords(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein SEO-Experte, spezialisiert auf Keyword-Recherche.\"),\n","        HumanMessage(content=f\"\"\"\n","        Analysiere den folgenden Unternehmens-Text und finde die besten SEO-Keywords.\n","        Ber√ºcksichtige lokale Infos, falls vorhanden.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir eine Liste von Keywords.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"4tpet7NjXCgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_text_for_seo(text, keywords):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein professioneller SEO-Texter.\"),\n","        HumanMessage(content=f\"\"\"\n","        Optimiere den folgenden Text f√ºr SEO, indem du diese Keywords sinnvoll integrierst:\n","\n","        Keywords: {keywords}\n","\n","        Achte auf nat√ºrliche Sprache, gute Lesbarkeit und Vermeidung von Keyword-Stuffing.\n","\n","        Text:\n","        {text}\n","\n","        Gib mir den optimierten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text, keywords=keywords)  # Pass keywords here\n","    response = llm(messages)\n","    return response.content\n"],"metadata":{"id":"6kpgSaXrY240"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grammar_and_style_check(text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein erfahrener Lektor und Sprachexperte.\"),\n","        HumanMessage(content=f\"\"\"\n","        Pr√ºfe den folgenden Text auf Grammatik, Rechtschreibung und Stil.\n","        Mache den Text fl√ºssig, professionell und fehlerfrei.\n","\n","        Text:\n","        {text}\n","\n","        Gib den verbesserten Text zur√ºck.\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(text=text)  # Format with text\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"BJ-Q38kFY5lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def supervisor_check(original_text, keywords, optimized_text, final_text):\n","    prompt = ChatPromptTemplate.from_messages([\n","        SystemMessage(content=\"Du bist ein Supervisor, der SEO- und Textqualit√§t √ºberpr√ºft.\"),\n","        HumanMessage(content=f\"\"\"\n","        Hier sind die Arbeitsschritte:\n","\n","        Urspr√ºnglicher Text:\n","        {original_text}\n","\n","        Gefundene Keywords:\n","        {keywords}\n","\n","        SEO-optimierter Text:\n","        {optimized_text}\n","\n","        Finaler Text (nach Lektorat):\n","        {final_text}\n","\n","        Beantworte:\n","        1. Sind alle wichtigen Keywords sinnvoll eingebaut?\n","        2. Ist der Text professionell und lesbar?\n","        3. Verbesserungsvorschl√§ge?\n","        Wenn alles gut ist, antworte: 'Finaler Text akzeptiert.'\n","        \"\"\")\n","    ])\n","    # format_messages converts the messages to a list of dictionaries\n","    messages = prompt.format_messages(\n","        original_text=original_text,\n","        keywords=keywords,\n","        optimized_text=optimized_text,\n","        final_text=final_text\n","    )  # Format with all variables\n","    response = llm(messages)\n","    return response.content"],"metadata":{"id":"Um1Q9boRY8rY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seo_pipeline(original_text):\n","    # Keywords finden\n","    print(\"Schritt 1: Keywords finden...\")\n","    keywords = extract_keywords(original_text)\n","    print(\"Gefundene Keywords:\", keywords)\n","\n","    # SEO-Optimierung\n","    print(\"\\nSchritt 2: SEO-Optimierung...\")\n","    optimized_text = optimize_text_for_seo(original_text, keywords)\n","    print(\"SEO-optimierter Text:\\n\", optimized_text)\n","\n","    # Grammatikpr√ºfung\n","    print(\"\\nSchritt 3: Grammatikpr√ºfung...\")\n","    final_text = grammar_and_style_check(optimized_text)\n","    print(\"Finaler Text nach Lektorat:\\n\", final_text)\n","\n","    # Supervisor\n","    print(\"\\nSupervisor pr√ºft...\")\n","    supervisor_feedback = supervisor_check(original_text, keywords, optimized_text, final_text)\n","    print(\"Supervisor Feedback:\\n\", supervisor_feedback)\n","\n","    return final_text\n"],"metadata":{"id":"_T9EbqsEZBJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    unternehmens_text = seo_json[START_URL]['alt']\n","\n","    final_output = seo_pipeline(unternehmens_text)\n","    print(\"\\n--- Finaler SEO-optimierter Text ---\\n\")\n","    print(final_output)"],"metadata":{"id":"VlJfLxlpO9ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìü all-in-one solution SEO Texte + html"],"metadata":{"id":"rRMI7Rf5Wsi_"}},{"cell_type":"code","source":["import os\n","import re\n","import json\n","import logging\n","import requests\n","import chardet\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin, urlparse\n","from datetime import datetime\n","\n","from chatbot import Chatbot\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","class SEOPageOptimizer:\n","    \"\"\"\n","    Diese Klasse l√§dt HTML, extrahiert alle sichtbaren Texte (inkl. Link-/Accordion-Attribute),\n","    sendet alles in einem St√ºck an ChatGPT (mit Markern),\n","    parsed die Antwort in Bl√∂cke und ersetzt sie im DOM.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 output_dir=\"output/final\",\n","                 prompts_file=\"data/prompts/seo_prompts.json\",\n","                 google_ads_keywords=\"\"\n","    ):\n","        \"\"\"\n","        :param output_dir: Ordner f√ºr die optimierte HTML-Ausgabe.\n","        :param prompts_file: JSON-Datei mit allen Prompt-Texten (system_prompt & user_prompt_template).\n","        :param google_ads_keywords: String mit deinen Google Ads Keywords, kommasepariert etc.\n","        \"\"\"\n","        self.output_dir = output_dir\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        self.prompts = self.load_prompts(prompts_file)\n","\n","        # Neu: hier speicherst du die google ads keywords\n","        self.google_ads_keywords = google_ads_keywords\n","\n","    def load_prompts(self, prompts_file):\n","        logging.info(f\"Lade Prompts aus Datei: {prompts_file}\")\n","        if not os.path.exists(prompts_file):\n","            logging.warning(f\"Prompt-Datei {prompts_file} nicht gefunden. R√ºckgabe eines leeren Dict.\")\n","            return {}\n","        with open(prompts_file, \"r\", encoding=\"utf-8\") as f:\n","            return json.load(f)\n","\n","    def fetch_html(self, url: str) -> str:\n","        logging.info(f\"Lade HTML (raw) von {url}...\")\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()\n","\n","        raw_data = response.content\n","        detected = chardet.detect(raw_data)\n","        encoding = detected[\"encoding\"] or \"utf-8\"\n","        logging.info(f\"chardet sagt: {encoding} (Confidence: {detected['confidence']})\")\n","\n","        # forced decode\n","        text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","        # Optional: override\n","        response.encoding = 'utf-8'\n","        text_data = response.text\n","\n","        logging.info(\"HTML erfolgreich geladen und manuell dekodiert.\")\n","        return text_data\n","\n","    def extract_dom_texts(self, html: str):\n","        soup = BeautifulSoup(html, \"html.parser\")\n","\n","        title_tag = soup.find(\"title\")\n","        meta_desc_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n","        title_text = title_tag.get_text(strip=True) if title_tag else \"\"\n","        meta_desc_text = meta_desc_tag.get(\"content\", \"\").strip() if meta_desc_tag else \"\"\n","\n","        text_blocks = []\n","        block_id_num = 1\n","\n","        # Body-Texte\n","        for tag_name in [\"p\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"li\"]:\n","            for elem in soup.find_all(tag_name):\n","                txt = elem.get_text(separator=\" \", strip=True)\n","                if txt:\n","                    block_id = f\"BLOCK_{block_id_num:03d}\"\n","                    text_blocks.append({\n","                        \"id\": block_id,\n","                        \"elem\": elem,\n","                        \"source\": \"body\",\n","                        \"original_text\": txt,\n","                        \"optimized_text\": None\n","                    })\n","                    block_id_num += 1\n","\n","        # Link-Attribute: a[title]\n","        for a_tag in soup.find_all(\"a\"):\n","            if a_tag.has_attr(\"title\"):\n","                txt = a_tag[\"title\"].strip()\n","                if txt:\n","                    block_id = f\"BLOCK_{block_id_num:03d}\"\n","                    text_blocks.append({\n","                        \"id\": block_id,\n","                        \"elem\": a_tag,\n","                        \"source\": \"attribute\",\n","                        \"attr_name\": \"title\",\n","                        \"original_text\": txt,\n","                        \"optimized_text\": None\n","                    })\n","                    block_id_num += 1\n","\n","        # Accordion-Attribute: data-accordion-title\n","        for div_tag in soup.find_all(attrs={\"data-accordion-title\": True}):\n","            txt = div_tag[\"data-accordion-title\"].strip()\n","            if txt:\n","                block_id = f\"BLOCK_{block_id_num:03d}\"\n","                text_blocks.append({\n","                    \"id\": block_id,\n","                    \"elem\": div_tag,\n","                    \"source\": \"attribute\",\n","                    \"attr_name\": \"data-accordion-title\",\n","                    \"original_text\": txt,\n","                    \"optimized_text\": None\n","                })\n","                block_id_num += 1\n","\n","        logging.info(f\"Blocks gesamt: {len(text_blocks)} | Title: {title_text} | Meta: {meta_desc_text}\")\n","        return text_blocks, title_text, meta_desc_text, soup\n","\n","    def build_big_string(self, text_blocks):\n","        \"\"\"\n","        Baut 1 gro√üen String mit Markern:\n","        ---BLOCK_001_START---\n","        Original text...\n","        ---BLOCK_001_END---\n","        \"\"\"\n","        lines = []\n","        for block in text_blocks:\n","            lines.append(f\"---{block['id']}_START---\")\n","            lines.append(block[\"original_text\"])\n","            lines.append(f\"---{block['id']}_END---\")\n","        return \"\\n\".join(lines)\n","\n","    def generate_llm_text(self, agent_key, **kwargs):\n","        \"\"\"\n","        Liest system_prompt + user_prompt_template aus self.prompts[agent_key]\n","        => formatiert user_prompt_template\n","        => Chatbot => text\n","        \"\"\"\n","        if agent_key not in self.prompts:\n","            logging.warning(f\"Agent-Key '{agent_key}' nicht im JSON. Return ''\")\n","            return \"\"\n","        system_prompt = self.prompts[agent_key].get(\"system_prompt\",\"\")\n","        user_template = self.prompts[agent_key].get(\"user_prompt_template\",\"\")\n","        if not user_template:\n","            logging.warning(f\"Kein 'user_prompt_template' f√ºr agent_key '{agent_key}'. Return leer.\")\n","            return \"\"\n","\n","        user_prompt = user_template.format(**kwargs)\n","        cb = Chatbot(systemprompt=system_prompt, userprompt=user_prompt)\n","        response = cb.chat()\n","        return response.strip()\n","\n","    def parse_llm_response(self, llm_text):\n","        \"\"\"\n","        Regex => parse blocks:\n","        ---BLOCK_001_START---\n","        ... Neuer Text ...\n","        ---BLOCK_001_END---\n","        \"\"\"\n","        pattern = re.compile(r'---(BLOCK_\\d+)_START---\\s*(.*?)\\s*---\\1_END---', re.DOTALL)\n","        found = pattern.findall(llm_text)\n","        return found  # [(\"BLOCK_001\",\"NeuerText1\"), ...]\n","\n","    def do_large_text_optimization(self, text_blocks):\n","        \"\"\"\n","        1) Fulltext => ChatGPT => parse => store in text_blocks\n","        => google_ads_keywords => in Prompt\n","        \"\"\"\n","        full_text = self.build_big_string(text_blocks)\n","        logging.info(\"Sende Full-Text an ChatGPT (+ Google Ads Keywords) f√ºr globale Optimierung...\")\n","\n","        llm_output = self.generate_llm_text(\n","            \"large_text_optimization\",\n","            full_text=full_text,\n","            google_ads_keywords=self.google_ads_keywords  # <--- !!!\n","        )\n","\n","        block_results = self.parse_llm_response(llm_output)\n","        block_map = { b[\"id\"]: b for b in text_blocks }\n","        for b_id, new_txt in block_results:\n","            if b_id in block_map:\n","                block_map[b_id][\"optimized_text\"] = new_txt.strip()\n","        return text_blocks\n","\n","    def do_title_meta(self, text_blocks):\n","        \"\"\"\n","        Title & Meta => optional\n","        \"\"\"\n","        pass\n","\n","    def inject_content(self, soup, text_blocks, new_title=None, new_desc=None):\n","        logging.info(\"Injektion in HTML...\")\n","\n","        for block in text_blocks:\n","            opt_txt = block[\"optimized_text\"]\n","            if not opt_txt:\n","                continue\n","            elem = block[\"elem\"]\n","            if block[\"source\"] == \"body\":\n","                elem.clear()\n","                elem.append(opt_txt)\n","            elif block[\"source\"] == \"attribute\":\n","                elem[block[\"attr_name\"]] = opt_txt\n","\n","        if new_title:\n","            title_tag = soup.find(\"title\")\n","            if not title_tag:\n","                head = soup.find(\"head\")\n","                if head:\n","                    title_tag = soup.new_tag(\"title\")\n","                    head.append(title_tag)\n","            if title_tag:\n","                title_tag.string = new_title\n","\n","        if new_desc:\n","            desc_tag = soup.find(\"meta\", attrs={\"name\":\"description\"})\n","            if not desc_tag:\n","                head = soup.find(\"head\")\n","                if head:\n","                    new_meta = soup.new_tag(\"meta\", attrs={\"name\":\"description\",\"content\":new_desc})\n","                    head.append(new_meta)\n","            else:\n","                desc_tag[\"content\"] = new_desc\n","\n","        return soup\n","\n","    def save_html(self, soup, filepath):\n","        logging.info(f\"Speichere optimiertes HTML unter: {filepath}\")\n","        # meta charset utf-8 einf√ºgen\n","        head = soup.find(\"head\")\n","        if head and not head.find(\"meta\", attrs={\"charset\": True}):\n","            meta_charset = soup.new_tag(\"meta\", charset=\"utf-8\")\n","            head.insert(0, meta_charset)\n","\n","        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","            f.write(str(soup))\n","        logging.info(\"Datei gespeichert.\")\n","\n","    def optimize_page(self, url: str, outfile: str):\n","        logging.info(f\"Starte Optimierung f√ºr {url} => {outfile}\")\n","\n","        html = self.fetch_html(url)\n","        text_blocks, old_title, old_desc, soup = self.extract_dom_texts(html)\n","        text_blocks = self.do_large_text_optimization(text_blocks)\n","        # optional: new_title, new_desc = self.do_title_meta(text_blocks)...\n","\n","        new_soup = self.inject_content(soup, text_blocks, new_title=old_title, new_desc=old_desc)\n","        self.save_html(new_soup, outfile)\n","        logging.info(\"Fertig.\")\n"],"metadata":{"id":"m3EQshYWs5pj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from seopageoptimizer import SEOPageOptimizer\n","\n","optimizer = SEOPageOptimizer(\n","    output_dir=OUTPUT_PATH + \"/final\",\n","    prompts_file=PROMPT_PATH + \"/seo_prompts_2.json\",\n","    google_ads_keywords=keywords_final\n",")\n","\n","url_to_optimize = START_URL\n","outfile = OUTPUT_PATH + \"/final/optimized.html\"\n","\n","optimizer.optimize_page(url_to_optimize, outfile)\n","print(\"Done! File at:\", outfile)\n"],"metadata":{"id":"zxMaldbmjEx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r6oSSBn4qZox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ü•ºLab"],"metadata":{"id":"MM7uu--42W4u"}},{"cell_type":"code","source":["import os\n","import re\n","import json\n","import logging\n","import requests\n","import chardet\n","from bs4 import BeautifulSoup\n","from datetime import datetime\n","\n","from chatbot import Chatbot\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","class SEOPageOptimizer:\n","    \"\"\"\n","    Diese Klasse extrahiert die Texte in Bl√∂cken, l√§sst ChatGPT:\n","     - globale Analyse (analysis_original_text)\n","     - blockweise rewriting (large_text_optimization)\n","     - globale Beschreibung der √Ñnderungen (describe_improvements)\n","    und baut eine final HTML + JSON-Report.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 output_dir=\"output/final\",\n","                 prompts_file=\"data/prompts/seo_prompts.json\",\n","                 google_ads_keywords=\"\"\n","    ):\n","        self.output_dir = output_dir\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        self.prompts = self.load_prompts(prompts_file)\n","        self.google_ads_keywords = google_ads_keywords\n","\n","    def load_prompts(self, prompts_file):\n","        logging.info(f\"Lade Prompts aus Datei: {prompts_file}\")\n","        if not os.path.exists(prompts_file):\n","            logging.warning(f\"Prompt-Datei {prompts_file} nicht gefunden! R√ºckgabe leeres Dict.\")\n","            return {}\n","        with open(prompts_file, \"r\", encoding=\"utf-8\") as f:\n","            return json.load(f)\n","\n","    def fetch_html(self, url: str) -> str:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()\n","        raw_data = response.content\n","        detected = chardet.detect(raw_data)\n","        encoding = detected[\"encoding\"] or \"utf-8\"\n","        logging.info(f\"chardet sagt: {encoding}\")\n","        text_data = raw_data.decode(encoding, errors=\"replace\")\n","        return text_data\n","\n","    def extract_dom_texts(self, html: str):\n","        soup = BeautifulSoup(html, \"html.parser\")\n","        title_tag = soup.find(\"title\")\n","        meta_desc_tag = soup.find(\"meta\", attrs={\"name\":\"description\"})\n","        title_text = title_tag.get_text(strip=True) if title_tag else \"\"\n","        meta_desc_text = meta_desc_tag.get(\"content\", \"\").strip() if meta_desc_tag else \"\"\n","\n","        text_blocks = []\n","        block_id_num = 1\n","\n","        # Body\n","        for tag_name in [\"p\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"li\"]:\n","            for elem in soup.find_all(tag_name):\n","                txt = elem.get_text(separator=\" \", strip=True)\n","                if txt:\n","                    block_id = f\"BLOCK_{block_id_num:03d}\"\n","                    text_blocks.append({\n","                        \"id\": block_id,\n","                        \"elem\": elem,\n","                        \"source\": \"body\",\n","                        \"original_text\": txt,\n","                        \"optimized_text\": None\n","                    })\n","                    block_id_num += 1\n","\n","        # Link-Attribute\n","        for a_tag in soup.find_all(\"a\"):\n","            if a_tag.has_attr(\"title\"):\n","                txt = a_tag[\"title\"].strip()\n","                if txt:\n","                    block_id = f\"BLOCK_{block_id_num:03d}\"\n","                    text_blocks.append({\n","                        \"id\": block_id,\n","                        \"elem\": a_tag,\n","                        \"source\": \"attribute\",\n","                        \"attr_name\": \"title\",\n","                        \"original_text\": txt,\n","                        \"optimized_text\": None\n","                    })\n","                    block_id_num += 1\n","\n","        # Accordion-Attribute\n","        for div_tag in soup.find_all(attrs={\"data-accordion-title\": True}):\n","            txt = div_tag[\"data-accordion-title\"].strip()\n","            if txt:\n","                block_id = f\"BLOCK_{block_id_num:03d}\"\n","                text_blocks.append({\n","                    \"id\": block_id,\n","                    \"elem\": div_tag,\n","                    \"source\": \"attribute\",\n","                    \"attr_name\": \"data-accordion-title\",\n","                    \"original_text\": txt,\n","                    \"optimized_text\": None\n","                })\n","                block_id_num += 1\n","\n","        logging.info(f\"Blocks gesamt: {len(text_blocks)}\")\n","        return text_blocks, title_text, meta_desc_text, soup\n","\n","    def build_big_string(self, text_blocks):\n","        \"\"\"\n","        Baut 1 gro√üen String => Original-Bl√∂cke\n","        \"\"\"\n","        lines = []\n","        for b in text_blocks:\n","            lines.append(f\"---{b['id']}_START---\")\n","            lines.append(b[\"original_text\"])\n","            lines.append(f\"---{b['id']}_END---\")\n","        return \"\\n\".join(lines)\n","\n","    def build_big_string_optimized(self, text_blocks):\n","        \"\"\"\n","        Dasselbe, aber mit optimized_text\n","        \"\"\"\n","        lines = []\n","        for b in text_blocks:\n","            text = b[\"optimized_text\"] or \"(keine Optimierung)\"\n","            lines.append(f\"---{b['id']}_START---\")\n","            lines.append(text)\n","            lines.append(f\"---{b['id']}_END---\")\n","        return \"\\n\".join(lines)\n","\n","    def generate_llm_text(self, agent_key, **kwargs):\n","        if agent_key not in self.prompts:\n","            logging.warning(f\"Agent-Key '{agent_key}' nicht in JSON. Return ''\")\n","            return \"\"\n","        system_prompt = self.prompts[agent_key].get(\"system_prompt\",\"\")\n","        user_template = self.prompts[agent_key].get(\"user_prompt_template\",\"\")\n","        if not user_template:\n","            logging.warning(f\"Kein 'user_prompt_template' f√ºr agent_key '{agent_key}'. Return leer.\")\n","            return \"\"\n","\n","        user_prompt = user_template.format(**kwargs)\n","        cb = Chatbot(systemprompt=system_prompt, userprompt=user_prompt)\n","        response = cb.chat()\n","        return response.strip()\n","\n","    def parse_llm_response(self, llm_text):\n","        pattern = re.compile(r'---(BLOCK_\\d+)_START---\\s*(.*?)\\s*---\\1_END---', re.DOTALL)\n","        found = pattern.findall(llm_text)\n","        return found\n","\n","    # ================== 1) Globale Analyse ==================\n","    def do_pre_analysis(self, text_blocks):\n","        full_text = self.build_big_string(text_blocks)\n","        analysis = self.generate_llm_text(\"analysis_original_text\", full_text=full_text)\n","        return analysis\n","\n","    # ================== 2) SEO-Optimierung ==================\n","    def do_large_text_optimization(self, text_blocks):\n","        full_text = self.build_big_string(text_blocks)\n","        llm_out = self.generate_llm_text(\n","            \"large_text_optimization\",\n","            full_text=full_text,\n","            google_ads_keywords=self.google_ads_keywords\n","        )\n","        results = self.parse_llm_response(llm_out)\n","        block_map = { b[\"id\"]: b for b in text_blocks }\n","        for b_id, new_txt in results:\n","            block_map[b_id][\"optimized_text\"] = new_txt.strip()\n","        return text_blocks\n","\n","    # ================== 3) Globale Beschreibung =============\n","    def describe_improvements_global(self, text_blocks):\n","        \"\"\"\n","        Original => build_big_string\n","        Neu => build_big_string_optimized\n","        => 'describe_improvements' => 1 Gesamter Bericht\n","        \"\"\"\n","        original_str = self.build_big_string(text_blocks)\n","        optimized_str = self.build_big_string_optimized(text_blocks)\n","\n","        desc = self.generate_llm_text(\n","            \"describe_improvements\",\n","            original_text=original_str,\n","            optimized_text=optimized_str\n","        )\n","        return desc\n","\n","    # ================== 4) Re-Inject in HTML ================\n","    def inject_content(self, soup, text_blocks, new_title=None, new_desc=None):\n","        for b in text_blocks:\n","            opt_txt = b[\"optimized_text\"]\n","            if not opt_txt:\n","                continue\n","            elem = b[\"elem\"]\n","            if b[\"source\"] == \"body\":\n","                elem.clear()\n","                elem.append(opt_txt)\n","            elif b[\"source\"] == \"attribute\":\n","                attr_name = b[\"attr_name\"]\n","                elem[attr_name] = opt_txt\n","\n","        # Falls Title / Desc\n","        if new_title:\n","            title_tag = soup.find(\"title\")\n","            if not title_tag:\n","                head = soup.find(\"head\")\n","                if head:\n","                    title_tag = soup.new_tag(\"title\")\n","                    head.append(title_tag)\n","            if title_tag:\n","                title_tag.string = new_title\n","\n","        if new_desc:\n","            desc_tag = soup.find(\"meta\", attrs={\"name\":\"description\"})\n","            if not desc_tag:\n","                head = soup.find(\"head\")\n","                if head:\n","                    new_meta = soup.new_tag(\"meta\", attrs={\"name\":\"description\", \"content\": new_desc})\n","                    head.append(new_meta)\n","            else:\n","                desc_tag[\"content\"] = new_desc\n","\n","        # charset\n","        head = soup.find(\"head\")\n","        if head and not head.find(\"meta\", attrs={\"charset\": True}):\n","            meta_charset = soup.new_tag(\"meta\", charset=\"utf-8\")\n","            head.insert(0, meta_charset)\n","\n","        return soup\n","\n","    def save_html(self, soup, filepath):\n","        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","            f.write(str(soup))\n","        logging.info(f\"‚úÖ Datei gespeichert: {filepath}\")\n","\n","    # ================== 5) Hauptworkflow ====================\n","    def optimize_page(self, url: str, outfile: str):\n","        logging.info(f\"SEO-Optimierung startet => {url} => {outfile}\")\n","\n","        # 1) HTML + extrahieren\n","        html = self.fetch_html(url)\n","        blocks, old_title, old_desc, soup = self.extract_dom_texts(html)\n","\n","        # 2) Analyse Original\n","        analysis_text = self.do_pre_analysis(blocks)\n","\n","        # 3) SEO => block rewriting\n","        blocks = self.do_large_text_optimization(blocks)\n","\n","        # 4) Globale Beschreibung\n","        improvement_desc = self.describe_improvements_global(blocks)\n","\n","        # 5) Title & Desc => optional\n","        new_title = old_title\n","        new_desc  = old_desc\n","\n","        # 6) Re-Inject\n","        new_soup = self.inject_content(soup, blocks, new_title, new_desc)\n","        self.save_html(new_soup, outfile)\n","\n","        # 7) combined report\n","        combined_data = {\n","            \"analysis_of_original_text\": analysis_text,\n","            \"improvement_description\": improvement_desc,\n","            \"blocks\": []\n","        }\n","        for b in blocks:\n","            combined_data[\"blocks\"].append({\n","                \"id\": b[\"id\"],\n","                \"original_text\": b[\"original_text\"],\n","                \"optimized_text\": b[\"optimized_text\"]\n","            })\n","        # Save as JSON\n","        json_report = os.path.join(self.output_dir, \"report.json\")\n","        with open(json_report, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(combined_data, f, indent=2, ensure_ascii=False)\n","\n","        logging.info(f\"Fertig! => {outfile}\\nReport => {json_report}\")\n"],"metadata":{"id":"MB4FgTpBqab7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from seopageoptimizer import SEOPageOptimizer\n","\n","my_keywords = keywords_final\n","\n","optimizer = SEOPageOptimizer(\n","    output_dir=OUTPUT_PATH + \"/final\",\n","    prompts_file=PROMPT_PATH + \"/seo_prompts_2.json\",\n","    google_ads_keywords=keywords_final\n",")\n","\n","url_to_optimize = START_URL\n","outfile = OUTPUT_PATH + \"/final/optimized.html\"\n","\n","optimizer.optimize_page(url_to_optimize, outfile)\n","print(\"Done! File at:\", outfile)\n"],"metadata":{"id":"QWIyl9pgqfwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CBAhkM_G8oLC"},"execution_count":null,"outputs":[]}]}