{"cells":[{"cell_type":"markdown","metadata":{"id":"Pns8QF8VWrHR"},"source":["#üåçglobale Parameter\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"oPvQ5aC7QgFS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743769408616,"user_tz":-120,"elapsed":4020,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"3a922ac4-f3f0-48a5-fc20-289debe69c42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### HYPERPARAMETER ###\n","\n","# START_URL = 'https://www.ewms-tech.com/'\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","# START_URL = 'https://www.malerarbeiten-koenig.de/'\n","\n","\n","EXCLUDED_WEBSITES = [\"impressum\", \"datenschutz\", \"datenschutzerkl√§rung\", \"agb\"]\n","\n","\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","PROJECT_ROOT_ESC_STR = PROJECT_ROOT.replace('Colab Notebooks', 'Colab\\ Notebooks')\n","\n","SRC_PATH, DATA_PATH, TEST_PATH, OUTPUT_PATH = PROJECT_ROOT + \"/src\", PROJECT_ROOT + \"/data\", PROJECT_ROOT + '/tests', PROJECT_ROOT + '/output'\n","FAISS_PATH = DATA_PATH + '/faiss_db'\n","KEYWORD_PATH = DATA_PATH + '/keywords'\n","PROMPT_PATH = DATA_PATH + '/prompts'\n","\n","\n","FINAL_IMAGES = {}\n"]},{"cell_type":"markdown","metadata":{"id":"EnjP4Zhat_8t"},"source":["# üìî READ_ME und TO_DO"]},{"cell_type":"markdown","metadata":{"id":"xaxxhKAgyeQq"},"source":["#### README"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"443XYilDN9NA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743769408673,"user_tz":-120,"elapsed":54,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"2d5fcac3-3ecb-4748-95f4-0c86df67ead0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/README.md\n"]}],"source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/README.md'\n","# üöÄ SEO Automation Pipeline mit OpenAI & Retrieval (RAG)\n","\n","Dieses Projekt bietet eine **komplette End-to-End-Pipelines f√ºr die SEO-Optimierung von Websites**, inklusive **Web-Crawling, SEO-Analyse, KI-gest√ºtzter Text-Optimierung und Qualit√§tskontrolle**.\n","\n","Kern des Projekts sind **automatisierte Abl√§ufe**, die von der **Datengewinnung bis zur SEO-optimierten Textgenerierung** reichen.\n","Es wird eine pipeline zur Text-Optimierung Mithilfe von **OpenAI (ChatGPT)** hergestellt. Die Textelemente eines HTML Dokumentes werden mit ChatGPT SEO optimiert und danach wieder in den code eingebaut.\n","Der Kunde erh√§lt eine Datei als Vorschau auf seine SEO optimierte website.\n","\n","## üìö Inhaltsverzeichnis\n","\n","- Features\n","- Projektstruktur\n","- Ablauf & Module\n","- Technologien\n","- Installation\n","- Nutzung\n","- Ziele\n","- Roadmap\n","\n","## ‚úÖ Features\n","\n","- üåê **Automatisiertes Web Crawling** (inkl. Filter f√ºr relevante Inhalte)\n","- ‚úçÔ∏è **Generierung von SEO-optimierten Texten** mithilfe der OpenAI API\n","- üß† **RAG-gest√ºtzte Fehlererkennung & Textkorrektur** mit Vektordatenbank (FAISS)\n","- üìä **Analyse der Optimierungsergebnisse** (Statistiken, √Ñhnlichkeiten, Visualisierungen)\n","- üìà **Keyword-Analyse und Keyword-Optimierung**\n","- üì¶ Ausgabe in **HTML und PDF** f√ºr Kunden\n","- üìä Umfangreiche **Datenvisualisierungen** (Wordclouds, Cosine Similarity, Keyword-Verteilung)\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=10oR2bcugvN2MClp14ia7gnzMGX5b896t\" alt=\"SEO Heatmap\" width=\"600\">\n","\n","\n","\n","\n","## üóÇÔ∏è Projektstruktur\n","\n","```\n","SEO-Project/\n","‚îú‚îÄ‚îÄ data/                # Prompts, Fehler-Korrektur-Daten, weitere JSON Dateien\n","‚îú‚îÄ‚îÄ notebooks/           # Colab/Notebooks zum Starten und Entwickeln\n","‚îú‚îÄ‚îÄ output/              # Erzeugte Dateien (HTML, PDF, Bilder)\n","‚îÇ   ‚îú‚îÄ‚îÄ final           # Dokumente f√ºr Kunden (HTML, PDF)\n","‚îÇ   ‚îî‚îÄ‚îÄ images          # Visualisierungen\n","‚îú‚îÄ‚îÄ src/                # Source Code (Python-Klassen und Module)\n","‚îÇ   ‚îú‚îÄ‚îÄ webscraper.py    # Webscrawling und Text-Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ llmprocessor.py # Anbindung an OpenAI API, Keyword Extraktion\n","‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py       # Zentrale Chatbot-Klasse zur Kommunikation mit GPT\n","‚îÇ   ‚îú‚îÄ‚îÄ seoanalyzer.py   # Analyse und Auswertung der Texte\n","‚îÇ   ‚îú‚îÄ‚îÄ github.py        # Automatischer Upload ins GitHub Repo\n","‚îÇ   ‚îú‚îÄ‚îÄ utils.py         # Hilfsmodule (z.B. f√ºr Prompt-Management)\n","‚îÇ   ‚îî‚îÄ‚îÄ embeddingdemo.py# 3D Embedding- und Cosine Similarity Visualisierungen\n","‚îú‚îÄ‚îÄ tests/              # pytest der Hauptfunktionalit√§ten\n","‚îî‚îÄ‚îÄ requirements.txt    # Python-Abh√§ngigkeiten\n","```\n","\n","## ‚öôÔ∏è Ablauf & Module\n","\n","### 1. **Web Crawling**\n","- **src/webscraper.py**: Holt Inhalte von Webseiten, filtert irrelevante Seiten (z.B. Impressum, AGB).\n","\n","### 2. **SEO-Optimierung mit OpenAI**\n","- **src/llmprocessor.py**:\n","  - Extrahiert Keywords aus den Inhalten.\n","  - Optimiert die Texte f√ºr SEO mit gezielten Prompts.\n","\n","### 3. **Analyse & Visualisierung**\n","- **src/seoanalyzer.py**: Verarbeitet und analysiert die Original- und optimierten Texte.\n","\n","### 4. **GitHub Automation**\n","- **src/github.py**: L√§dt finale Ergebnisse in ein GitHub-Repo hoch.\n","\n","## üß∞ Technologien\n","\n","| Technologie                  | Beschreibung                                       |\n","|-----------------------------|---------------------------------------------------|\n","| Python                      | Hauptsprache                                       |\n","| OpenAI API (ChatGPT, GPT-4)  | Generative KI f√ºr SEO-Texte                       |\n","| FAISS                      | Vektorsuche f√ºr RAG und Text-Fehler                |\n","| Pandas, NumPy               | Datenanalyse und Verarbeitung                      |\n","| Matplotlib, Seaborn         | Visualisierungen                                   |\n","| Sentence Transformers       | Embedding-Erstellung f√ºr Vektordatenbank          |\n","| BeautifulSoup, Requests     | Webcrawling                                        |\n","| Google Colab                | Entwicklung und Ausf√ºhrung                        |\n","\n","## üöÄ Installation\n","\n","```bash\n","pip install -r requirements.txt\n","python -m spacy download de_core_news_sm\n","pip install faiss-cpu sentence-transformers openai wordcloud matplotlib seaborn\n","```\n","\n","## üíª Nutzung\n","\n","```python\n","scraper = WebsiteScraper(start_url=\"https://www.example.com\")\n","scraper.scrape_website()\n","\n","llm_processor = LLMProcessor(prompts_folder, get_filtered_texts, google_ads_keywords)\n","llm_processor.run_all()\n","\n","seo_analyzer = SEOAnalyzer(seo_json, original_texts, keywords_final)\n","seo_analyzer.run_analysis()\n","```\n","\n","## üéØ Ziele\n","\n","- ‚úÖ Vollst√§ndige Automatisierung der SEO-Optimierung\n","- ‚úÖ RAG f√ºr sprachliche Qualit√§tskontrolle\n","- ‚úÖ Kundenfertige PDF/HTML-Reports\n","\n","## üöß Roadmap\n","\n","- [ ] **Produkt f√ºr Kunden finalisieren:** all-in-one solution f√ºr webcrawl + SEO + optimierten html code\n","- [ ] Automatische SEO Scores (z.B. Google Ads API)\n","- [ ] Automatische Keyword-Erweiterung\n","- [ ] Mehrsprachigkeit\n","- [ ] WordPress-Integration\n"]},{"cell_type":"markdown","metadata":{"id":"JtMpu3BMyh6-"},"source":["####TODO"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WBtOaSe-CHyO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743769408729,"user_tz":-120,"elapsed":52,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"c0c81752-f964-4c94-f10b-d3b0098ce65a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Colab Notebooks/SEO/TODO.md\n"]}],"source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/TODO.md'\n","# To-Do Liste: SEO Automation & KI-Projekt\n","\n","Diese Liste fasst alle anstehenden Aufgaben im Projekt zusammen\n","\n","---\n","\n","## 0. **Aktuelles und dringendes**\n","- [ ] 18.3.2025 **Version missmatch**: numpy 2.2.3 und spacy 3.5 **side effects on**: dependencies.py, excelimporter.py, Installation.ipynb **ursachen**: spacy version 3.5 verlangt numpy 1.26.4 -> version missmatch\n","      -> 23.5. class SEOAnlyzer refaktoriert und spacy ersetzt\n","- [ ] 28.3.25 lokale SEO keywords liefern manchmal falsche Stadt\n","- [ ] prompts besser organisieren\n","- [ ] all-in-one l√∂sung weiter verfolgen\n","\n","---\n","\n","## 1. **Allgemeine Projektorganisation**\n","- [ ] **Projektstruktur verbessern**: Ordner √ºbersichtlich gestalten (z.B. `src/`, `data/`, `tests/`, `notebooks/`, dependencies.py).\n","- [ ] **Dokumentation erweitern**: READ_ME und Wiki (bzw. GitHub Pages) zu jedem Modul anlegen.\n","- [ ] **Automatisierte Tests** Pytest f√ºr Kernfunktionen ausbauen.\n","- [ ] **Produkt f√ºr Kunden finalisieren**\n","- [ ] **FAISS DB**: automatisierte Erweiterung bei neu gefundenen Fehlern\n","- [ ] **Template GitHub**: issues\n","- [ ] Funktionalit√§ten aus **utils.py** √ºberdenken\n","- [ ] langfristig Umstieg auf **langchain**\n","- [ ] textprocessor durch openai **function calling** ersetzen\n","- [ ] **dependencies** und versionen robuster machen\n","- [ ] **bug reporting system** einrichten\n","\n","---\n","\n","## 2. **Vector-Datenbank (FAISS) & Retrieval**\n","- [ ] **VectorDB-Klasse finalisieren**:\n","  - [ ] Kleinere Bugs beheben\n","  - [ ] Userfreundliche Methoden f√ºr neue Eintr√§ge\n","- [ ] **Einrichtung der DB** bei Projektstart (Neubau vs. Laden) vereinheitlichen\n","- [ ] **Konfigurierbare √Ñhnlichkeits-Schwelle** (z.B. `threshold=0.6`) besser dokumentieren\n","- [ ] **Dynamische Filter** f√ºr bestimmte Fehlerkategorien (z.B. Stil vs. Grammatik) √ºberlegen\n","- [ ] **hybrides system mit knowledge tree und RAG** etablieren\n","\n","---\n","\n","## 3. **SEO-Optimierungs-Pipeline**\n","- [ ] **Prompts in JSON-Dateien** verlagern (z.B. `/data/prompts/`) und sauber verlinken\n","- [ ] **Supervisor-Feedback** integrieren & QA-Schritte definieren\n","- [ ] Langchain und SEOPageOptimizer verbinden\n","\n","---\n","\n","## 4. **SEOGrammarChecker & PromptManager**\n","- [ ] Klassenrefactoring:\n","  - [ ] **`VectorDB`** vs. **`PromptManager`** vs. **`SEOGrammarChecker`** sauber trennen\n","  - [ ] M√∂glichst wenig Code-Duplikate, mehr modulare Testbarkeit\n","- [ ] **Konfigurationsdatei** (z.B. YAML) f√ºr Pfade, wie `FAISS_PATH` & Promptordner\n","- [ ] **Erweiterbare Prompt-Templates**:\n","  - [ ] Z.B. `seo_optimization.json`, `grammar_check.json`, `supervisor.json`, etc.\n","\n","---\n","\n","## 5. **Abschluss & Integration**\n","- [ ] **Dokumentation** aller Pipelines & Klassen in der README (oder in separater Doku)\n","- [ ] **Optionale WordPress-Integration** in der Zukunft (Ideenspeicher)\n","  - [ ] Upload via REST API\n","  - [ ] Metadaten (Title, Slug, Tags etc.)\n"]},{"cell_type":"markdown","metadata":{"id":"TPz7fgRaRAUv"},"source":["# üèÅ Install requirements"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"shZS1Psx1ZXo","executionInfo":{"status":"ok","timestamp":1743769462745,"user_tz":-120,"elapsed":54014,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"outputs":[],"source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZmlHuIgWE5x5","executionInfo":{"status":"ok","timestamp":1743769462751,"user_tz":-120,"elapsed":2,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"outputs":[],"source":["# %run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"]},{"cell_type":"markdown","metadata":{"id":"o38bsnD-EWcq"},"source":["# ‚õ© push to github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"do4Bf0uw-y8U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5c96caf-89bf-43b8-d99b-2db97b8b745b"},"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Klonen des GitHub-Repositories...\n"]}],"source":["import importlib\n","import github\n","importlib.reload(github)\n","from github import GitHubManager\n","\n","# Starte den GitHub-Sync\n","git_manager = GitHubManager(\n","    userdata.get(\"github_pat\"),\n","    userdata.get(\"github_email\"),\n","    userdata.get(\"github_username\"),\n","    userdata.get(\"github_repo_seo\"),\n","    PROJECT_ROOT_ESC_STR\n",")\n","\n","git_manager.clone_repo()  # Klonen des Repos\n","git_manager.sync_project()"]},{"cell_type":"markdown","metadata":{"id":"vNVT-lr5jejz"},"source":["# üï∏ crawl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kksmfARinqon"},"outputs":[],"source":["import importlib\n","import webscraper\n","importlib.reload(webscraper)\n","from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_WEBSITES)\n","\n","original_texts = scraper.get_filtered_texts()"]},{"cell_type":"markdown","metadata":{"id":"veMjko5Tvdtt"},"source":["# üîë SEO keywords"]},{"cell_type":"markdown","metadata":{"id":"eHxAg4fuIc2T"},"source":["### üì∫ google ads seo keywords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REeveZ8rwEys"},"outputs":[],"source":["import excelimporter\n","importlib.reload(excelimporter)\n","from excelimporter import ExcelImporter\n","\n","importer = ExcelImporter(project_folder=PROJECT_ROOT, header=2)\n","keyword_df = importer.import_all()\n","\n","if keyword_df is not None:\n","    excluded_seo_keywords = ['spange de', 'kfo zentrum essen', 'gerade zahne', 'zahn spange']\n","\n","    keyword_df = keyword_df[(keyword_df['Avg. monthly searches'] > 10) &\n","    (~keyword_df['Keyword'].isin(excluded_seo_keywords))\n","    ].sort_values(by='Avg. monthly searches', ascending=False).reset_index(drop=True).copy()\n","\n","    google_ads_keywords = list(keyword_df['Keyword'])\n","\n","    keyword_df.set_index(keyword_df.columns[0], inplace=True)\n","    searches_df = keyword_df[[col for col in keyword_df.columns if col.startswith('Searches')]]\n","\n","else:\n","    print(\"Error: importer.import_all() returned None. Check your data file and ExcelImporter class.\")\n","    google_ads_keywords = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnJ5j0VZ_JWK"},"outputs":[],"source":["import keywordvisualizer\n","importlib.reload(keywordvisualizer)\n","from keywordvisualizer import KeywordVisualizer\n","\n","\n","visualizer = KeywordVisualizer(\n","    df=searches_df,\n","    output_dir=OUTPUT_PATH+'/images',\n","    image_paths=FINAL_IMAGES\n",")\n","visualizer.heatmap()"]},{"cell_type":"markdown","metadata":{"id":"nunU-GpIVCgq"},"source":["##üîÆkeywords LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXzSL_SR_JRG"},"outputs":[],"source":["import chatbot\n","importlib.reload(chatbot)\n","from chatbot import Chatbot\n","from utils import load_prompts\n","\n","prompts = load_prompts(PROMPT_PATH + '/extract_keywords.json')\n","system_prompt = prompts['system_prompt']\n","user_prompt = prompts['user_prompt']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTZ6UNUT7BdI"},"outputs":[],"source":["user_prompt = user_prompt.replace('{original_text}', \" \".join(list(original_texts.values())))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXLiqhti_JOm"},"outputs":[],"source":["ch = Chatbot(system_prompt, user_prompt)\n","keywords_llm = ch.chat()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gixIRMw5V4B5"},"outputs":[],"source":["keywords_llm_google = keywords_llm.split(\", \") + google_ads_keywords"]},{"cell_type":"markdown","metadata":{"id":"1zqvsEJvx33W"},"source":["# üõè embedding demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toL_RwWuO72N"},"outputs":[],"source":["import os\n","import embeddingdemo\n","importlib.reload(embeddingdemo)\n","from embeddingdemo import EmbeddingDemo\n","\n","demo = EmbeddingDemo(output_dir=OUTPUT_PATH+'/images', final_images=FINAL_IMAGES)\n","demo.run_all_visualizations()\n","\n","print(\"Alle Embedding-Bilder:\", demo.get_image_paths())\n","print(\"Gemeinsame Bilder (shared dict):\", FINAL_IMAGES)"]},{"cell_type":"markdown","metadata":{"id":"HraJ3_5SJHSv"},"source":["# üì• Tests"]},{"cell_type":"markdown","metadata":{"id":"hdqvYkNsg7fx"},"source":["#### üìÉ Doku"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1xV8mVljdkL"},"outputs":[],"source":["%%writefile '/content/drive/MyDrive/Colab Notebooks/SEO/tests/DOKU_TESTS.md'\n","# ‚úÖ Pytest Template: Chatbot Klasse\n","\n","## 1. Klassen & Methoden die getestet werden sollen\n","\n","- **Chatbot**\n","  - `chat()`\n","  - `chat_with_streaming()`\n","\n","---\n","\n","## 2. Beispielhafte Inputs + erwartete Outputs pro Methode\n","\n","| Methode                      | Beispiel Input                                                           | Erwartete Ausgabe    |\n","|-----------------------------|-------------------------------------------------------------------------|----------------------|\n","| `Chatbot.chat()`             | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'           | \"Test erfolgreich\"   |\n","| `Chatbot.chat_with_streaming()` | 'Das ist ein Test. Schreibe als Antwort \"Test erfolgreich\".'         | \"Test erfolgreich\"   |\n","\n","---\n","\n","## 3. Return-Typen der Methoden\n","\n","| Methode                      | R√ºckgabe-Typ |\n","|-----------------------------|--------------|\n","| `Chatbot.chat()`             | `str`        |\n","| `Chatbot.chat_with_streaming()` | `str`     |\n","\n","---\n","\n","## 4. Externe Services mocken?\n","\n","| Service         |  Mocken?                           |\n","|-----------------|-------------------------------------|\n","| OpenAI API      |  Nein                                |\n","| FAISS Index     |  Ja (kleine Test-Datenbank f√ºr FAISS) |\n","\n","---\n","\n","## 5. Ordnerstruktur f√ºr Tests\n","\n","```bash\n","/project-root/\n","    /src/\n","        chatbot.py\n","    /tests/\n","        test_chatbot.py\n","    /logs/\n","        test_report.log\n","    ...\n","```\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"dr3XCOHn7Nc_"},"source":["#### üë∑ code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n68k1npffFsc"},"outputs":[],"source":["import pytest\n","pytest.main(['-v', TEST_PATH+'/chatbot_test/chatbot_test.py'])"]},{"cell_type":"markdown","metadata":{"id":"tCeeWN-UjXaI"},"source":["# üé® Error Collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_KjsdzFHBpL"},"outputs":[],"source":["error_corrections = {\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\":\n","    \"Eine Zahnspange kann Kiefergelenksbeschwerden, sowie Kau- und Sprechprobleme effektiv behandeln.\",\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\":\n","    \"Als in den USA geborene Kieferorthop√§din bringt Dr. Meier eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch und Swahili. Dazu hat sie Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\",\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf abgeschlossen.\":\n","    \"Sie hat ihren Master of Science in Geochemie von der Universit√§t M√ºnster, Deutschland, und hat an der Universit√§t D√ºsseldorf promoviert.\",\n","    \"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln.\":\n","    \"Sie ist hoch qualifiziert und hat eine besondere Hingabe zu einem √§sthetischen L√§cheln.\",\n","    \"behandlungsorientierte Zahnberatung\": \"patientenorientierte Beratung\",\n","    \"√§stehthetisches L√§cheln\": \"√§sthetisches L√§cheln\",\n","    \"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\": \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\",\n","    \"Der aktuelle Text zur Zahnspangenpraxis\": \"Der aktuelle Text zur kieferorthop√§dischen Praxis\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQJFBU_9-T9d"},"outputs":[],"source":["new_error_corrections = {\"Das ist ein neuer Fehler.\": \"Das ist ein korrigierter Fehler.\"}"]},{"cell_type":"markdown","metadata":{"id":"lNqdhhAit9q9"},"source":["# üé® RAG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alRDUdT4I7o0"},"outputs":[],"source":["%%capture\n","import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 1) VectorDB\n","# -------------------------\n","\n","#test_text = seo_json[list(seo_json.keys())[0]][\"SEO\"]\n","\n","class VectorDB:\n","    \"\"\"\n","    Eine Klasse f√ºr alles rund um die Vektordatenbank:\n","    - Aufbauen & Laden (FAISS)\n","    - Neue Eintr√§ge hinzuf√ºgen\n","    - Querying f√ºr Context Retrieval\n","    \"\"\"\n","\n","    def __init__(self, db_folder):\n","        \"\"\"\n","        :param db_folder: Pfad zum Datenbank-Ordner\n","        \"\"\"\n","        self.db_folder = db_folder\n","        self.index_file = os.path.join(db_folder, \"faiss_index.bin\")\n","        self.json_file  = os.path.join(db_folder, \"faiss_index.json\")\n","\n","        self.index = None\n","        self.error_dict = {}\n","\n","        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","    def build_index(self, error_corrections: dict):\n","        \"\"\"\n","        Baut einen neuen FAISS-Index aus den √ºbergebenen Fehler-Korrektur-Paaren.\n","        \"\"\"\n","        print(\"üî® Baue neuen FAISS-Index...\")\n","        os.makedirs(self.db_folder, exist_ok=True)\n","\n","        self.error_dict = error_corrections\n","        errors = list(self.error_dict.keys())\n","\n","        # Embeddings\n","        embeddings = np.array([self.model.encode(e) for e in errors], dtype=\"float32\")\n","\n","        # FAISS-Index anlegen\n","        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n","        self.index.add(embeddings)\n","\n","        # Daten auf Festplatte schreiben\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ Neuer Index + JSON in '{self.db_folder}' erstellt.\")\n","\n","    def load_index(self):\n","        \"\"\"\n","        L√§dt einen bereits existierenden FAISS-Index und die Fehler-Daten.\n","        \"\"\"\n","        if not (os.path.exists(self.index_file) and os.path.exists(self.json_file)):\n","            raise FileNotFoundError(\"‚ùå Kein FAISS-Index gefunden. Bitte build_index() aufrufen.\")\n","\n","        print(\"üîé Lade vorhandenen FAISS-Index...\")\n","        self.index = faiss.read_index(self.index_file)\n","\n","        with open(self.json_file, \"r\", encoding=\"utf-8\") as f:\n","            self.error_dict = json.load(f)\n","\n","        print(\"‚úÖ Index & Fehler-Korrekturen geladen.\")\n","\n","    def add_entries(self, new_error_corrections: dict):\n","        \"\"\"\n","        F√ºgt weitere Fehler-Korrektur-Paare hinzu, ohne alles neu zu bauen.\n","        \"\"\"\n","        if self.index is None:\n","            # Versuch zu laden, falls vorhanden\n","            if os.path.exists(self.index_file) and os.path.exists(self.json_file):\n","                self.load_index()\n","            else:\n","                raise FileNotFoundError(\"‚ùå Kein Index vorhanden. Bitte erst build_index() nutzen.\")\n","\n","        # Merge in self.error_dict\n","        for fehler, korrektur in new_error_corrections.items():\n","            self.error_dict[fehler] = korrektur\n","\n","        # embeddings nur f√ºr die neuen keys\n","        new_keys = list(new_error_corrections.keys())\n","        new_embeds = np.array([self.model.encode(k) for k in new_keys], dtype=\"float32\")\n","\n","        # An Index anh√§ngen\n","        self.index.add(new_embeds)\n","\n","        # Speichern\n","        faiss.write_index(self.index, self.index_file)\n","        with open(self.json_file, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.error_dict, f, ensure_ascii=False)\n","\n","        print(f\"‚úÖ {len(new_keys)} neue Eintr√§ge hinzugef√ºgt und Index aktualisiert.\")\n","\n","    def query(self, text: str, top_k=3, threshold=0.6):\n","        \"\"\"\n","        Sucht in der DB nach √§hnlichen fehlerhaften Formulierungen.\n","\n","        :param text: Der zu pr√ºfende Satz/Abschnitt\n","        :param top_k: Anzahl der gesuchten √Ñhnlichkeiten\n","        :param threshold: Distanzschwelle\n","        :return: Liste [(fehler, korrektur), ...]\n","        \"\"\"\n","        if self.index is None:\n","            self.load_index()\n","\n","        embed = np.array([self.model.encode(text)], dtype=\"float32\")\n","        distances, indices = self.index.search(embed, top_k)\n","\n","        all_errors = list(self.error_dict.keys())\n","\n","        results = []\n","        for i in range(top_k):\n","          idx = indices[0][i]\n","          # Sicherstellen, dass idx in den Bereich von all_errors passt\n","          if idx < len(all_errors):\n","              if distances[0][i] < threshold:\n","                  fehler_key = all_errors[idx]\n","                  korrektur = self.error_dict[fehler_key]\n","                  results.append((fehler_key, korrektur))\n","        return results\n","\n","\n","    def retrieve_context(self, seo_text: str) -> str:\n","        \"\"\"\n","        Durchsucht den seo_text Satz f√ºr Satz, holt ggf. Korrekturvorschl√§ge\n","        und baut einen Kontextstring.\n","        \"\"\"\n","        lines = []\n","        for s in seo_text.split(\". \"):\n","            suggestions = self.query(s)\n","            for old, new in suggestions:\n","                lines.append(f\"- Fehler: {old} ‚ûù Verbesserung: {new}\")\n","\n","        if lines:\n","            return \"Bekannte Fehler/Korrekturen:\\n\" + \"\\n\".join(lines)\n","        else:\n","            return \"Keine bekannten Fehler gefunden.\"\n","\n","\n","\n","db = VectorDB(db_folder=FAISS_PATH)\n","db.build_index(error_corrections)\n","db.add_entries(new_error_corrections)\n","#db.retrieve_context(test_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TWN0JGYI7l0"},"outputs":[],"source":["%%capture\n","import os\n","import json\n","import faiss\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from chatbot import Chatbot\n","\n","# -------------------------\n","# 2) PromptManager\n","# -------------------------\n","\n","class PromptManager:\n","    \"\"\"\n","    L√§dt Prompts aus dem /data/prompts Ordner und kombiniert sie mit\n","    dem Context aus der VectorDB, um einen finalen Prompt zu erstellen.\n","    \"\"\"\n","\n","    def __init__(self, prompts_folder=\"./data/prompts\"):\n","        \"\"\"\n","        :param prompts_folder: Ordner, in dem .json (oder .txt) Prompts liegen\n","        \"\"\"\n","        self.prompts_folder = prompts_folder\n","\n","    def load_prompt(self, filename: str) -> dict:\n","        \"\"\"\n","        L√§dt einen JSON-Prompt aus dem Ordner, z.B. 'grammar_prompt.json'.\n","        \"\"\"\n","        path = os.path.join(self.prompts_folder, filename)\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                return json.load(f)\n","        except FileNotFoundError:\n","            print(f\"‚ö†Ô∏è Prompt-Datei {path} nicht gefunden!\")\n","            return {}\n","        except json.JSONDecodeError:\n","            print(f\"‚ö†Ô∏è Ung√ºltiges JSON in {path}\")\n","            return {}\n","\n","    def build_final_prompt(self, base_prompt_file: str, context: str, user_text: str) -> (str, str):\n","        \"\"\"\n","        Kombiniert:\n","         - base_prompt_file (System-/User-Prompts)\n","         - den 'context' aus der VectorDB\n","         - den 'user_text' (SEO-Text)\n","        und gibt final (system_prompt, user_prompt) zur√ºck.\n","        \"\"\"\n","        prompt_data = self.load_prompt(base_prompt_file)\n","\n","        system_prompt = prompt_data.get(\"system_prompt\", \"\")\n","        user_prompt   = prompt_data.get(\"user_prompt\", \"\")\n","\n","        # Kontext an system_prompt anh√§ngen\n","        system_prompt_full = system_prompt\n","\n","        # SEO-Text an user_prompt anh√§ngen\n","        user_prompt_full = user_prompt.format(context=context,optimized_text=user_text)\n","\n","        return (system_prompt_full, user_prompt_full)\n","\n","# pm = PromptManager(prompts_folder=PROMPT_PATH)\n","# context = db.retrieve_context(test_text)\n","# final_prompts = pm.build_final_prompt(\"grammar_check.json\", context, test_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahrShDjzI7iz"},"outputs":[],"source":["\n","# from chatbot import Chatbot\n","\n","# -------------------------\n","# 3) SEOGrammarChecker\n","# -------------------------\n","\n","# cb = Chatbot(systemprompt=final_prompts[0], userprompt=final_prompts[1])\n","# final_text = cb.chat()\n","# final_text"]},{"cell_type":"markdown","metadata":{"id":"rRMI7Rf5Wsi_"},"source":["# üìüSEO Texte multiüëÖ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_5-vxhH3LIo"},"outputs":[],"source":["import seopageoptimizer\n","importlib.reload(seopageoptimizer)\n","from seopageoptimizer import SEOPageOptimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eqLQUME0ozZ"},"outputs":[],"source":["languages_to_produce = [\"German\"]  # oder nur [\"English\"]\n","\n","for url in list(original_texts.keys()):\n","    for lang in languages_to_produce:\n","        # SEO-Optimizer erstellen\n","        optimizer = SEOPageOptimizer(\n","            output_dir=OUTPUT_PATH + \"/final\",\n","            prompts_file=PROMPT_PATH + \"/seo_prompts_2.json\",\n","            google_ads_keywords=keywords_llm_google\n","        )\n","\n","        url_to_optimize = url\n","        # Erzeuge sinnvollen Dateinamen, z.B. \"page_optimized_English.html\"\n","        sanitized_url = str(url).replace(\"/\", \"\")\n","        html_filename = f\"{sanitized_url}_optimized_{lang}.html\"\n","        outfile = os.path.join(OUTPUT_PATH, \"final\", html_filename)\n","\n","        # now call optimize_page with translate=True, passing the language\n","        optimizer.optimize_page(\n","            url=url_to_optimize,\n","            outfile=outfile,\n","            translate=False if languages_to_produce == [\"German\"] else True,\n","            language=lang\n","        )\n","\n","        print(f\"Done! HTML for {url_to_optimize} in {lang} at:\", outfile)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syYldS-NGSJX"},"outputs":[],"source":["import os\n","import json\n","\n","def merge_reports_for_urls(url_list, output_dir=OUTPUT_PATH+\"/final\", final_merged=\"final_merged.json\"):\n","    \"\"\"\n","    - Liest f√ºr jede URL die jeweilige report_{url_sanitized}.json\n","    - Fasst analysis/improvement/Bloecke zusammen\n","    - Schreibt am Ende ein gro√ües JSON in `final_merged`\n","    \"\"\"\n","    # Dieses Dictionary wird alle zusammengefassten Infos enthalten\n","    big_merged_data = {}\n","\n","    for url in url_list:\n","        # Erzeuge den Dateinamen analog zur Optimierung\n","        # z. B.  \"report_https:example.compage.html.json\"\n","        sanitized_url = url.replace(\"/\", \"\")\n","        report_filename = f\"report_{sanitized_url}.json\"\n","        report_path = os.path.join(output_dir, report_filename)\n","\n","        # Pr√ºfen, ob Datei existiert\n","        if not os.path.isfile(report_path):\n","            print(f\"‚ö†Ô∏è Report-Datei fehlt: {report_path}\")\n","            continue\n","\n","        # JSON lesen\n","        with open(report_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","\n","        # analysis_of_original_text & improvement_description\n","        analysis = data.get(\"analysis_of_original_text\", \"\")\n","        improvement = data.get(\"improvement_description\", \"\")\n","\n","        # Blocks => original_text + optimized_text kumulieren\n","        blocks = data.get(\"blocks\", [])\n","        original_texts = []\n","        optimized_texts = []\n","\n","        for block in blocks:\n","            original_texts.append(block.get(\"original_text\",\"\"))\n","            optimized_texts.append(block.get(\"optimized_text\",\"\"))\n","\n","        # Zusammenfassen in je einen gro√üen String\n","        merged_original = \"\\n\".join(original_texts)\n","        merged_optimized = \"\\n\".join(optimized_texts)\n","\n","        # In big_merged_data ablegen\n","        big_merged_data[url] = {\n","            \"analysis\": analysis,\n","            \"improvement\": improvement,\n","            \"original_text\": merged_original,\n","            \"optimized_text\": merged_optimized\n","        }\n","\n","    # Alles als ein JSON speichern\n","    final_json_path = os.path.join(output_dir, final_merged)\n","    with open(final_json_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(big_merged_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"‚úÖ Zusammenfassung erstellt: {final_json_path}\")\n","\n","\n","# Beispiel-Aufruf:\n","if __name__ == \"__main__\":\n","    # Angenommen, hier deine URLs\n","    urls_to_process = list(original_texts.keys())\n","\n","    merge_reports_for_urls(urls_to_process, output_dir=OUTPUT_PATH+\"/final\", final_merged=\"allinone.json\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1SsHYtpkAWa"},"outputs":[],"source":["import os\n","import json\n","import pypandoc\n","\n","def create_html_report(merged_json_path, out_html=\"seo_report.html\", out_docx=\"seo_report.docx\"):\n","    \"\"\"\n","    Liest das zusammengefasste JSON (alle URLs, Analyse, Verbesserungen, Original & Optimiert)\n","    und erzeugt daraus ein HTML-Dokument. Anschlie√üend konvertiert es das HTML in eine DOCX-Datei.\n","    \"\"\"\n","\n","    # JSON laden\n","    with open(merged_json_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","        # => data = {\n","        #    \"https://example.com\" : {\n","        #       \"analysis\": \"...\",\n","        #       \"improvement\": \"...\",\n","        #       \"original_text\": \"...\",\n","        #       \"optimized_text\": \"...\",\n","        #    }, ...\n","        # }\n","\n","    # HTML Template zusammenbauen\n","    # Du kannst hier inline CSS nutzen oder <link> mit externem Stylesheet\n","    html_head = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","      <meta charset=\"UTF-8\"/>\n","      <title>SEO Zusammenfassung</title>\n","      <style>\n","        body {\n","          font-family: \"Helvetica Neue\", Arial, sans-serif;\n","          margin: 20px;\n","          line-height: 1.6;\n","          color: #333;\n","          background: #f2f2f2;\n","        }\n","        h1 {\n","          text-align: center;\n","          background: #007BFF;\n","          color: #fff;\n","          padding: 10px;\n","          border-radius: 4px;\n","        }\n","        .url-block {\n","          background: #fff;\n","          border-radius: 5px;\n","          padding: 20px;\n","          margin-bottom: 20px;\n","          box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n","        }\n","        .url-block h2 {\n","          margin-top: 0;\n","          color: #007BFF;\n","        }\n","        .subtitle {\n","          font-weight: bold;\n","          color: #555;\n","          margin-top: 1rem;\n","        }\n","        .text-box {\n","          background: #fafafa;\n","          border-left: 4px solid #007BFF;\n","          padding: 10px;\n","          margin: 10px 0;\n","          white-space: pre-wrap;  /* Damit \\n Zeilenumbr√ºche anzeigt */\n","        }\n","      </style>\n","    </head>\n","    <body>\n","      <h1>SEO Report &amp; Verbesserungs-√úbersicht</h1>\n","    \"\"\"\n","\n","    html_body = []\n","\n","    # F√ºr jede URL Sektion erstellen\n","    for idx, (url, content) in enumerate(data.items(), start=1):\n","        analysis = content.get(\"analysis\", \"\")\n","        improvement = content.get(\"improvement\", \"\")\n","        original_text = content.get(\"original_text\", \"\")\n","        optimized_text = content.get(\"optimized_text\", \"\")\n","\n","        section_html = f\"\"\"\n","        <div class=\"url-block\">\n","          <h2>{idx}. {url}</h2>\n","\n","          <div class=\"subtitle\">Analyse des Originaltexts:</div>\n","          <div class=\"text-box\">{analysis}</div>\n","\n","          <div class=\"subtitle\">Erkl√§rung der Verbesserungen:</div>\n","          <div class=\"text-box\">{improvement}</div>\n","        </div>\n","        \"\"\"\n","        html_body.append(section_html)\n","\n","    html_end = \"\"\"\n","    </body>\n","    </html>\n","    \"\"\"\n","\n","    # Finale HTML zusammensetzen\n","    final_html = html_head + \"\\n\".join(html_body) + html_end\n","\n","    # Speichern als HTML\n","    with open(out_html, \"w\", encoding=\"utf-8\") as f:\n","        f.write(final_html)\n","\n","    print(f\"‚úÖ HTML-Report erstellt: {out_html}\")\n","\n","    # Mit pypandoc in DOCX konvertieren\n","    # Daf√ºr muss pypandoc & pandoc installiert sein\n","    pypandoc.convert_file(\n","        source_file=out_html,\n","        to=\"docx\",\n","        outputfile=out_docx,\n","        extra_args=[\"--standalone\"]\n","    )\n","    print(f\"‚úÖ DOCX exportiert: {out_docx}\")\n","\n","\n","if __name__ == \"__main__\":\n","    merged_json = OUTPUT_PATH + \"/final/allinone.json\"  # Pfad zu deinem zusammengefassten JSON\n","    out_html = OUTPUT_PATH + \"/final/seo_report.html\"\n","    out_docx = OUTPUT_PATH + \"/final/seo_report.docx\"\n","\n","    create_html_report(merged_json, out_html, out_docx)\n"]},{"cell_type":"markdown","metadata":{"id":"BatTg4ZKvpZV"},"source":["# üîé SEO Analyses + Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lkqnkWkw5wW"},"outputs":[],"source":["# k√ºnstliche SEO-Daten\n","historical_data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OibUqk7zvd5k"},"outputs":[],"source":["json_path = OUTPUT_PATH + \"/final/allinone.json\"\n","with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","    seo_json = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtZrcL_wxKzk"},"outputs":[],"source":["import seoanalyzer\n","importlib.reload(seoanalyzer)\n","from seoanalyzer import SEOAnalyzer\n","\n","exclude_list = [\"leila\", \"graf\", \"koenig\", \"bjoern\", \"k√∂nig\", 'bj√∂rn', 'adolf', 'schmidt', 'strasse', 'stra√üe', 'tilo', 'remhof']\n","\n","keywords_final = google_ads_keywords\n","\n","seo_analyzer = SEOAnalyzer(\n","    seo_json=seo_json,\n","    keywords_final=keywords_llm.split(\", \"),\n","    output_dir=OUTPUT_PATH+\"/images\",\n","    historical_data=historical_data,\n","    wordcloud_exclude=exclude_list,\n","    shared_image_dict=FINAL_IMAGES\n",")\n","\n","analysis_paths = seo_analyzer.run_analysis()\n","print(\"Analysis Plot Pfade:\", analysis_paths)\n","\n","model_paths = seo_analyzer.run_models()\n","print(\"Model Plot Pfade:\", model_paths)\n","\n","all_paths = seo_analyzer.get_all_image_paths()\n","print(\"Alle Pfade gesammelt:\", all_paths)\n"]},{"cell_type":"markdown","metadata":{"id":"MM7uu--42W4u"},"source":["#ü•ºLab"]}],"metadata":{"colab":{"collapsed_sections":["Pns8QF8VWrHR","EnjP4Zhat_8t","xaxxhKAgyeQq","JtMpu3BMyh6-","TPz7fgRaRAUv","o38bsnD-EWcq","vNVT-lr5jejz","eHxAg4fuIc2T","hdqvYkNsg7fx","dr3XCOHn7Nc_","tCeeWN-UjXaI","lNqdhhAit9q9"],"provenance":[],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyPk6BbhUuTp5q5o/6FBiB+W"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}