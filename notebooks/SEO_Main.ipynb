{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Pns8QF8VWrHR","TPz7fgRaRAUv","o38bsnD-EWcq","V50QPoW3F6wW","BatTg4ZKvpZV","4v6-jGMeO9Zt"],"mount_file_id":"1yg7F5G-16nc3airG6O1JA06ixuVKAkQ8","authorship_tag":"ABX9TyMmWxuOnQlZlfmXjUH5CAzQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#üåãHYPERPARAMETER\n","\n"],"metadata":{"id":"Pns8QF8VWrHR"}},{"cell_type":"code","source":["### HYPERPARAMETER ###\n","\n","from google.colab import drive, userdata\n","drive.mount('/content/drive', force_remount=True)\n","\n","PROJECT_ROOT = userdata.get(\"gdrive_seo_root\")\n","SRC_PATH = PROJECT_ROOT + \"/src\"\n","START_URL = \"https://www.rue-zahnspange.de/\"\n","EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvQ5aC7QgFS","executionInfo":{"status":"ok","timestamp":1740981776409,"user_tz":-60,"elapsed":2530,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"fc57c538-41b5-4d96-ca61-e43c1b667863"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# üèÅ Install requirements + dependencies"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/Installation.ipynb'"],"metadata":{"id":"shZS1Psx1ZXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/src/dependencies.py'"],"metadata":{"id":"ZmlHuIgWE5x5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["%run '/content/drive/MyDrive/Colab Notebooks/SEO/notebooks/GitHub.ipynb'"],"metadata":{"id":"do4Bf0uw-y8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["from webscraper import WebsiteScraper\n","\n","scraper = WebsiteScraper(start_url=START_URL, max_pages=20, excluded_keywords=EXCLUDED_KEYWORDS)\n","\n","original_texts = scraper.get_filtered_texts()"],"metadata":{"id":"kksmfARinqon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆwebtext analysis + SEO"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["from llmprocessor import LLMProcessor\n","\n","llm_processor = LLMProcessor(PROJECT_ROOT, original_texts)\n","\n","optimized_texts = llm_processor.run_all()"],"metadata":{"id":"q3isJXZoebv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_texts.keys()"],"metadata":{"id":"2Ku3GhEImXCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_sections_to_json(texts, keys):\n","    \"\"\"\n","    Extrahiert Abschnitte aus mehreren Texten und konvertiert sie in JSON.\n","    Gesucht werden die √úberschriften 'Analyse', 'SEO', 'Erkl√§rung' und der jeweils\n","    folgende Inhalt bis zur n√§chsten √úberschrift oder zum Ende.\n","    \"\"\"\n","\n","    all_sections = []  # Liste f√ºr alle Abschnitte\n","\n","    # Neues, robusteres Pattern:\n","    pattern = re.compile(\n","        r\"(?m)^\\s*(Analyse|SEO|Erkl√§rung)\\s*(?:\\r?\\n\\s*)+\"\n","        r\"(.*?)(?=^\\s*(?:Analyse|SEO|Erkl√§rung)|\\Z)\",\n","        flags=re.DOTALL\n","    )\n","\n","    for text in texts:\n","        sections_dict = {}\n","        matches = pattern.findall(text)\n","        for match in re.finditer(pattern, text):\n","            heading = match.group(1)\n","            content = match.group(2).strip()\n","            sections_dict[heading] = content\n","\n","        all_sections.append(sections_dict)\n","\n","    # Kombinieren der Abschnitte mit Keys\n","    final_json_data = {}\n","    for i, sections_dict in enumerate(all_sections):\n","        key = keys[i]  # Key aus der Liste holen\n","        final_json_data[key] = sections_dict  # Abschnitte zum Dictionary hinzuf√ºgen\n","\n","    json_data = json.dumps(final_json_data, indent=4, ensure_ascii=False)\n","    return json_data\n","\n","# Beispielnutzung\n","if __name__ == \"__main__\":\n","    keys = filtered_urls\n","    json_output = extract_sections_to_json(combined_analysis_list, keys)\n","    seo_json = json.loads(json_output)\n","    print(seo_json)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"DkqmEsw-vnwn","executionInfo":{"status":"error","timestamp":1740981554700,"user_tz":-60,"elapsed":68,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"ed496e24-e7af-47be-903e-a028ccc3995b"},"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'filtered_urls' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-180871583f46>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Beispielnutzung\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mjson_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sections_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_analysis_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mseo_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'filtered_urls' is not defined"]}]},{"cell_type":"code","source":["# alten website text zu json hinzuf√ºgen\n","# for i, (_, url_content) in enumerate(seo_json.items()):\n","#    url_content[\"alt\"] = page_text_list[i]"],"metadata":{"id":"IsMhRnunGjee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìä SEO Analysis 2"],"metadata":{"id":"V50QPoW3F6wW"}},{"cell_type":"code","source":["# text clean up\n","\n","def clean_text(text):\n","    # Ersetze Zeilenumbr√ºche durch ein Leerzeichen\n","    text = text.replace('\\n', ' ')\n","\n","    # Beispiel: Entferne alle Zeichen, die nicht Buchstaben (inkl. Umlaute),\n","    # Ziffern, Satzzeichen oder Leerzeichen sind\n","    # Du kannst das RegEx anpassen, wenn du z.B. bestimmte Zeichen behalten oder entfernen willst\n","    text = re.sub(r'[^a-zA-Z0-9√§√∂√º√Ñ√ñ√ú√ü.,!?;:\\-\\s]', '', text)\n","\n","    # Mehrere aufeinanderfolgende Leerzeichen durch ein einzelnes ersetzen\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    # F√ºhrende oder nachfolgende Leerzeichen entfernen\n","    text = text.strip()\n","\n","    return text\n","\n","\n","combined_analysis_list_clean = []\n","page_text_list_clean = []\n","\n","for text in page_text_list:\n","  page_text_list_clean.append(clean_text(text))\n","\n","for text in combined_analysis_list:\n","  combined_analysis_list_clean.append(clean_text(text))\n","\n","\n"],"metadata":{"id":"Mokk2DMLGGOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple stats\n","\n","def text_stats(text):\n","    words = text.split()\n","    return {\n","        \"Zeichenanzahl\": len(text),\n","        \"Wortanzahl\": len(words),\n","        \"Satzanzahl\": text.count('.') + text.count('!') + text.count('?')\n","    }\n","\n","\n","vn = [page_text_list_clean, combined_analysis_list_clean]\n","\n","for _, listlist in enumerate(vn):\n","\n","  print(f'{[\"original\", \"SEO\"][_]}')\n","\n","  for text in listlist:\n","    print(text_stats(text))\n","\n","\n","\n"],"metadata":{"id":"GDBZiKVnH9dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def get_word_frequencies(text):\n","    words = text.lower().split()\n","    return Counter(words)\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","\n","  # Wortfrequenzen berechnen\n","  original_freq = get_word_frequencies(page_text_list_clean[_])\n","  optimized_freq = get_word_frequencies(combined_analysis_list_clean[_])\n","\n","  # Unterschied berechnen\n","  diff = {word: optimized_freq[word] - original_freq[word] for word in set(original_freq) | set(optimized_freq)}\n","\n","  # Sortiert ausgeben (absteigend nach √Ñnderung)\n","  sorted_diff = sorted(diff.items(), key=lambda x: x[1], reverse=True)\n","  for word, change in sorted_diff:\n","      print(f\"{word}: {change}\")\n"],"metadata":{"id":"qGiMiXwIH9ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"mqtebLSOeNzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","try:\n","    keywords_sstring = json.loads(keywords_final)\n","except json.JSONDecodeError:\n","    # Fallback if JSON decoding fails (e.g., if the response isn't a valid JSON string)\n","    # Try to extract the list using regex\n","    import re\n","    match = re.search(r'\\[(.*?)\\]', keywords_final)\n","    if match:\n","        keywords_sstring = match.group(1).split(', ')\n","        keywords_sstring = [item.strip().strip(\"'\").strip('\"') for item in keywords_sstring]\n","    else:\n","        # Handle case where the list couldn't be extracted\n","        keywords_sstring = []  # Or raise an exception, etc.\n","\n","keywords_sstring = \", \".join(keywords_sstring)\n","keywords_sstring_list = keywords_sstring.split(\", \")"],"metadata":{"id":"yFAfe_--E8os"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import ast\n","\n","\n","# keywords_sstring = ast.literal_eval(keywords_final)\n","# keywords_sstring = ast.literal_eval(keywords_final.replace('chars ', ''))\n","\n","\n","# keywords_sstring = \", \".join(keywords_sstring)\n","# keywords_sstring_list = keywords_sstring.split(\", \")\n","# keywords_sstring_list"],"metadata":{"id":"_DYkLx8uZG7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B7iE2va3gxwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for _, listlist in enumerate(combined_analysis_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, combined_analysis_list_clean[_]])\n","\n","  # Cosinus-√Ñhnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"√Ñhnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"W5bd8b1yH9YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, listlist in enumerate(page_text_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, page_text_list_clean[_]])\n","\n","  # Cosinus-√Ñhnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"√Ñhnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"z1-eRoarZ-9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","keywords = ast.literal_eval(keywords_final)\n","\n","def keyword_density(text, keywords):\n","    words = text.lower().split()\n","    total_words = len(words)\n","    # Check if total_words is 0 to avoid ZeroDivisionError\n","    if total_words == 0:\n","        return {kw: 0 for kw in keywords}  # Return 0 density for all keywords if text is empty\n","    density = {kw: words.count(kw) / total_words * 100 for kw in keywords}\n","    return density\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","  # Berechnung f√ºr beide Texte\n","  original_density = keyword_density(page_text_list_clean[_], keywords)\n","  optimized_density = keyword_density(combined_analysis_list_clean[0], keywords)\n","\n","  print(\"Original:\", original_density)\n","  print(\"SEO-Optimiert:\", optimized_density)"],"metadata":{"id":"cWop0AUQJ04x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","# Keywords und deren √Ñnderungen\n","keywords = list(original_density.keys())\n","original_values = list(original_density.values())\n","optimized_values = list(optimized_density.values())\n","\n","x = np.arange(len(keywords))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","ax.bar(x - width/2, original_values, width, label=\"Original\")\n","ax.bar(x + width/2, optimized_values, width, label=\"SEO-Optimiert\")\n","\n","ax.set_xlabel(\"Keywords\")\n","ax.set_ylabel(\"Keyword-Dichte (%)\")\n","ax.set_title(\"Keyword-Dichte: Vorher vs. Nachher\")\n","ax.set_xticks(x)\n","ax.set_xticklabels(keywords)\n","ax.legend()\n","\n","plt.show()\n"],"metadata":{"id":"rqHULQbhMsWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RppPqEA7MsUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"223xoPNxMsR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìäSEO 3"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["nlp = spacy.load('de_core_news_sm')\n","stop_words = set(stopwords.words('german'))"],"metadata":{"id":"SMTIt789vysc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seo_texte = []\n","for i, (_, url_content) in enumerate(seo_json.items()):\n","  seo_texte.append(url_content['SEO'])"],"metadata":{"id":"FS4UtISLvBof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_texts = page_text_list_clean\n","optimized_texts = seo_texte\n","seo_keywords = keywords_sstring_list"],"metadata":{"id":"Md68SpZfHRrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text):\n","    # 1. Lowercase\n","    text = text.lower()\n","    # 2. Entferne Zahlen, Sonderzeichen (optional)\n","    # text = re.sub(r'[0-9]+', '', text)\n","    # text = re.sub(r'[^\\w\\s]', '', text)\n","    # 3. Tokenisierung mit spaCy\n","    doc = nlp(text)\n","    # 4. Entferne Stopw√∂rter, ggf. Lemmatisierung\n","    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.lemma_ not in stop_words and token.is_alpha]\n","    return ' '.join(tokens)\n"],"metadata":{"id":"0HPl9WcSqWJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_original = [preprocess_text(t) for t in original_texts]\n","preprocessed_optimized = [preprocess_text(t) for t in optimized_texts]\n","preprocessed_keywords = [preprocess_text(k) for k in seo_keywords]\n","\n","print(\"Vorverarbeitete Originaltexte:\", preprocessed_original)\n","print(\"Vorverarbeitete Optimierte Texte:\", preprocessed_optimized)\n","print(\"Vorverarbeitete Keywords:\", preprocessed_keywords)"],"metadata":{"id":"HY46gMnYqZwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Zusammenf√ºhren aller Texte f√ºr den Vektorizer\n","all_texts = preprocessed_original + preprocessed_optimized + preprocessed_keywords\n","\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(all_texts)\n","\n","# Indizes definieren\n","original_indices = range(len(original_texts))  # 0, 1, ...\n","optimized_indices = range(len(original_texts), len(original_texts) + len(optimized_texts))\n","keyword_indices = range(len(original_texts) + len(optimized_texts), len(all_texts))\n","\n","# Cosine Similarities berechnen\n","similarities_original_to_keywords = []\n","similarities_optimized_to_keywords = []\n","\n","for i in original_indices:\n","    row_sim = []\n","    for j in keyword_indices:\n","        sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n","        row_sim.append(sim)\n","    similarities_original_to_keywords.append(row_sim)\n","\n","for i in optimized_indices:\n","    row_sim = []\n","    for j in keyword_indices:\n","        sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n","        row_sim.append(sim)\n","    similarities_optimized_to_keywords.append(row_sim)\n","\n","# Durchschnittliche Similarities\n","avg_original_sim = np.mean(similarities_original_to_keywords, axis=0)\n","avg_optimized_sim = np.mean(similarities_optimized_to_keywords, axis=0)\n","\n","print(\"Durchschnittliche Similarities (Original -> Keywords):\", avg_original_sim)\n","print(\"Durchschnittliche Similarities (Optimiert -> Keywords):\", avg_optimized_sim)\n"],"metadata":{"id":"qWe8yFoYqZtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_clean = [kw for kw in seo_keywords]  # F√ºr Beschriftung im Diagramm\n","\n","df_sim = pd.DataFrame({\n","    '': keywords_clean,\n","    'Original_Sim': avg_original_sim,\n","    'Optimiert_Sim': avg_optimized_sim\n","})\n","\n","# Schmelzen f√ºr Seaborn\n","df_melted = df_sim.melt(id_vars='', var_name='Textart', value_name='Cosine Similarity')\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='', y='Cosine Similarity', hue='Textart', data=df_melted)\n","plt.title('Durchschnittliche Cosine Similarity zu Keywords')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"OTLT1fLYqZpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_keyword_occurrences(text, keyword):\n","    # split text\n","    text_tokens = text.split()\n","    keyword_tokens = keyword.split()\n","\n","    count = 0\n","    for i in range(len(text_tokens) - len(keyword_tokens) + 1):\n","        if text_tokens[i:i+len(keyword_tokens)] == keyword_tokens:\n","            count += 1\n","    return count\n","\n","original_counts = []\n","optimized_counts = []\n","\n","for kw in preprocessed_keywords:\n","    o_sum = sum(count_keyword_occurrences(t, kw) for t in preprocessed_original)\n","    opt_sum = sum(count_keyword_occurrences(t, kw) for t in preprocessed_optimized)\n","    original_counts.append(o_sum)\n","    optimized_counts.append(opt_sum)\n","\n","df_counts = pd.DataFrame({\n","    'Keyword': keywords_clean,\n","    'Original Count': original_counts,\n","    'Optimiert Count': optimized_counts\n","})\n","\n","df_counts_melt = df_counts.melt(id_vars='Keyword', var_name='Textart', value_name='Count')\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='Keyword', y='Count', hue='Textart', data=df_counts_melt)\n","plt.title('Keyword H√§ufigkeit (exakte Matches)')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"GN10PEIuqZl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_original_text = ' '.join(preprocessed_original)\n","all_optimized_text = ' '.join(preprocessed_optimized)\n","\n","wc_original = WordCloud(width=600, height=400, background_color='white').generate(all_original_text)\n","wc_optimized = WordCloud(width=600, height=400, background_color='white').generate(all_optimized_text)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n","ax[0].imshow(wc_original, interpolation='bilinear')\n","ax[0].set_title('Wordcloud Original Texte')\n","ax[0].axis('off')\n","\n","ax[1].imshow(wc_optimized, interpolation='bilinear')\n","ax[1].set_title('Wordcloud SEO-Optimierte Texte')\n","ax[1].axis('off')\n","\n","plt.show()\n"],"metadata":{"id":"mKKsK15mqZit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Similarity pro Seite und Keyword in eine Matrix\n","sim_matrix_original = np.array(similarities_original_to_keywords)\n","sim_matrix_optimized = np.array(similarities_optimized_to_keywords)\n","\n","plt.figure(figsize=(8, 5))\n","sns.heatmap(sim_matrix_original, annot=True, cmap='Blues', xticklabels=keywords_clean)\n","plt.title(\"Cosine Similarities (Originaltexte -> Keywords)\")\n","plt.xlabel(\"Keywords\")\n","plt.ylabel(\"Original-Seiten\")\n","plt.show()\n","\n","plt.figure(figsize=(8, 5))\n","sns.heatmap(sim_matrix_optimized, annot=True, cmap='Greens', xticklabels=keywords_clean)\n","plt.title(\"Cosine Similarities (SEO-optimierte Texte -> Keywords)\")\n","plt.xlabel(\"Keywords\")\n","plt.ylabel(\"Optimierte-Seiten\")\n","plt.show()\n"],"metadata":{"id":"ldVTTxkdtlvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispielhafter Pandas DataFrame mit Zeit- und SEO-Daten\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","# Angenommen, du hast historische Daten pro Monat\n","data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],  # 2% -> 3.1%\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]  # in Sekunden\n","}\n","\n","df_metrics = pd.DataFrame(data)\n","df_metrics[\"Date\"] = pd.to_datetime(df_metrics[\"Date\"])\n","\n","print(df_metrics)\n"],"metadata":{"id":"i33foHZ7zPrW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","fig, ax1 = plt.subplots(figsize=(10,5))\n","\n","# Prim√§re y-Achse (Organic Sessions)\n","ax1.plot(df_metrics['Date'], df_metrics['Organic_Sessions'], color='blue', marker='o', label='Organic Sessions')\n","ax1.set_xlabel('Datum')\n","ax1.set_ylabel('Anzahl organischer Sitzungen', color='blue')\n","ax1.tick_params(axis='y', labelcolor='blue')\n","\n","# Sekund√§re y-Achse (Conversion Rate)\n","ax2 = ax1.twinx()\n","ax2.plot(df_metrics['Date'], df_metrics['Conversion_Rate'], color='red', marker='s', label='Conversion Rate')\n","ax2.set_ylabel('Conversion Rate', color='red')\n","ax2.tick_params(axis='y', labelcolor='red')\n","\n","fig.tight_layout()\n","plt.title('Entwicklung der Sitzungen und Conversion Rate')\n","plt.show()\n"],"metadata":{"id":"c8SO4GBLzVoW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,5))\n","plt.bar(df_metrics['Date'].dt.strftime('%b %Y'), df_metrics['Average_Time_on_Page'], color='green')\n","plt.title('Durchschnittliche Verweildauer pro Monat')\n","plt.xlabel('Monat')\n","plt.ylabel('Verweildauer in Sekunden')\n","plt.show()"],"metadata":{"id":"QGTI7DSOzj7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","# Function to visualize 3D vectors and their cosine similarity\n","def plot_3d_cosine_similarity(vec1, vec2, title=\"3D Cosine Similarity Visualization\"):\n","    fig = plt.figure(figsize=(12, 8))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Plot vectors in 3D\n","    ax.quiver(0, 0, 0, vec1[0], vec1[1], vec1[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec2[0], vec2[1], vec2[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Cosine similarity calculation\n","    cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n","\n","    # Plot settings\n","    ax.set_xlim([-1, 1])\n","    ax.set_ylim([-1, 1])\n","    ax.set_zlim([-1, 1])\n","    ax.set_title(f\"{title}\\nCosine Similarity: {cos_sim:.2f}\")\n","    ax.set_xlabel('X')\n","    ax.set_ylabel('Y')\n","    ax.set_zlabel('Z')\n","    ax.legend()\n","\n","    plt.show()\n","\n","# Example 3D vectors representing keyword frequencies in different dimensions (Old Text and Optimized Text)\n","vec_old_3d = np.array([0.6, 0.6, 0.3])\n","vec_optimized_3d = np.array([0.9, 0.4, 0.5])\n","\n","# Visualizing cosine similarity between the old and optimized texts in 3D\n","plot_3d_cosine_similarity(vec_old_3d, vec_optimized_3d, \"3D Cosine Similarity between Old and Optimized Texts\")\n"],"metadata":{"id":"6QlhTrYPkcZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","\n","# Function to visualize keyword similarity in 3D using spheres to represent keyword clouds\n","def plot_3d_keyword_similarity(title=\"Keyword Cloud Similarity Visualization\"):\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Generate data points for the keyword cloud\n","    np.random.seed(42)\n","    keywords = np.random.rand(50, 3) - 0.5  # Random points for keyword representation\n","    old_text = keywords + np.array([0.4, 0.4, 0.4])  # Shifted to represent the old text\n","    optimized_text = keywords + np.array([-0.4, -0.3, -0.2])  # Shifted to represent the optimized text\n","\n","    # Plot the keyword cloud\n","    ax.scatter(keywords[:, 0], keywords[:, 1], keywords[:, 2], color='g', label='Keywords', alpha=0.6, s=40)\n","\n","    # Plot old text keyword distribution\n","    ax.scatter(old_text[:, 0], old_text[:, 1], old_text[:, 2], color='r', label='Old Text', alpha=0.7, s=50)\n","\n","    # Plot optimized text keyword distribution\n","    ax.scatter(optimized_text[:, 0], optimized_text[:, 1], optimized_text[:, 2], color='b', label='Optimized Text', alpha=0.7, s=50)\n","\n","    # Plot settings\n","    ax.set_xlim([-1, 1])\n","    ax.set_ylim([-1, 1])\n","    ax.set_zlim([-1, 1])\n","    ax.set_title(title)\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","# Visualizing keyword distribution in 3D for Old Text, Optimized Text, and Keywords\n","plot_3d_keyword_similarity(\"3D Keyword Cloud Similarity: Old Text vs Optimized Text\")\n"],"metadata":{"id":"rTEF2o98kcXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visual explanation of cosine similarity with embeddings, dot product, and how the result turns into a probability\n","\n","def plot_cosine_similarity_steps():\n","    fig = plt.figure(figsize=(18, 6))\n","\n","    # Step 1: Plot the embeddings\n","    ax1 = fig.add_subplot(131, projection='3d')\n","    vec1 = np.array([0.8, 0.6, 0.3])\n","    vec2 = np.array([0.9, 0.4, 0.5])\n","\n","    ax1.quiver(0, 0, 0, vec1[0], vec1[1], vec1[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax1.quiver(0, 0, 0, vec2[0], vec2[1], vec2[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","    ax1.set_xlim([0, 1])\n","    ax1.set_ylim([0, 1])\n","    ax1.set_zlim([0, 1])\n","    ax1.set_title('Step 1: Embeddings (Keyword Vectors)')\n","    ax1.set_xlabel('X')\n","    ax1.set_ylabel('Y')\n","    ax1.set_zlabel('Z')\n","    ax1.legend()\n","\n","    # Step 2: Calculate and visualize the dot product\n","    ax2 = fig.add_subplot(132)\n","    dot_product = np.dot(vec1, vec2)\n","    ax2.bar(['Dot Product'], [dot_product], color='orange')\n","    ax2.set_ylim(0, 1.5)\n","    ax2.set_title('Step 2: Dot Product Calculation')\n","    ax2.text(0, dot_product + 0.05, f'{dot_product:.2f}', ha='center', fontsize=12)\n","\n","    # Step 3: Convert to cosine similarity and interpret as probability\n","    ax3 = fig.add_subplot(133)\n","    cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n","    ax3.bar(['Cosine Similarity'], [cos_sim], color='green')\n","    ax3.set_ylim(0, 1.1)\n","    ax3.set_title('Step 3: Cosine Similarity as Probability')\n","    ax3.text(0, cos_sim + 0.05, f'{cos_sim:.2f}', ha='center', fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_cosine_similarity_steps()\n"],"metadata":{"id":"-J9rBQbokcUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization showing keyword vector, old text vector, and optimized text vector with clear comparison\n","\n","def plot_3d_cosine_comparison():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Define vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.6, 0.4, 0.3])\n","    vec_optimized_text = np.array([0.75, 0.65, 0.55])\n","\n","    # Plot vectors\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='g', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison()\n"],"metadata":{"id":"LX0sKs1ZkcSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization with cosine similarity values and enhanced color scheme\n","\n","def plot_3d_cosine_comparison_with_values():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Define vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.2, 0.15, 0.1])\n","    vec_optimized_text = np.array([0.6, 0.4, 0.3])\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Calculate cosine similarity\n","    cos_sim_old = cosine_similarity([vec_keywords], [vec_old_text])[0][0]\n","    cos_sim_optimized = cosine_similarity([vec_keywords], [vec_optimized_text])[0][0]\n","\n","    # Add cosine similarity values as annotations\n","    ax.text(0.4, 0.5, 0.7, f'Cosine Similarity (Old): {cos_sim_old:.2f}', color='#FF5733', fontsize=12)\n","    ax.text(0.4, 0.5, 0.65, f'Cosine Similarity (Optimized): {cos_sim_optimized:.2f}', color='#1E90FF', fontsize=12)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Cosine Similarity Values\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_values()\n"],"metadata":{"id":"qn6l05MTkcP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust vectors to create larger differences in cosine similarity values\n","def plot_3d_cosine_comparison_with_adjusted_values():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.4, 0.3, 0.2])  # Adjusted to have lower similarity (~0.6)\n","    vec_optimized_text = np.array([0.6, 0.5, 0.4])  # Adjusted to have higher similarity (~0.7)\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Calculate cosine similarity\n","    cos_sim_old = cosine_similarity([vec_keywords], [vec_old_text])[0][0]\n","    cos_sim_optimized = cosine_similarity([vec_keywords], [vec_optimized_text])[0][0]\n","\n","    # Add cosine similarity values as annotations\n","    ax.text(0.4, 0.5, 0.7, f'Cosine Similarity (Old): {cos_sim_old:.3f}', color='#FF5733', fontsize=12)\n","    ax.text(0.4, 0.5, 0.65, f'Cosine Similarity (Optimized): {cos_sim_optimized:.3f}', color='#1E90FF', fontsize=12)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Adjusted Cosine Similarity Values\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_adjusted_values()\n"],"metadata":{"id":"VeAwSLMMkcMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust vectors to create cosine similarities of approximately 0.15 and 0.19\n","def plot_3d_cosine_comparison_with_low_similarity():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text to achieve low similarities\n","    vec_keywords = np.array([0.9, 0.8, 0.7])\n","    vec_old_text = np.array([0.1, 0.2, 0.15])  # Cosine similarity ~0.15\n","    vec_optimized_text = np.array([0.2, 0.25, 0.18])  # Cosine similarity ~0.19\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Calculate cosine similarity\n","    cos_sim_old = cosine_similarity([vec_keywords], [vec_old_text])[0][0]\n","    cos_sim_optimized = cosine_similarity([vec_keywords], [vec_optimized_text])[0][0]\n","\n","    # Add cosine similarity values as annotations\n","    ax.text(0.4, 0.5, 0.7, f'Cosine Similarity (Old): {cos_sim_old:.2f}', color='#FF5733', fontsize=12)\n","    ax.text(0.4, 0.5, 0.65, f'Cosine Similarity (Optimized): {cos_sim_optimized:.2f}', color='#1E90FF', fontsize=12)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Low Cosine Similarity Values\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_low_similarity()\n"],"metadata":{"id":"awmpffH_kcJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization with vector names instead of cosine similarity values\n","def plot_3d_cosine_comparison_with_labels():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.4, 0.3, 0.2])\n","    vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Add labels to the vector tips\n","    ax.text(vec_keywords[0], vec_keywords[1], vec_keywords[2], 'SEO Keywords', color='#4CAF50', fontsize=12, fontweight='bold')\n","    ax.text(vec_old_text[0], vec_old_text[1], vec_old_text[2], 'Old Text', color='#FF5733', fontsize=12, fontweight='bold')\n","    ax.text(vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], 'Optimized Text', color='#1E90FF', fontsize=12, fontweight='bold')\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Labels\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_labels()\n"],"metadata":{"id":"zAQSqDAXxD03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers torch\n","import torch\n","from transformers import BertTokenizer, BertModel\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"2CDAU4AVivSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Function to get BERT embeddings for a word\n","def get_bert_embedding(word):\n","    inputs = tokenizer(word, return_tensors='pt')\n","    outputs = model(**inputs)\n","    # Take the mean of the last hidden state as the word embedding\n","    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n","\n","# List of words for embeddings\n","words = [\"king\", \"queen\", \"onion\", \"man\",\"feather\",\"scale\", \"lioness\", \"bull\", \"cow\", \"slither\",\"skin\", \"wing\", \"fin\",\n","         \"green\", \"fly\", \"run\", \"snake\", \"sneak\", \"wolf\", \"fur\", \"swim\", \"fish\", \"air\", \"water\", \"swimming\", \"bird\",\n","         \"cheetah\", \"woman\", \"apple\", \"orange\", \"car\", \"train\", \"dog\", \"running\",\"cat\", \"red\", \"yellow\", \"banana\", \"lion\", \"wolf\", \"dog\"\n","         ,\"day\", \"sun\", \"night\", \"moon\"]\n","\n","# Generate BERT embeddings for all words\n","bert_embeddings = np.array([get_bert_embedding(word) for word in words])\n","\n","# Function to calculate pairwise distances for word relationships\n","def calculate_pairwise_distances(word_pairs):\n","    distance_results = []\n","    for (word1, word2, word3, word4) in word_pairs:\n","        idx1, idx2, idx3, idx4 = words.index(word1), words.index(word2), words.index(word3), words.index(word4)\n","        distance1 = np.linalg.norm(bert_embeddings[idx1] - bert_embeddings[idx2])  # Distance between word1 and word2\n","        distance2 = np.linalg.norm(bert_embeddings[idx3] - bert_embeddings[idx4])  # Distance between word3 and word4\n","        distance_results.append({\n","            \"Word Pair 1\": f\"{word1} - {word2}\",\n","            \"Distance 1\": round(distance1, 2),\n","            \"Word Pair 2\": f\"{word3} - {word4}\",\n","            \"Distance 2\": round(distance2, 2)\n","        })\n","\n","    return pd.DataFrame(distance_results)\n","\n","# Define specific word pairs to compare distances\n","word_pairs_for_distances = [\n","    (\"king\", \"man\", \"queen\", \"woman\"),\n","    (\"apple\", \"red\", \"banana\", \"yellow\"),\n","    (\"day\", \"sun\", \"night\", \"moon\")\n","]\n","\n","# Calculate pairwise distances using BERT embeddings\n","pairwise_distance_df = calculate_pairwise_distances(word_pairs_for_distances)\n","\n","# Display the resulting table\n","import IPython.display as display\n","display.display(pairwise_distance_df)"],"metadata":{"id":"ROYD3s4SGC9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Function to get BERT embeddings for a word\n","def get_bert_embedding(word):\n","    inputs = tokenizer(word, return_tensors='pt')\n","    outputs = model(**inputs)\n","    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n","\n","# List of words and their embeddings\n","words = [\"king\", \"queen\", \"man\", \"woman\",\"red\", \"yellow\", \"banana\", \"apple\", \"day\", \"sun\", \"night\", \"moon\"]\n","bert_embeddings = np.array([get_bert_embedding(word) for word in words])\n","\n","# Reduce dimensionality to 3D using PCA\n","pca = PCA(n_components=3)\n","bert_embeddings_3d = pca.fit_transform(bert_embeddings)\n","\n","# Create Plotly figure\n","fig = go.Figure()\n","\n","\n","# Add 3D scatter points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[bert_embeddings_3d[i, 0]], y=[bert_embeddings_3d[i, 1]], z=[bert_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=10, color='skyblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Customize layout with initial camera settings\n","ticktext = [\"-4\", \"-2\", \"0\", \"2\", \"4\"]\n","tickvals = [-5, -2, 0, 2, 5]\n","prange=[-5, 5]\n","\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of BERT Word Embeddings\",\n","    scene=dict(\n","        xaxis=dict(\n","            range=prange,\n","            title=\"PCA Component 1\",\n","            tickmode=\"array\",\n","            tickvals=tickvals,  # Custom tick positions\n","            ticktext=ticktext # Custom tick labels (optional)\n","        ),\n","        yaxis=dict(\n","            range=[-4,3],\n","            title=\"PCA Component 2\",\n","            tickvals=tickvals,  # Custom tick positions\n","            ticktext=ticktext\n","        ),\n","        zaxis=dict(\n","            range=prange,\n","            title=\"PCA Component 3\",\n","            tickmode=\"array\",\n","            tickvals=tickvals,  # Custom tick positions\n","            ticktext=ticktext  # Example custom labels\n","        ),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=100, b=100)\n",")\n","\n","# Highlight the relationships with arrows\n","def add_arrow(word1, word2, color):\n","    idx1, idx2 = words.index(word1), words.index(word2)\n","    fig.add_trace(go.Scatter3d(\n","        x=[bert_embeddings_3d[idx1, 0], bert_embeddings_3d[idx2, 0]],\n","        y=[bert_embeddings_3d[idx1, 1], bert_embeddings_3d[idx2, 1]],\n","        z=[bert_embeddings_3d[idx1, 2], bert_embeddings_3d[idx2, 2]],\n","        mode='lines',\n","        line=dict(color=color, width=4),\n","        showlegend=False\n","    ))\n","\n","# Add arrows for specific relationships\n","add_arrow(\"king\", \"man\", \"crimson\")\n","add_arrow(\"queen\", \"woman\", \"crimson\")\n","add_arrow(\"apple\", \"red\", \"olivedrab\")\n","add_arrow(\"banana\", \"yellow\", \"olivedrab\")\n","add_arrow(\"day\", \"sun\", \"orange\")\n","add_arrow(\"night\", \"moon\", \"orange\")\n","\n","\n","\n","fig.show()\n"],"metadata":{"id":"hgf2jzILNLrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","\n","aliceblue, antiquewhite, aqua, aquamarine, azure,\n","beige, bisque, black, blanchedalmond, blue,\n","blueviolet, brown, burlywood, cadetblue,\n","chartreuse, chocolate, coral, cornflowerblue,\n","cornsilk, crimson, cyan, darkblue, darkcyan,\n","darkgoldenrod, darkgray, darkgrey, darkgreen,\n","darkkhaki, darkmagenta, darkolivegreen, darkorange,\n","darkorchid, darkred, darksalmon, darkseagreen,\n","darkslateblue, darkslategray, darkslategrey,\n","darkturquoise, darkviolet, deeppink, deepskyblue,\n","dimgray, dimgrey, dodgerblue, firebrick,\n","floralwhite, forestgreen, fuchsia, gainsboro,\n","ghostwhite, gold, goldenrod, gray, grey, green,\n","greenyellow, honeydew, hotpink, indianred, indigo,\n","ivory, khaki, lavender, lavenderblush, lawngreen,\n","lemonchiffon, lightblue, lightcoral, lightcyan,\n","lightgoldenrodyellow, lightgray, lightgrey,\n","lightgreen, lightpink, lightsalmon, lightseagreen,\n","lightskyblue, lightslategray, lightslategrey,\n","lightsteelblue, lightyellow, lime, limegreen,\n","linen, magenta, maroon, mediumaquamarine,\n","mediumblue, mediumorchid, mediumpurple,\n","mediumseagreen, mediumslateblue, mediumspringgreen,\n","mediumturquoise, mediumvioletred, midnightblue,\n","mintcream, mistyrose, moccasin, navajowhite, navy,\n","oldlace, olive, olivedrab, orange, orangered,\n","orchid, palegoldenrod, palegreen, paleturquoise,\n","palevioletred, papayawhip, peachpuff, peru, pink,\n","plum, powderblue, purple, red, rosybrown,\n","royalblue, rebeccapurple, saddlebrown, salmon,\n","sandybrown, seagreen, seashell, sienna, silver,\n","skyblue, slateblue, slategray, slategrey, snow,\n","springgreen, steelblue, tan, teal, thistle, tomato,\n","turquoise, violet, wheat, white, whitesmoke,\n","yellow, yellowgreen\n","\n","\"\"\""],"metadata":{"id":"3SolLFOgNLij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_analysis_list"],"metadata":{"id":"hH3FCp3VNLcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"299u6UB_T9Y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fAUp050WT9Vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o9JlW5XpT9SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qtUH6A7dT9N3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1b_7q9_8T9J2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["%%capture\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout f√ºr \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox f√ºr zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um Zeilenumbr√ºche aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und Erkl√§rung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">{{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from jinja2 import Template\n","\n","def json_to_html(json_data):\n","    # HTML-Template mit EINER Spalte f√ºr \"SEO\" (die \"alt\"-Spalte entf√§llt)\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            /* Zeilenumbr√ºche aus dem JSON in <br> wandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- \"Analyse\" normal untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">\n","                {{ sections.Analyse | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- \"Erkl√§rung\" normal untereinander -->\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">\n","                {{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}\n","            </p>\n","\n","            <!-- NUR noch die \"SEO\"-Spalte -->\n","            <div class=\"column\">\n","                <p class=\"header\">SEO</p>\n","                <p class=\"content\">\n","                    {{ sections.SEO | replace('\\\\n','<br>') | safe }}\n","                </p>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    template = Template(html_template)\n","    return template.render(data=json_data)\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"Cx6qfyVfJzI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschlie√üend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, R√§nder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthop√§din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\"\n","\n","\"Sie hat ihren Master of Science in Kieferorthop√§die von der Danube Private University, Krems, √ñsterreich, und hat an der Heinrich-Heine-Universit√§t D√ºsseldorf abgeschlossen.\"\n","\n","\"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln. \"\n","\n","\"behandlungsorientierte Zahnberatung\"\n","\n","\"√§stehthetisches L√§cheln\"\n","\n","\"Nachdem Ihr Behandlungsplan von der Krankenkasse genehmigt wurde\" \"Nachdem Ihr Behandlungsplan von der Krankenkasse best√§tigt wurde\"\n","\n","\"Der aktuelle Text zur Zahnspangenpraxis\""],"metadata":{"id":"fH4mRdPXceeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u"},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def chunk_text_2(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    F√ºr echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text_2(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk √§hneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenf√ºgen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","# test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","# retrieved_context = get_context_from_vector_store(test_chunk)\n","# print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu pr√ºfen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu ber√ºcksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu pr√ºfenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu pr√ºfen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur Pr√ºfung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text gr√ºndlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu ver√§ndern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zus√§tzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein g√ºltiges JSON zur√ºckgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine g√ºltige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","all_corrected_texts = []\n","all_new_mistakes = []\n","\n","#for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","#    corrected_text_chunks = []\n","\n","#    for chunk in text_chunks:\n","#        # 3a) Kontext abfragen\n","#        context = get_context_from_vector_store(chunk)\n","#\n","#\n","#       # 4a) Prompt ChatGPT (Korrektur)\n","#        result = proofread_text_with_context(chunk, context)\n","#\n","#        corrected_text = result[\"corrected_text\"]\n","#        new_mistakes = result[\"new_mistakes_found\"]\n","#\n","#        # Sammeln\n","#        corrected_text_chunks.append(corrected_text)\n","#        all_new_mistakes.extend(new_mistakes)\n","#\n","#    # Pro SEO-Text f√ºgen wir die korrigierten Chunks zusammen.\n","#    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","#    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# for _ in all_corrected_texts:\n","#   print(_)"],"metadata":{"id":"qIUE9Dfl5YzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3"},"execution_count":null,"outputs":[]}]}