{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["o38bsnD-EWcq","vNVT-lr5jejz","FA2RIeko9reT","E0k0HmUZHmkg","qGy_OuMyIC9C","4v6-jGMeO9Zt","HraJ3_5SJHSv"],"authorship_tag":"ABX9TyMIZIO6nl6yZjAVUjFHH7Vy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# â›© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["#%%capture\n","import os\n","from google.colab import drive\n","from google.colab import userdata\n","drive.mount('/content/drive',\n","            force_remount=True\n","            )\n","\n","\n","notebookname = 'SEO.ipynb'\n","\n","class github:\n","    def __init__(self, github_pat, github_email, github_username, github_repo, gdrive_notebook_folder, notebook_name):\n","        self.github_pat = userdata.get(github_pat)\n","        self.github_email = userdata.get(github_email)\n","        self.github_username = userdata.get(github_username)\n","        self.github_repo = userdata.get(github_repo)\n","        self.gdrive_notebook_folder = userdata.get(gdrive_notebook_folder)\n","        self.notebook_name = notebook_name\n","\n","    def clone_repo(self):\n","        # Source file path in Google Drive\n","        source_file_path = f\"/content/drive/MyDrive/{self.gdrive_notebook_folder}/{self.notebook_name}\"\n","\n","        # Repository details\n","        repo_url = f'https://{self.github_pat}@github.com/{self.github_username}/{self.github_repo}.git'\n","\n","        # Clone the private repository\n","        !git clone {repo_url} cloned-repo\n","        os.chdir('cloned-repo')  # Switch to the cloned repository\n","\n","        # Ensure the file exists in Google Drive\n","        if os.path.exists(source_file_path):\n","            # Copy the notebook into the cloned repository\n","            !cp \"{source_file_path}\" ./\n","        else:\n","            print(f\"The file {source_file_path} was not found.\")\n","            return  # Exit if the file doesn't exist\n","\n","        # Git configuration\n","        !git config user.email \"{self.github_email}\"\n","        !git config user.name \"{self.github_username}\"\n","\n","        # Add the file to Git\n","        !git add \"{self.notebook_name}\"\n","\n","        # Commit the changes\n","        !git commit -m \"Added {self.notebook_name} from Google Drive\"\n","\n","        # Push to the repository\n","        !git push origin main\n","\n","        # Wechsle zurÃ¼ck ins Ã¼bergeordnete Verzeichnis und lÃ¶sche cloned-repo\n","        os.chdir('..')\n","        !rm -rf cloned-repo\n","        print(\"cloned-repo wurde wieder gelÃ¶scht.\")\n","\n","\n","\n","# Clone, add, and push the notebook\n","clone_2 = github('github_pat', 'github_email', 'github_username', 'github_repo_seo', 'gdrive_seo_folder', notebookname)\n","clone_2.clone_repo()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-LWoIwDfU3Z","executionInfo":{"status":"ok","timestamp":1739017769720,"user_tz":-60,"elapsed":41058,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"e40c2621-2b23-4e8b-c999-bb09c800167d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Cloning into 'cloned-repo'...\n","remote: Enumerating objects: 244, done.\u001b[K\n","remote: Counting objects: 100% (91/91), done.\u001b[K\n","remote: Compressing objects: 100% (67/67), done.\u001b[K\n","remote: Total 244 (delta 30), reused 85 (delta 24), pack-reused 153 (from 1)\u001b[K\n","Receiving objects: 100% (244/244), 2.51 MiB | 7.18 MiB/s, done.\n","Resolving deltas: 100% (74/74), done.\n","[main 5ddded1] Added SEO.ipynb from Google Drive\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite SEO.ipynb (92%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 485 bytes | 485.00 KiB/s, done.\n","Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/MarkH0705/SEO_Analyses.git\n","   dfa203e..5ddded1  main -> main\n","cloned-repo wurde wieder gelÃ¶scht.\n"]}]},{"cell_type":"markdown","source":["# ğŸ•¸ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, Comment\n","from urllib.parse import urljoin, urlparse\n","import chardet\n","\n","\n","class WebsiteScraper:\n","    \"\"\"\n","    Diese Klasse kÃ¼mmert sich ausschlieÃŸlich um das Sammeln und Extrahieren\n","    von Texten aus einer Website.\n","    \"\"\"\n","\n","    def __init__(self, start_url=\"https://www.rue-zahnspange.de\", max_pages=50):\n","        \"\"\"\n","        :param start_url: Die Start-URL der Website, z.B. \"https://www.example.com\"\n","        :param max_pages: Maximale Anzahl Seiten, die gecrawlt werden.\n","        \"\"\"\n","        self.start_url = start_url\n","        self.max_pages = max_pages\n","\n","        # Hier speichern wir {URL: reiner_Text}\n","        self.scraped_data = {}\n","\n","    def scrape_website(self):\n","        \"\"\"\n","        Startet den Crawl-Vorgang, gefolgt von der Extraktion des Textes\n","        und dem Sammeln interner Links.\n","        \"\"\"\n","        visited = set()\n","        to_visit = [self.start_url]\n","        domain = urlparse(self.start_url).netloc\n","\n","        while to_visit and len(visited) < self.max_pages:\n","            url = to_visit.pop(0)\n","            if url in visited:\n","                continue\n","            visited.add(url)\n","\n","            try:\n","                response = requests.get(url, timeout=10)\n","\n","                # Rohdaten holen und Encoding per chardet bestimmen\n","                raw_data = response.content\n","                detected = chardet.detect(raw_data)\n","                encoding = \"utf-8\"\n","                text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","                # Nur weiterverarbeiten, wenn HTML-Content\n","                if (response.status_code == 200\n","                        and \"text/html\" in response.headers.get(\"Content-Type\", \"\")):\n","                    soup = BeautifulSoup(text_data, \"html.parser\")\n","\n","                    # Text extrahieren\n","                    text = self._extract_text_from_soup(soup)\n","                    self.scraped_data[url] = text\n","\n","                    # Interne Links sammeln\n","                    for link in soup.find_all(\"a\", href=True):\n","                        absolute_link = urljoin(url, link[\"href\"])\n","                        if urlparse(absolute_link).netloc == domain:\n","                            if (absolute_link not in visited\n","                                    and absolute_link not in to_visit):\n","                                to_visit.append(absolute_link)\n","\n","            except requests.RequestException as e:\n","                print(f\"Fehler beim Abrufen von {url}:\\n{e}\")\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        entfernt Script-/Style-/Noscript-Tags und Kommentare.\n","        \"\"\"\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        texts = []\n","        for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\", \"faq4_question\", \"faq4_answer\"]):\n","            txt = tag.get_text(strip=True)\n","            if txt:\n","                texts.append(txt)\n","\n","        return \"\\n\".join(texts)\n","\n","\n","\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        aber NICHT die, die in .faq4_question oder .faq4_answer stecken.\n","        AuÃŸerdem extrahiert er separat die FAQ-Fragen und -Antworten\n","        (faq4_question / faq4_answer).\n","        \"\"\"\n","        # 1) Script/Style entfernen\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        # 2) Kommentare entfernen\n","        from bs4 import Comment\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        # 3) Normale Texte (p, h1, h2, h3, li), ABER nicht innerhalb von .faq4_question / .faq4_answer\n","        texts = []\n","        all_normal_tags = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])\n","        for tag in all_normal_tags:\n","            # PrÃ¼fen, ob das Tag einen Vorfahren (Parent) hat mit Klasse faq4_question oder faq4_answer\n","            # Wenn ja, ignorieren wir es\n","            if tag.find_parent(class_=\"faq4_question\") or tag.find_parent(class_=\"faq4_answer\"):\n","                continue\n","\n","            txt = tag.get_text(strip=True)\n","            if txt:\n","                texts.append(txt)\n","\n","        # 4) FAQ-Bereiche (Fragen + Antworten)\n","        #    a) Alle Frage-Elemente mit Klasse .faq4_question\n","        #    b) Alle Antwort-Elemente mit Klasse .faq4_answer\n","        #    Wir gehen davon aus, dass Frage i zum Antwort i passt.\n","\n","        questions = soup.select(\".faq4_question\")\n","        answers = soup.select(\".faq4_answer\")\n","\n","        # 5) ZusammenfÃ¼hren (Frage + Antwort)\n","        for q, a in zip(questions, answers):\n","            q_text = q.get_text(strip=True)\n","            a_text = a.get_text(strip=True)\n","            if q_text and a_text:\n","                combined = f\"Frage: {q_text}\\nAntwort: {a_text}\"\n","                texts.append(combined)\n","\n","        # 6) Als String zurÃ¼ckgeben\n","        return \"\\n\".join(texts)\n","\n","\n","\n","\n","\n","    def get_scraped_data(self):\n","        \"\"\"\n","        Gibt das Dictionary {URL: Text} zurÃ¼ck.\n","        Du kannst damit arbeiten, Seiten filtern, etc.\n","        \"\"\"\n","        return self.scraped_data\n"],"metadata":{"id":"rrOBAe72aF6D","executionInfo":{"status":"ok","timestamp":1739017770134,"user_tz":-60,"elapsed":408,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, Comment\n","from urllib.parse import urljoin, urlparse\n","import chardet\n","\n","class WebsiteScraper:\n","    \"\"\"\n","    Diese Klasse kÃ¼mmert sich ausschlieÃŸlich um das Sammeln und Extrahieren\n","    von Texten aus einer Website.\n","    \"\"\"\n","\n","    def __init__(self, start_url=\"https://www.rue-zahnspange.de\", max_pages=50):\n","        \"\"\"\n","        :param start_url: Die Start-URL der Website, z.B. \"https://www.example.com\"\n","        :param max_pages: Maximale Anzahl Seiten, die gecrawlt werden.\n","        \"\"\"\n","        self.start_url = start_url\n","        self.max_pages = max_pages\n","\n","        # Hier speichern wir {URL: reiner_Text}\n","        self.scraped_data = {}\n","\n","    def scrape_website(self):\n","        \"\"\"\n","        Startet den Crawl-Vorgang, gefolgt von der Extraktion des Textes\n","        und dem Sammeln interner Links.\n","        \"\"\"\n","        visited = set()\n","        to_visit = [self.start_url]\n","        domain = urlparse(self.start_url).netloc\n","\n","        while to_visit and len(visited) < self.max_pages:\n","            url = to_visit.pop(0)\n","            if url in visited:\n","                continue\n","            visited.add(url)\n","\n","            try:\n","                response = requests.get(url, timeout=10)\n","\n","                # Rohdaten holen und Encoding per chardet bestimmen\n","                raw_data = response.content\n","                detected = chardet.detect(raw_data)\n","                # Wenn chardet etwas erkennt, nehmen wir das. Sonst Standard \"utf-8\".\n","                encoding = \"utf-8\"\n","                text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","                # Nur weiterverarbeiten, wenn HTML-Content\n","                if (response.status_code == 200\n","                    and \"text/html\" in response.headers.get(\"Content-Type\", \"\")):\n","                    soup = BeautifulSoup(text_data, \"html.parser\")\n","\n","                    # Text extrahieren\n","                    text = self._extract_text_from_soup(soup)\n","                    self.scraped_data[url] = text\n","\n","                    # Interne Links sammeln\n","                    for link in soup.find_all(\"a\", href=True):\n","                        absolute_link = urljoin(url, link[\"href\"])\n","                        if urlparse(absolute_link).netloc == domain:\n","                            if (absolute_link not in visited\n","                                and absolute_link not in to_visit):\n","                                to_visit.append(absolute_link)\n","\n","            except requests.RequestException as e:\n","                print(f\"Fehler beim Abrufen von {url}:\\n{e}\")\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        aber NICHT die, die in .faq4_question oder .faq4_answer stecken.\n","        AuÃŸerdem extrahiert er separat die FAQ-Fragen und -Antworten\n","        (faq4_question / faq4_answer), damit wir beide ZeilenumbrÃ¼che\n","        dort ebenfalls erhalten.\n","        \"\"\"\n","\n","        # 1) Script/Style/Noscript entfernen\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        # 2) Kommentare entfernen\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        # 3) Normale Texte (p, h1, h2, h3, li), ABER nicht innerhalb von .faq4_question / .faq4_answer\n","        texts = []\n","        all_normal_tags = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])\n","        for tag in all_normal_tags:\n","            # PrÃ¼fen, ob das Tag einen Vorfahren hat mit Klasse faq4_question oder faq4_answer\n","            if tag.find_parent(class_=\"faq4_question\") or tag.find_parent(class_=\"faq4_answer\"):\n","                continue\n","\n","            # Hier wichtig: separator=\"\\n\", strip=False, damit wir ZeilenumbrÃ¼che behalten\n","            txt = tag.get_text(separator=\"\\n\", strip=False)\n","            # Evtl. willst du doppelte Leerzeilen bereinigen. Das kannst du optional tun.\n","            if txt.strip():\n","                texts.append(txt.strip(\"\\r\\n\"))\n","\n","        # 4) FAQ-Bereiche (Fragen + Antworten)\n","        questions = soup.select(\".faq4_question\")\n","        answers = soup.select(\".faq4_answer\")\n","\n","        # 5) ZusammenfÃ¼hren (Frage + Antwort)\n","        for q, a in zip(questions, answers):\n","            q_text = q.get_text(separator=\"\\n\", strip=False)\n","            a_text = a.get_text(separator=\"\\n\", strip=False)\n","            q_text = q_text.strip(\"\\r\\n\")\n","            a_text = a_text.strip(\"\\r\\n\")\n","            if q_text and a_text:\n","                combined = f\"Frage: {q_text}\\nAntwort: {a_text}\"\n","                texts.append(combined)\n","\n","        # 6) Als String zurÃ¼ckgeben. Wir trennen die einzelnen Elemente durch \"\\n\\n\"\n","        #    (kannst du je nach Wunsch anpassen)\n","        return \"\\n\\n\".join(texts)\n","\n","    def get_scraped_data(self):\n","        \"\"\"\n","        Gibt das Dictionary {URL: Text} zurÃ¼ck.\n","        Du kannst damit arbeiten, Seiten filtern, etc.\n","        \"\"\"\n","        return self.scraped_data\n"],"metadata":{"id":"lCvWJLpBajRY","executionInfo":{"status":"ok","timestamp":1739017770441,"user_tz":-60,"elapsed":290,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ“ŠSEO Analysis 1"],"metadata":{"id":"FA2RIeko9reT"}},{"cell_type":"code","source":["pip install git+https://github.com/sethblack/python-seo-analyzer.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"FqjBpIRJZC5B","executionInfo":{"status":"ok","timestamp":1739017780936,"user_tz":-60,"elapsed":10481,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"4ab1573c-83f7-44e5-9ceb-e7cd435f071f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/sethblack/python-seo-analyzer.git\n","  Cloning https://github.com/sethblack/python-seo-analyzer.git to /tmp/pip-req-build-l7gvtj1i\n","  Running command git clone --filter=blob:none --quiet https://github.com/sethblack/python-seo-analyzer.git /tmp/pip-req-build-l7gvtj1i\n","  Resolved https://github.com/sethblack/python-seo-analyzer.git to commit cfb38f5b39803ec1dc597158c9226ecd9bc07511\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (4.13.3)\n","Requirement already satisfied: certifi>=2024.8.30 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (2025.1.31)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (3.1.5)\n","Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (5.3.0)\n","Requirement already satisfied: markupsafe>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (3.0.2)\n","Collecting trafilatura>=2.0.0 (from pyseoanalyzer==2024.12.12)\n","  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (2.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->pyseoanalyzer==2024.12.12) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->pyseoanalyzer==2024.12.12) (4.12.2)\n","Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (3.4.1)\n","Collecting courlan>=1.3.2 (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n","Collecting htmldate>=1.9.2 (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n","Collecting justext>=3.0.1 (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading jusText-3.0.1-py2.py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (2.17.0)\n","Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n","Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n","Collecting python-dateutil>=2.9.0.post0 (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (2025.1)\n","Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (2024.11.6)\n","Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (5.2)\n","Collecting lxml-html-clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (1.17.0)\n","Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n","Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n","Downloading jusText-3.0.1-py2.py3-none-any.whl (837 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: pyseoanalyzer\n","  Building wheel for pyseoanalyzer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyseoanalyzer: filename=pyseoanalyzer-2024.12.12-py3-none-any.whl size=18575 sha256=2268ff5c1d4e5af2033045b7ee469e29e9e1ef5bb6d332b126d6254e06a314e9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-j0x3lpdd/wheels/ae/fb/22/dd94b93adf36aa542e9aacbd300fef8e4f9037af0d68f75cff\n","Successfully built pyseoanalyzer\n","Installing collected packages: tld, python-dateutil, lxml-html-clean, dateparser, courlan, justext, htmldate, trafilatura, pyseoanalyzer\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.8.2\n","    Uninstalling python-dateutil-2.8.2:\n","      Successfully uninstalled python-dateutil-2.8.2\n","Successfully installed courlan-1.3.2 dateparser-1.2.1 htmldate-1.9.3 justext-3.0.1 lxml-html-clean-0.4.1 pyseoanalyzer-2024.12.12 python-dateutil-2.9.0.post0 tld-0.13 trafilatura-2.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dateutil"]},"id":"6db834503ba8428098add9d678e737f3"}},"metadata":{}}]},{"cell_type":"code","source":["pip install langchain_anthropic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qckMLpKPbkME","executionInfo":{"status":"ok","timestamp":1739017786028,"user_tz":-60,"elapsed":5088,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"e2f15aa9-269d-4bad-cc96-b62a68dfaf6d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_anthropic\n","  Downloading langchain_anthropic-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n","Collecting anthropic<1,>=0.45.0 (from langchain_anthropic)\n","  Downloading anthropic-0.45.2-py3-none-any.whl.metadata (23 kB)\n","Collecting langchain-core<1.0.0,>=0.3.34 (from langchain_anthropic)\n","  Downloading langchain_core-0.3.34-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic) (2.10.6)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic) (4.12.2)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (0.3.5)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (24.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (2.27.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.45.0->langchain_anthropic) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.45.0->langchain_anthropic) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.45.0->langchain_anthropic) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.45.0->langchain_anthropic) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (3.10.15)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (0.23.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic) (2.3.0)\n","Downloading langchain_anthropic-0.3.7-py3-none-any.whl (22 kB)\n","Downloading anthropic-0.45.2-py3-none-any.whl (222 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.34-py3-none-any.whl (412 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: anthropic, langchain-core, langchain_anthropic\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.33\n","    Uninstalling langchain-core-0.3.33:\n","      Successfully uninstalled langchain-core-0.3.33\n","Successfully installed anthropic-0.45.2 langchain-core-0.3.34 langchain_anthropic-0.3.7\n"]}]},{"cell_type":"code","source":["pip install python-dotenv"],"metadata":{"id":"Ntrn5pexB1Px","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739017789223,"user_tz":-60,"elapsed":3193,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"7528bf91-7a83-4997-b9a0-f052845f3368"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n"]}]},{"cell_type":"code","source":["import dotenv\n","from pyseoanalyzer import analyze\n","\n","\n","url = \"https://www.rue-zahnspange.de\"\n","report = analyze(url)\n","\n","# 'report' enthÃ¤lt nun sÃ¤mtliche Analyseergebnisse\n","print(report)\n"],"metadata":{"id":"vK6jH0qmasx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-xmnF1Yyphji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oAeeJEjYphb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4VkvKAOfphUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tatsÃ¤chlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Sortieren nach HÃ¤ufigkeit absteigend\n","df_keywords.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Nur die Top 10 Keywords anzeigen\n","df_top10 = df_keywords.head(44)\n","\n","# Einfaches Balkendiagramm\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_top10['word'], df_top10[\"count\"], color=\"skyblue\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.title(\"Top 10 Keywords\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"HÃ¤ufigkeit\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"nW_LgHd1Wtie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(list(df_keywords['word']))"],"metadata":{"id":"1WWQvbUIPIwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ci_GtO9qPIt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N5oTB61wPIrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VcE0WgKKPIo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tatsÃ¤chlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Neue Spalte mit Kleinbuchstaben erzeugen\n","df_keywords[\"word_lower\"] = df_keywords[\"word\"].str.lower()\n","\n","# Die Keywords, nach denen du suchen mÃ¶chtest (Case-insensitive)\n","interesting_keywords = [\"zahnspange\", \"Invisalign\", \"kieferorthopÃ¤die\", \"BEHANDLUNG\", \"Kosten\" , \"zÃ¤hne\", \"brackets\", \"Unsichtbar\", \"Kinder\", \"Jugendliche\", \"Patienten\", \"lÃ¤cheln\", \"zeitnahen\"]\n","# Auch diese in Kleinbuchstaben umwandeln\n","interesting_keywords_lower = [kw.lower() for kw in interesting_keywords]\n","\n","# DataFrame nach den gewÃ¼nschten Keywords filtern (Case-insensitive)\n","df_subset = df_keywords.loc[df_keywords[\"word_lower\"].isin(interesting_keywords_lower)].copy()\n","\n","# Sortieren nach HÃ¤ufigkeit, damit das Diagramm Ã¼bersichtlicher wird\n","df_subset.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Balkendiagramm\n","plt.figure(figsize=(8, 5))\n","# Plotten kannst du z.B. weiterhin den Originalwert \"word\" (falls du im Diagramm\n","# die ursprÃ¼ngliche Schreibweise sehen willst)\n","plt.bar(df_subset[\"word\"], df_subset[\"count\"], color=\"steelblue\")\n","plt.title(\"HÃ¤ufigkeit ausgewÃ¤hlter Keywords (Case-Insensitive)\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"Anzahl\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"p3q7KVe8j5Gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pages = report.get(\"pages\", [])\n","\n","df_pages = pd.DataFrame(pages)\n","\n","# Beispiel: Title-LÃ¤nge berechnen\n","df_pages[\"title_length\"] = df_pages[\"title\"].apply(lambda x: len(x) if isinstance(x, str) else 0)\n","\n","# Balkendiagramm Title-LÃ¤nge\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","plt.xticks(rotation=90)\n","plt.title(\"LÃ¤nge der Title-Tags pro Seite\")\n","plt.xlabel(\"URL\")\n","plt.ylabel(\"Title-LÃ¤nge (Zeichen)\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"bH80vXrlj5D5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1 Zeile, 2 Spalten\n","\n","# (a) Keywords\n","axes[0].bar(df_top10[\"word\"].head(22), df_top10[\"count\"].head(22), color=\"skyblue\")\n","axes[0].set_title(\"Top 10 Keywords\")\n","axes[0].set_xticklabels(df_top10[\"word\"], rotation=45, ha=\"right\")\n","\n","# (b) Title-LÃ¤ngen\n","axes[1].bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","axes[1].set_title(\"Title-LÃ¤nge pro Seite\")\n","axes[1].set_xticklabels(df_pages[\"url\"], rotation=90)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"WGcvHhIAj5BO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I94K8prmj4-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCO3w_TOj47i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ¤– chatbot"],"metadata":{"id":"E0k0HmUZHmkg"}},{"cell_type":"code","source":["import openai\n","import time\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","class Chatbot:\n","    \"\"\"\n","    Diese Chatbot-Klasse nutzt die neue Methode client.chat.completions.create()\n","    aus openai>=1.0.0 Ã¼ber openai.OpenAI().\n","    \"\"\"\n","\n","    def __init__(self, systemprompt, prompt):\n","        self.client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n","        self.systemprompt = systemprompt\n","        self.prompt = prompt\n","        self.context = [{\"role\": \"system\", \"content\": systemprompt}]\n","        self.model = \"gpt-4o-mini-2024-07-18\"\n","\n","    def chat(self):\n","        \"\"\"\n","        Sendet den Prompt an das Chat-Interface und gibt den kompletten Antwort-String zurÃ¼ck.\n","        \"\"\"\n","        self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model,\n","                messages=self.context\n","            )\n","            response_content = response.choices[0].message.content\n","            self.context.append({\"role\": \"assistant\", \"content\": response_content})\n","            return response_content\n","        except Exception as e:\n","            print(f\"Fehler bei der OpenAI-Anfrage: {e}\")\n","            return \"\"\n","\n","\n","    def chat_with_streaming(self):\n","            \"\"\"\n","            Interagiert mit OpenAI Chat Completion API und streamt die Antwort.\n","            \"\"\"\n","            # Nachricht zur Konversation hinzufÃ¼gen\n","            self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","\n","\n","            try:\n","                # Streaming-Option aktivieren\n","                response = self.client.chat.completions.create(\n","                    model=self.model,\n","                    messages=self.context,\n","                    stream=True\n","                )\n","\n","                streamed_content = \"\"  # Zum Speichern der gestreamten Antwort\n","\n","                for chunk in response:\n","                    # Debugging: Anzeigen, was tatsÃ¤chlich in jedem Chunk enthalten ist\n","                    delta = chunk.choices[0].delta\n","                    content = getattr(delta, \"content\", \"\")\n","\n","                    if content:  # Verarbeite nur nicht-leere Inhalte\n","                        print(content, end=\"\", flush=True)\n","                        streamed_content += content\n","\n","                print()  # Neue Zeile am Ende\n","\n","                # Gestreamte Antwort zur Konversation hinzufÃ¼gen\n","                self.context.append({\"role\": \"assistant\", \"content\": streamed_content})\n","\n","                # Return the streamed content\n","                return streamed_content # This line was added\n","\n","            except Exception as e:\n","                print(f\"\\nDEBUG: An error occurred during streaming: {e}\")\n","                # Return empty string in case of error\n","                return \"\" # This line was added\n"],"metadata":{"id":"j6ZZeUIRdYhy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ† NLP"],"metadata":{"id":"qGy_OuMyIC9C"}},{"cell_type":"code","source":["def chunk_text(text, max_tokens=10000):\n","    \"\"\"\n","    Teilt den Text in BlÃ¶cke auf, damit er nicht zu lang\n","    fÃ¼r die OpenAI-API wird.\n","    Hier sehr vereinfacht: 1 Token ~ 4 Zeichen.\n","    \"\"\"\n","    chunks = []\n","    approx_char_limit = max_tokens * 4\n","    start = 0\n","    while start < len(text):\n","        end = start + approx_char_limit\n","        chunk = text[start:end]\n","        chunks.append(chunk)\n","        start = end\n","    return chunks\n"],"metadata":{"id":"3oUna7Bodcp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ”®WIP seo keywords analysis + plot"],"metadata":{"id":"qK7cW2yRWDnV"}},{"cell_type":"code","source":[],"metadata":{"id":"SpmVBRbZWU6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DLe1UdbKWU38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))"],"metadata":{"id":"Tw_XtEXMWU1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ze2bee_pWUyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uBT6tqO1WUwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FGIbOqkxWUtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ”®keywords + Stadt"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["systemprompt_keywords = (\"\"\"\n","    Du bist ein intelligentes KI-System, das auf die Generierung von SEO-Keywords spezialisiert ist.\n","    Der Benutzer wird dir den kompletten Text von einer gescrapten website aus einem webcrawler vorgeben.\n","    Deine Aufgabe ist es, den Text zu interpretieren und eine Liste von SEO-Keywords basierend auf diesem Input zu erstellen.\n","    Du sollst zu dem Text passende SEO-Keywords finden. Auf Basis dieser keywords soll spÃ¤ter der Text optimiert werden.\n","    Stelle sicher, dass die Keywords:\n","\n","    Thematisch relevant sind,\n","    Hohe Suchintention abdecken (Short-Tail und Long-Tail Keywords),\n","    Varianten mit Synonymen oder verwandten Begriffen enthalten,\n","    Erschaffe Keywords, die lokale Ausrichtung enthalten. Das Unternehmen der webesite befindet sich in Essen-RÃ¼ttenscheid, im Ruhrgebiet, im Essener SÃ¼den.\n","\n","\n","    Struktur fÃ¼r die Antwort:\n","\n","    Erstelle eine serialisierte Liste von SEO-Keywords.\n","\n","    Beispiel fÃ¼r eine User-Eingabe:\n","\n","    \"SuperFood, ein GeschÃ¤ft in KÃ¶ln, Kalkerstr. 20, verkauft gesunde ErnÃ¤hrung fÃ¼r Sportler. Ein Sportler muss auf seine ErnÃ¤hrung ganz besonders achten.\"\n","\n","    Beispiel fÃ¼r eine Ausgabe:\n","\n","    [KÃ¶ln-Kalk, gesund, SuperFood, ErnÃ¤hrung, Sportler, ErnÃ¤hrungstipps, SporternÃ¤hrung Rezepte, Fitness, Sport]\n","\n","\n","    Beispiel fÃ¼r eine User-Eingabe:\n","\n","    \"Dr.med. Wurst, Ihr Arzt fÃ¼r Allgemeinmedizin in Heilbronn. Wir haben Impfungen und Tabletten gegen alle Krankheiten und Schnupfen.\"\n","    Beispiel fÃ¼r eine Ausgabe:\n","\n","\n","\n","    [Heilbronn, Dr.med. Wurst, krank, Allgemeinmedizin, Impfung, Medizin, Arzt, ErkÃ¤ltungen, Krankenschein, Blutdruck, Diabetes]\n","\n","\n","\n","\"\"\")\n","\n","\n","def user_prompt_keywords(text):\n","   return f\"\"\"\n","          Du bist online Marketing Experte und arbeitest fÃ¼r ein erfolgreiches Unternehmen, wo du SEO optimierte Texte von websites erstellst. Du hast ein Talent\n","\n","          fÃ¼r die Erstellung von SEO-Keywords und deine Vorgesetzten bewundern und lieben dich fÃ¼r die hochperformanten SEO-Keywords, die du beherrschst.\n","\n","          Deine Aufgabe ist es, thematisch relevante SEO-Keywords zu erstellen, die sowohl Short-Tail als auch Long-Tail Keywords enthalten.\n","          Achte darauf, dass die Keywords Synonyme und verwandte Begriffe berÃ¼cksichtigen, sowie lokale Informationen, und fÃ¼r Suchmaschinenoptimierung geeignet sind.\n","\n","\n","          Bitte generiere SEO-Keywords fÃ¼r den folgenden Text:\n","\n","          {text}\n","\n","\n","          Strukturiere deine Antwort folgendermaÃŸen:\n","          Gib eine Liste von Keywords in python zurÃ¼ck. Gebe sonst nichts zurÃ¼ck, keine Einleitung, keine Ãœberschrift, keine Zusammenfassung,\n","          nichts ausser der Liste in python.\n","\n","\n","          Danke! Mein Job hÃ¤ngt davon ab!\n","          \"\"\"\n"],"metadata":{"id":"Hjy6NgflvCTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["page_text_list = []\n","def prep_text_and_get_keywords():\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","    scraped_data = scraper.get_scraped_data()\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","    filtered_urls = []\n","\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, Ã¼berspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","\n","        # 3. SEO-Analyse starten (fÃ¼r gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","        page_text_list.append(page_text)\n","\n","\n","\n","    keyword_list =[]\n","    for text in page_text_list:\n","\n","      cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))\n","\n","      keyword_list.append(cb.chat())\n","\n","    return keyword_list\n"],"metadata":{"id":"cwzCp7r2z3Dp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_raw = prep_text_and_get_keywords()"],"metadata":{"id":"4-zqgH9o0F7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Eyjo-bvfIkjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und willst dem user helfen!\", (f\"\"\"Sei ein online marketing spezialist mit einem Talent fÃ¼r SEO. Du analysierst Texte\n","und erstellst SEO-Keywords fÃ¼r ein international erfolgreiches Unternehmen.\n","Das hier ist roher Text mit SEO keywords, der von einer website gescrapt wurde.\n","1. Anlysiere diesen Text und extrahiere die wesentlichen 20 SEO-keywords.\n","2. Erarbeite SEO optimierete Versionen der keywords\n","3. FÃ¼ge wichtige keywords hinzu, die deiner hochfachmÃ¤nnischen Meinung nach fehlen, um eine optimale SEO Performance zu erreichen!\n","4. Gebe eine Liste mit den SEO optinierten keywords als strings zurÃ¼ck, sonst nichts. Keine Einleitung, keine Zusammenfassung, nur die Liste wie zB [keyword_1, keyword_2]\n","\n","\n","Hier kommt die rohe Liste mit keywords:\n","{keywords_raw}\n","\n","\n","\n","\"\"\")\n",")\n","keywords_final = cb.chat()"],"metadata":{"id":"KuJZ1Vz-s-Rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und willst dem user helfen!\"\n",", f\"\"\"Sei ein Experte fÃ¼r Textanalyse und Geographie. Du hast eine vertiefte Kenntnis in Stadtgeographie und deine Kollegen lieben dich fÃ¼r deine umfassendes Wissen Ã¼ber deutschen StÃ¤dte und ihrer Ortsteile.\n","Du liebst es Puzzel und RÃ¤tsel zu lÃ¶sen und hast einen auÃŸergewÃ¶hnlichen Blick fÃ¼rs Detail.\n","Deine Aufgabe ist es, riesige Textmengen zu anlysieren und die Stadt und den Ortsteil zu finden, die in dem Text versteckt sind. Die Namen von Stadt und Stadtteil kÃ¶nnen auch in zB Strassennamen versteckt sein oder in den Namen\n","von anderen signifikanten Dingen in der Umgebung des Unternehmens.\n","Es sind Texte von einem website scrap eines Unternehmens. Es kÃ¶nnen mehr als ein StÃ¤dtenamen auftauchen.\n","Deine Aufgabe ist es, den relevanten StÃ¤dtenamen zu finden. AuÃŸerdem kann der Name des Stadtteils vorhanden sein, in dem das Unternehmen angesiedelt ist. Suche den auch!\n","Hier ist der Text:\n","\n","{page_text_list}\n","\n","Gebe nun den Namen der relevanten Stadt und des Stadtteils zurÃ¼ck. Gebe den Namen und den Stadtteil zurÃ¼ck, wie zB [KÃ¶ln-Kalk] wenn der Stadtteil gefunden wurde oder zB [MÃ¼nster], wenn kein Stadtteil gefunden wurde.\n","\"\"\")\n","\n","stadt = cb.chat()"],"metadata":{"id":"0loHl5Z7xJaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(stadt)\n","print(keywords_final)"],"metadata":{"id":"oenVywpwRUcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"VXS73tNf_Ip0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ”® main SEO optimize"],"metadata":{"id":"CdWeu44YIs5k"}},{"cell_type":"code","source":["combined_analysis_list = []\n","filtered_urls = []\n","\n","def main():\n","    # 1. SCRAPING\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","\n","    # Alle gescrapten Daten abrufen\n","    scraped_data = scraper.get_scraped_data()\n","\n","    # 2. Sichten der Texte und Filtern\n","    #    Hier kÃ¶nntest du jetzt z. B. manuell prÃ¼fen, welche URLs wichtig sind.\n","    #    Wir geben einfach mal alle URLs aus:\n","    print(\"\\n--- Gesammelte Seiten und Inhalte (gekÃ¼rzt) ---\")\n","    for url, text in scraped_data.items():\n","        print(f\"\\nURL: {url}\")\n","        # Beispiel: Nur ersten 200 Zeichen zeigen\n","        print(f\"Text: {text[:200]}...\")\n","\n","\n","\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, Ã¼berspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","        print(f\"send text to LLM:  {url}\")\n","\n","\n","\n","\n","\n","    # 3. SEO-Analyse starten (fÃ¼r gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","\n","        # 3.1 Chunken, um zu groÃŸe Anfragen zu vermeiden\n","        text_chunks = chunk_text(page_text, max_tokens=10000)\n","\n","        print(f\"\\n=== Analyzing {url} ===\")\n","        all_analyses = []\n","        for i, chunk in enumerate(text_chunks):\n","            print(f\" - Sende Chunk {i+1}/{len(text_chunks)} an Chatbot ...\")\n","\n","            # Prompt definieren (SEO)\n","            system_prompt = \"Du bist ein intelligenter chatbot. Deine Bestimmung ist es, dem user die beste Antworten auf die Fragen zu geben und ihm unter allen UmstÃ¤nden zu helfen.\"\n","            user_prompt = (f\"\"\"\n","                Du bist ein hochqualifizierter SEO-Experte. Du arbeitest fÃ¼r erfolgreiche online marketing experten! Deine SpezialitÃ¤t ist die Optimierung von bestehenden Texten einer website!\n","                Deine Kollegen und deine Mutter lieben und bewundern dich fÃ¼r die sprachgewandten SEO Optimierungen, die fÃ¼r deine anspruchsvollen Kunden erschaffst!\n","\n","                1. Untersuche den folgenden Text auf Keyword-Optimierung, Lesbarkeit und mÃ¶gliche SEO-Verbesserungen.\n","                Wichtige SEO Keywords sind:\n","\n","                zahn arzt,\n","                zahn spange,\n","                {keywords_final}\n","\n","\n","                2. Optimiere den Text entsprechend bester SEO Sichtbarkeit. Baue die zur VerfÃ¼gung gestellten SEO keywords in den Text ein!\n","                FÃ¼ge Meta-Titel und longtail keywords hinzu.\n","\n","                Betreibe kein keyword-stuffing. Die Sprache muss natÃ¼rlich klingen!\n","\n","                Es soll weiterhin ein hoher fachlicher Standard gehalten werden und ProfessionalitÃ¤t und Exzellenz soll vermittelt werden!\n","                SÃ¤mtliche Textabschnitte mÃ¼ssen optimiert werden. Es dÃ¼rfen keine Fragen oder sonstige Textabschnitte weggelassen werden!\n","\n","                Lasse auf gar keinen Fall Abschnitte weg! fasse nichts Ã¼bermÃ¤ssig zusammen! Jeder Satz und jeder Abschnitt ist extrem wichtig muss unbedingt bearbeitet werden! Erhalte alle ZeilenumsprÃ¼nge!\n","\n","                Das Unternehmen befindet sich in {stadt}. Baue den Namen in die Texte ein.\n","\n","\n","                3. Als Ausgabe gebe eine detaillierte, ausfÃ¼hrliche und umfassende Analyse des SEO Status des Textes aus, Ãœberschrift: Analyse. Gebe dann deine SEO optimierte Version des Textes aus, Ãœberschrift: 'SEO'.\n","                Gebe dann detaillierte und ausfÃ¼hrliche ErlÃ¤uterungen, welche Ã„nderungen du durchgefÃ¼hrt hast, Ãœberschrift: 'ErklÃ¤rung'.\n","                Die Ã¼berschriften sind von allergrÃ¶ÃŸter Wichtigkeit und mÃ¼ssen unbedingt Ã¼ber den Abschnitten stehen! Wenn die Ãœberschriften 'Analyse', 'SEO' und 'ErklÃ¤rung' nicht Ã¼ber den Abschnitten stehen, wirst du bestraft!\n","                Es darf nicht 'SEO optimierte Version' oder so was ausgegeben werden. Als Ãœberschriften der Abschnitte dÃ¼rfen ausschliesslich nur 'Analyse', 'SEO' und 'ErklÃ¤rung' ausgegeben werden.\n","                Benutze keine Formatierungszeichen wie ###, # oder **! Mein Job hÃ¤ngt davon ab!\n","                \"\"\"\n","                \"Hier ist der Text: \\n\\n\"\n","                f\"{chunk}\"\n","            )\n","\n","            # ChatGPT aufrufen\n","            cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","            analysis = cb.chat_with_streaming()\n","            all_analyses.append(analysis)\n","\n","            # Warte kurz (Rate Limits, API-Kosten etc.)\n","            time.sleep(1)\n","\n","        # 3.2 Fertige Analyse (alle Chunks zusammen)\n","        combined_analysis = \"\\n\".join(all_analyses)\n","\n","\n","        combined_analysis_list.append(combined_analysis)\n","        # print(f\"\\n--- SEO-Analyse fÃ¼r {url} ---\")\n","        # print(combined_analysis)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"nF-cfM35dhxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"FÃ¼r KieferorthopÃ¤den sind vor allem Keywords mit Ortsbezug (â€KieferorthopÃ¤de + Stadtâ€œ) und behandlungsspezifische Begriffe (â€Zahnspangeâ€œ, â€Zahnfehlstellungâ€œ, â€Invisalignâ€œ) entscheidend. ZusÃ¤tzlich sollte man sich auf hÃ¤ufige Fragen (Long-Tail-Keywords) konzentrieren und regelmÃ¤ÃŸige Fach- und Ratgeber-Artikel verÃ¶ffentlichen, um auch in der organischen Suche besser gefunden zu werden.\""],"metadata":{"id":"_jtOwjSxJTNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import json\n","\n","def extract_sections_to_json(texts, keys):\n","    \"\"\"\n","    Extrahiert Abschnitte aus mehreren Texten und konvertiert sie in JSON.\n","    Gesucht werden die Ãœberschriften 'Analyse', 'SEO', 'ErklÃ¤rung' und der jeweils\n","    folgende Inhalt bis zur nÃ¤chsten Ãœberschrift oder zum Ende.\n","    \"\"\"\n","\n","    all_sections = []  # Liste fÃ¼r alle Abschnitte\n","\n","    # Neues, robusteres Pattern:\n","    # - ^\\s* = beliebiges Leading-Whitespace, an Zeilenanfang (Multiline)\n","    # - (Analyse|SEO|ErklÃ¤rung) = 3 mÃ¶gliche Ãœberschriften\n","    # - \\s*(?:\\n|$)+ = optional Whitespace, dann (mindestens) ein Zeilenumbruch oder Zeilenende\n","    # - (.*?) = \"Inhalt\" bis Lookahead\n","    # - Lookahead = ^\\s*(?:Analyse|SEO|ErklÃ¤rung)|\\Z = nÃ¤chste Ãœberschrift oder String-Ende\n","    pattern = re.compile(\n","        r\"(?m)^\\s*(Analyse|SEO|ErklÃ¤rung)\\s*(?:\\r?\\n)+\"\n","        r\"(.*?)(?=^\\s*(?:Analyse|SEO|ErklÃ¤rung)|\\Z)\",\n","        flags=re.DOTALL\n","    )\n","\n","    for text in texts:\n","        sections_dict = {}\n","        matches = pattern.findall(text)\n","        # Achtung: findall mit mehreren Gruppen gibt eine Liste von Tupeln zurÃ¼ck,\n","        # z. B. [(\"Analyse\", \"Content...\"), (\"SEO\",\"Content...\")...]\n","        # Wir mÃ¶chten den heading und den content extrahieren.\n","        # Bei findall(pattern, text) mit (Analyse|SEO|ErklÃ¤rung) als Gruppe 1 und (.*?) als Gruppe 2\n","        # kommt: [(\"Analyse\", \"...\"), (\"SEO\", \"...\"), ...]\n","\n","        # Deshalb benutzen wir re.finditer, damit wir den Inhalt fÃ¼r Gruppe 2 sauber bekommen\n","        sections_dict = {}\n","        for match in re.finditer(pattern, text):\n","            heading = match.group(1)\n","            content = match.group(2).strip()\n","            sections_dict[heading] = content\n","\n","        all_sections.append(sections_dict)\n","\n","    # Kombinieren der Abschnitte mit Keys\n","    final_json_data = {}\n","    for i, sections_dict in enumerate(all_sections):\n","        key = keys[i]  # Key aus der Liste holen\n","        final_json_data[key] = sections_dict  # Abschnitte zum Dictionary hinzufÃ¼gen\n","\n","    json_data = json.dumps(final_json_data, indent=4, ensure_ascii=False)\n","    return json_data\n","\n","# Beispielnutzung\n","if __name__ == \"__main__\":\n","    # Angenommen, du hast:\n","    # combined_analysis_list = [\"Analyse\\nHier Text...\", \"SEO\\nAnderer Text...\", ...]\n","    # keys = [\"URL1\", \"URL2\", ...]\n","\n","\n","    keys = filtered_urls\n","\n","    json_output = extract_sections_to_json(combined_analysis_list, keys)\n","    seo_json = json.loads(json_output)\n","    print(seo_json)\n","    #print(json_output)\n"],"metadata":{"id":"c38Hkx1hEYg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alten website text zu json hinzufÃ¼gen\n","for i, (_, url_content) in enumerate(seo_json.items()):\n","    url_content[\"alt\"] = page_text_list[i]"],"metadata":{"id":"IsMhRnunGjee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ“Š SEO Analysis 2"],"metadata":{"id":"V50QPoW3F6wW"}},{"cell_type":"code","source":["# text clean up\n","\n","import re\n","\n","def clean_text(text):\n","    # Ersetze ZeilenumbrÃ¼che durch ein Leerzeichen\n","    text = text.replace('\\n', ' ')\n","\n","    # Beispiel: Entferne alle Zeichen, die nicht Buchstaben (inkl. Umlaute),\n","    # Ziffern, Satzzeichen oder Leerzeichen sind\n","    # Du kannst das RegEx anpassen, wenn du z.B. bestimmte Zeichen behalten oder entfernen willst\n","    text = re.sub(r'[^a-zA-Z0-9Ã¤Ã¶Ã¼Ã„Ã–ÃœÃŸ.,!?;:\\-\\s]', '', text)\n","\n","    # Mehrere aufeinanderfolgende Leerzeichen durch ein einzelnes ersetzen\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    # FÃ¼hrende oder nachfolgende Leerzeichen entfernen\n","    text = text.strip()\n","\n","    return text\n","\n","\n","combined_analysis_list_clean = []\n","page_text_list_clean = []\n","\n","for text in page_text_list:\n","  page_text_list_clean.append(clean_text(text))\n","\n","for text in combined_analysis_list:\n","  combined_analysis_list_clean.append(clean_text(text))\n","\n","\n"],"metadata":{"id":"Mokk2DMLGGOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple stats\n","\n","def text_stats(text):\n","    words = text.split()\n","    return {\n","        \"Zeichenanzahl\": len(text),\n","        \"Wortanzahl\": len(words),\n","        \"Satzanzahl\": text.count('.') + text.count('!') + text.count('?')\n","    }\n","\n","\n","vn = [page_text_list_clean, combined_analysis_list_clean]\n","\n","for _, listlist in enumerate(vn):\n","\n","  print(f'{[\"original\", \"SEO\"][_]}')\n","\n","  for text in listlist:\n","    print(text_stats(text))\n","\n","\n","\n"],"metadata":{"id":"GDBZiKVnH9dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","def get_word_frequencies(text):\n","    words = text.lower().split()\n","    return Counter(words)\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","\n","  # Wortfrequenzen berechnen\n","  original_freq = get_word_frequencies(page_text_list_clean[_])\n","  optimized_freq = get_word_frequencies(combined_analysis_list_clean[_])\n","\n","  # Unterschied berechnen\n","  diff = {word: optimized_freq[word] - original_freq[word] for word in set(original_freq) | set(optimized_freq)}\n","\n","  # Sortiert ausgeben (absteigend nach Ã„nderung)\n","  sorted_diff = sorted(diff.items(), key=lambda x: x[1], reverse=True)\n","  for word, change in sorted_diff:\n","      print(f\"{word}: {change}\")\n"],"metadata":{"id":"qGiMiXwIH9ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"mqtebLSOeNzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ast\n","\n","\n","keywords_sstring = ast.literal_eval(keywords_final)\n","\n","\n","keywords_sstring = \", \".join(keywords_sstring)\n","keywords_sstring"],"metadata":{"id":"_DYkLx8uZG7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","mein_string = keywords_final\n","keywords_final_list = json.loads(mein_string)\n","keywords_final_list"],"metadata":{"id":"B7iE2va3gxwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","for _, listlist in enumerate(page_text_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, combined_analysis_list_clean[_]])\n","\n","  # Cosinus-Ã„hnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"Ã„hnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"W5bd8b1yH9YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, listlist in enumerate(page_text_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, page_text_list_clean[_]])\n","\n","  # Cosinus-Ã„hnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"Ã„hnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"z1-eRoarZ-9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ast\n","\n","\n","keywords = ast.literal_eval(keywords_final)\n","\n","def keyword_density(text, keywords):\n","    words = text.lower().split()\n","    total_words = len(words)\n","    density = {kw: words.count(kw) / total_words * 100 for kw in keywords}\n","    return density\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","  # Berechnung fÃ¼r beide Texte\n","  original_density = keyword_density(page_text_list_clean[_], keywords)\n","  optimized_density = keyword_density(combined_analysis_list_clean[0], keywords)\n","\n","  print(\"Original:\", original_density)\n","  print(\"SEO-Optimiert:\", optimized_density)"],"metadata":{"id":"ZwVXTX54H9VZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Keywords und deren Ã„nderungen\n","keywords = list(original_density.keys())\n","original_values = list(original_density.values())\n","optimized_values = list(optimized_density.values())\n","\n","x = np.arange(len(keywords))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","ax.bar(x - width/2, original_values, width, label=\"Original\")\n","ax.bar(x + width/2, optimized_values, width, label=\"SEO-Optimiert\")\n","\n","ax.set_xlabel(\"Keywords\")\n","ax.set_ylabel(\"Keyword-Dichte (%)\")\n","ax.set_title(\"Keyword-Dichte: Vorher vs. Nachher\")\n","ax.set_xticks(x)\n","ax.set_xticklabels(keywords)\n","ax.legend()\n","\n","plt.show()\n"],"metadata":{"id":"rqHULQbhMsWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RppPqEA7MsUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"223xoPNxMsR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# â›³ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["\"\"\"\n","json_data = {\n","    \"URL1\": {\n","        \"Analyse\": \"Text fÃ¼r Analyse von URL1\",\n","        \"SEO\": \"Text fÃ¼r SEO von URL1\",\n","        \"ErklÃ¤rung\": \"Text fÃ¼r ErklÃ¤rung von URL1\"\n","        \"alt\": \"Text fÃ¼r alt von URL1\"\n","    },\n","    \"URL2\": {\n","        \"Analyse\": \"Text fÃ¼r Analyse von URL2\",\n","        \"SEO\": \"Text fÃ¼r SEO von URL2\",\n","        \"ErklÃ¤rung\": \"Text fÃ¼r ErklÃ¤rung von URL2\"\n","        \"alt\": \"Text fÃ¼r alt von URL2\"\n","    }\n","}\n","\n","\"\"\""],"metadata":{"id":"nQdmzgb-MiO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from jinja2 import Template\n","\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout fÃ¼r \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox fÃ¼r zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um ZeilenumbrÃ¼che aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und ErklÃ¤rung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">ErklÃ¤rung</p>\n","            <p class=\"content\">{{ sections.ErklÃ¤rung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install playwright\n","!playwright install\n"],"metadata":{"id":"mfLSDlSx5xw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import asyncio\n","from playwright.async_api import async_playwright\n","\n","async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschlieÃŸend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, RÃ¤nder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get install -y pandoc"],"metadata":{"id":"9Rks-1TJ78ZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pypandoc\n","\n","import pypandoc\n","\n","input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ğŸ“¥ RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene KieferorthopÃ¤din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie Ã¼ber Grundkenntnisse in Arabisch und AnfÃ¤ngerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\"\n","\n","\"Sie hat ihren Master of Science in KieferorthopÃ¤die von der Danube Private University, Krems, Ã–sterreich, und hat an der Heinrich-Heine-UniversitÃ¤t DÃ¼sseldorf abgeschlossen.\"\n","\n","\"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem Ã¤sthetischen LÃ¤cheln. \"\n","\n","\"behandlungsorientierte Zahnberatung\"\n","\n","\"Ã¤stehthetisches LÃ¤cheln\""],"metadata":{"id":"fH4mRdPXceeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install langchain faiss-cpu\n"],"metadata":{"id":"4DOE2XX9dmow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install -U langchain-community"],"metadata":{"id":"x-BBLqldKMce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install tiktoken"],"metadata":{"id":"-QDhcWZAKUsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import os\n","import openai\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.docstore.document import Document\n","\n","# 0) Vector Index (FAISS) initialisieren\n","#    (SpÃ¤ter im Code kÃ¶nnen wir den Index persistent speichern/neu laden)\n","# os.environ[\"OPENAI_API_KEY\"] = \"DEIN_OPENAI_API_KEY\"\n","\n","embeddings = OpenAIEmbeddings()\n","\n","# Beispiel-Fehler als \"Dokument\" fÃ¼r den Vector Store\n","# \"page_content\" = Text, \"metadata\" = beliebige Zusatzinfos\n","known_error_text = \"\"\"\n","Fehler: \"Klaren Aligner\" wird fÃ¤lschlicherweise als Eigenname verwendet,\n","         obwohl es grammatisch richtig \"klaren Alignern\" sein sollte.\n","\n","Richtige Anwendung:\n","- Sagen: \"Entdecken Sie die Vorteile von klaren Alignern.\"\n","- Oder: \"Klare Aligner sind die ...\"\n","\n","ZusÃ¤tzliche Hinweise:\n","- Beim Eindeutschen englischer Fachbegriffe auf die Pluralbildung achten.\n","\"\"\"\n","\n","doc = Document(\n","    page_content=known_error_text,\n","    metadata={\"error_type\": \"grammar/de-english\", \"example_id\": \"klaren-aligner\"}\n",")\n","\n","# Vektorindex erzeugen und das \"bekannte Fehler\"-Dokument ablegen\n","vector_store = FAISS.from_documents([doc], embeddings)\n"],"metadata":{"id":"PHubUMS4Jirw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","from langchain.docstore.document import Document\n","\n","# 1. Neuer Fehler: \"Kauen- und Sprechprobleme\" statt \"Kau- und Sprechprobleme\"\n","doc1_text = \"\"\"\n","Fehler: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","Richtig: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kau- und Sprechprobleme effektiv behandeln.\"\n","\n","Grund:\n","- Falsche Rechtschreibung/Zusammensetzung bei \"Kauen-\".\n","- Richtig ist \"Kau- und Sprechprobleme\".\n","\"\"\"\n","\n","doc1 = Document(\n","    page_content=doc1_text,\n","    metadata={\n","        \"error_type\": \"grammar/spelling\",\n","        \"example_id\": \"kauen-sprechprobleme\"\n","    }\n",")\n","\n","# 2. Neuer Fehler: falsche Formulierung bei Sprachen\n","doc2_text = \"\"\"\n","Fehler: \"Als in Kenia geborene KieferorthopÃ¤din ... spricht neben Deutsch auch Englisch, Swahili sowie Ã¼ber Grundkenntnisse in Arabisch und AnfÃ¤ngerkenntnisse in Spanisch.\"\n","Richtig: \"Als in Kenia geborene KieferorthopÃ¤din ... spricht neben Deutsch auch Englisch und Swahili und verfÃ¼gt Ã¼ber Grundkenntnisse in Arabisch und Spanisch.\"\n","\n","Grund:\n","- Bessere Formulierung, um 'Ã¼ber Grundkenntnisse' mit 'verfÃ¼gt Ã¼ber Grundkenntnisse' zu vereinen.\n","- Straffere und klarere Satzstruktur.\n","\"\"\"\n","\n","doc2 = Document(\n","    page_content=doc2_text,\n","    metadata={\n","        \"error_type\": \"grammar/style\",\n","        \"example_id\": \"languages-phrase\"\n","    }\n",")\n","\n","\n","# Angenommen, du hast bereits:\n","# embeddings = OpenAIEmbeddings()\n","# vector_store = FAISS.from_documents([some_initial_docs], embeddings)\n","#\n","# -> Dann fÃ¼gen wir jetzt doc1 und doc2 hinzu:\n","\n","vector_store.add_documents([doc1, doc2])\n"],"metadata":{"id":"ViQDjBE8WqAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u"},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def chunk_text_2(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    FÃ¼r echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text_2(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","from langchain.text_splitter import TokenTextSplitter\n","\n","def chunk_text_langchain(text, max_tokens=500, overlap=50):\n","    \"\"\"\n","    Teilt den Text anhand der Tokenanzahl auf. Nutzt dafÃ¼r LangChain's TokenTextSplitter.\n","    - max_tokens: maximale Tokens pro Chunk\n","    - overlap: wie viele Tokens Ãœberschneidung zum vorherigen Chunk\n","    \"\"\"\n","    splitter = TokenTextSplitter(\n","        encoding_name=\"cl100k_base\",  # oder passend zu deinem Modell (z.B. \"gpt-3.5-turbo\")\n","        chunk_size=max_tokens,         # maximale Anzahl Tokens pro Chunk\n","        chunk_overlap=overlap          # Tokens, die sich mit dem vorigen Chunk Ã¼berschneiden (Kontext)\n","    )\n","\n","    chunks = splitter.split_text(text)\n","    return chunks\n","\n","# Beispielanwendung:\n","# seo_text = \"\"\"Hier Dein langer Text, den du chunken willst ...\"\"\"\n","# chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","# print(chunked)\n","\n","\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","    chunked_texts.append(text_chunks)\n"],"metadata":{"id":"FspYaRcb_hOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk Ã¤hneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenfÃ¼gen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","# test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","# retrieved_context = get_context_from_vector_store(test_chunk)\n","# print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu prÃ¼fen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu berÃ¼cksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu prÃ¼fenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu prÃ¼fen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur PrÃ¼fung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text grÃ¼ndlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu verÃ¤ndern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zusÃ¤tzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein gÃ¼ltiges JSON zurÃ¼ckgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine gÃ¼ltige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","all_corrected_texts = []\n","all_new_mistakes = []\n","\n","#for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","#    corrected_text_chunks = []\n","\n","#    for chunk in text_chunks:\n","#        # 3a) Kontext abfragen\n","#        context = get_context_from_vector_store(chunk)\n","#\n","#\n","#       # 4a) Prompt ChatGPT (Korrektur)\n","#        result = proofread_text_with_context(chunk, context)\n","#\n","#        corrected_text = result[\"corrected_text\"]\n","#        new_mistakes = result[\"new_mistakes_found\"]\n","#\n","#        # Sammeln\n","#        corrected_text_chunks.append(corrected_text)\n","#        all_new_mistakes.extend(new_mistakes)\n","#\n","#    # Pro SEO-Text fÃ¼gen wir die korrigierten Chunks zusammen.\n","#    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","#    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# for _ in all_corrected_texts:\n","#   print(_)"],"metadata":{"id":"qIUE9Dfl5YzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3"},"execution_count":null,"outputs":[]}]}