{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["o38bsnD-EWcq","vNVT-lr5jejz","E0k0HmUZHmkg","qGy_OuMyIC9C","CdWeu44YIs5k","HraJ3_5SJHSv"],"authorship_tag":"ABX9TyMQzFbLIBepw0Hr2+fcYBxE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ‚õπ push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["#%%capture\n","import os\n","from google.colab import drive\n","from google.colab import userdata\n","drive.mount('/content/drive',\n","            force_remount=True\n","            )\n","\n","\n","notebookname = 'SEO.ipynb'\n","\n","class github:\n","    def __init__(self, github_pat, github_email, github_username, github_repo, gdrive_notebook_folder, notebook_name):\n","        self.github_pat = userdata.get(github_pat)\n","        self.github_email = userdata.get(github_email)\n","        self.github_username = userdata.get(github_username)\n","        self.github_repo = userdata.get(github_repo)\n","        self.gdrive_notebook_folder = userdata.get(gdrive_notebook_folder)\n","        self.notebook_name = notebook_name\n","\n","    def clone_repo(self):\n","        # Source file path in Google Drive\n","        source_file_path = f\"/content/drive/MyDrive/{self.gdrive_notebook_folder}/{self.notebook_name}\"\n","\n","        # Repository details\n","        repo_url = f'https://{self.github_pat}@github.com/{self.github_username}/{self.github_repo}.git'\n","\n","        # Clone the private repository\n","        !git clone {repo_url} cloned-repo\n","        os.chdir('cloned-repo')  # Switch to the cloned repository\n","\n","        # Ensure the file exists in Google Drive\n","        if os.path.exists(source_file_path):\n","            # Copy the notebook into the cloned repository\n","            !cp \"{source_file_path}\" ./\n","        else:\n","            print(f\"The file {source_file_path} was not found.\")\n","            return  # Exit if the file doesn't exist\n","\n","        # Git configuration\n","        !git config user.email \"{self.github_email}\"\n","        !git config user.name \"{self.github_username}\"\n","\n","        # Add the file to Git\n","        !git add \"{self.notebook_name}\"\n","\n","        # Commit the changes\n","        !git commit -m \"Added {self.notebook_name} from Google Drive\"\n","\n","        # Push to the repository\n","        !git push origin main\n","\n","        # Wechsle zur√ºck ins √ºbergeordnete Verzeichnis und l√∂sche cloned-repo\n","        os.chdir('..')\n","        !rm -rf cloned-repo\n","        print(\"cloned-repo wurde wieder gel√∂scht.\")\n","\n","\n","\n","# Clone, add, and push the notebook\n","clone_2 = github('github_pat', 'github_email', 'github_username', 'github_repo_seo', 'gdrive_seo_folder', notebookname)\n","clone_2.clone_repo()\n"],"metadata":{"id":"p-LWoIwDfU3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, Comment\n","from urllib.parse import urljoin, urlparse\n","import chardet\n","\n","\n","class WebsiteScraper:\n","    \"\"\"\n","    Diese Klasse k√ºmmert sich ausschlie√ülich um das Sammeln und Extrahieren\n","    von Texten aus einer Website.\n","    \"\"\"\n","\n","    def __init__(self, start_url, max_pages=50):\n","        \"\"\"\n","        :param start_url: Die Start-URL der Website, z.B. \"https://www.example.com\"\n","        :param max_pages: Maximale Anzahl Seiten, die gecrawlt werden.\n","        \"\"\"\n","        self.start_url = start_url\n","        self.max_pages = max_pages\n","\n","        # Hier speichern wir {URL: reiner_Text}\n","        self.scraped_data = {}\n","\n","    def scrape_website(self):\n","        \"\"\"\n","        Startet den Crawl-Vorgang, gefolgt von der Extraktion des Textes\n","        und dem Sammeln interner Links.\n","        \"\"\"\n","        visited = set()\n","        to_visit = [self.start_url]\n","        domain = urlparse(self.start_url).netloc\n","\n","        while to_visit and len(visited) < self.max_pages:\n","            url = to_visit.pop(0)\n","            if url in visited:\n","                continue\n","            visited.add(url)\n","\n","            try:\n","                response = requests.get(url, timeout=10)\n","\n","                # Rohdaten holen und Encoding per chardet bestimmen\n","                raw_data = response.content\n","                detected = chardet.detect(raw_data)\n","                encoding = \"utf-8\"\n","                text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","                # Nur weiterverarbeiten, wenn HTML-Content\n","                if (response.status_code == 200\n","                        and \"text/html\" in response.headers.get(\"Content-Type\", \"\")):\n","                    soup = BeautifulSoup(text_data, \"html.parser\")\n","\n","                    # Text extrahieren\n","                    text = self._extract_text_from_soup(soup)\n","                    self.scraped_data[url] = text\n","\n","                    # Interne Links sammeln\n","                    for link in soup.find_all(\"a\", href=True):\n","                        absolute_link = urljoin(url, link[\"href\"])\n","                        if urlparse(absolute_link).netloc == domain:\n","                            if (absolute_link not in visited\n","                                    and absolute_link not in to_visit):\n","                                to_visit.append(absolute_link)\n","\n","            except requests.RequestException as e:\n","                print(f\"Fehler beim Abrufen von {url}:\\n{e}\")\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        entfernt Script-/Style-/Noscript-Tags und Kommentare.\n","        \"\"\"\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        texts = []\n","        for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"]):\n","            txt = tag.get_text(strip=True)\n","            if txt:\n","                texts.append(txt)\n","\n","        return \"\\n\".join(texts)\n","\n","    def get_scraped_data(self):\n","        \"\"\"\n","        Gibt das Dictionary {URL: Text} zur√ºck.\n","        Du kannst damit arbeiten, Seiten filtern, etc.\n","        \"\"\"\n","        return self.scraped_data\n"],"metadata":{"id":"rrOBAe72aF6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ü§ñ chatbot"],"metadata":{"id":"E0k0HmUZHmkg"}},{"cell_type":"code","source":["import openai\n","import time\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","class Chatbot:\n","    \"\"\"\n","    Diese Chatbot-Klasse nutzt die neue Methode client.chat.completions.create()\n","    aus openai>=1.0.0 √ºber openai.OpenAI().\n","    \"\"\"\n","\n","    def __init__(self, systemprompt, prompt):\n","        self.client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n","        self.systemprompt = systemprompt\n","        self.prompt = prompt\n","        self.context = [{\"role\": \"system\", \"content\": systemprompt}]\n","        self.model = \"gpt-4o-mini-2024-07-18\"  # Beispiel-Modell\n","\n","    def chat(self):\n","        \"\"\"\n","        Sendet den Prompt an das Chat-Interface und gibt den kompletten Antwort-String zur√ºck.\n","        \"\"\"\n","        self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model,\n","                messages=self.context\n","            )\n","            response_content = response.choices[0].message.content\n","            self.context.append({\"role\": \"assistant\", \"content\": response_content})\n","            return response_content\n","        except Exception as e:\n","            print(f\"Fehler bei der OpenAI-Anfrage: {e}\")\n","            return \"\"\n","\n","\n","    def chat_with_streaming(self):\n","            \"\"\"\n","            Interagiert mit OpenAI Chat Completion API und streamt die Antwort.\n","            \"\"\"\n","            # Nachricht zur Konversation hinzuf√ºgen\n","            self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","\n","            # print(f\"User: {self.prompt}\")\n","            # print(\"AI: \", end=\"\", flush=True)\n","\n","            try:\n","                # Streaming-Option aktivieren\n","                response = self.client.chat.completions.create(\n","                    model=self.model,\n","                    messages=self.context,\n","                    stream=True\n","                )\n","\n","                streamed_content = \"\"  # Zum Speichern der gestreamten Antwort\n","\n","                for chunk in response:\n","                    # Debugging: Anzeigen, was tats√§chlich in jedem Chunk enthalten ist\n","                    delta = chunk.choices[0].delta\n","                    content = getattr(delta, \"content\", \"\")\n","\n","                    if content:  # Verarbeite nur nicht-leere Inhalte\n","                        print(content, end=\"\", flush=True)\n","                        streamed_content += content\n","\n","                print()  # Neue Zeile am Ende\n","\n","                # Gestreamte Antwort zur Konversation hinzuf√ºgen\n","                self.context.append({\"role\": \"assistant\", \"content\": streamed_content})\n","\n","                # Return the streamed content\n","                return streamed_content # This line was added\n","\n","            except Exception as e:\n","                print(f\"\\nDEBUG: An error occurred during streaming: {e}\")\n","                # Return empty string in case of error\n","                return \"\" # This line was added\n"],"metadata":{"id":"j6ZZeUIRdYhy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üÜé NLP"],"metadata":{"id":"qGy_OuMyIC9C"}},{"cell_type":"code","source":["def chunk_text(text, max_tokens=2000):\n","    \"\"\"\n","    Teilt den Text in Bl√∂cke auf, damit er nicht zu lang\n","    f√ºr die OpenAI-API wird.\n","    Hier sehr vereinfacht: 1 Token ~ 4 Zeichen.\n","    \"\"\"\n","    chunks = []\n","    approx_char_limit = max_tokens * 4\n","    start = 0\n","    while start < len(text):\n","        end = start + approx_char_limit\n","        chunk = text[start:end]\n","        chunks.append(chunk)\n","        start = end\n","    return chunks\n"],"metadata":{"id":"3oUna7Bodcp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üöß main SEO optimize"],"metadata":{"id":"CdWeu44YIs5k"}},{"cell_type":"code","source":["combined_analysis_list = []\n","filtered_urls = []\n","\n","def main():\n","    # 1. SCRAPING\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=10)\n","    scraper.scrape_website()\n","\n","    # Alle gescrapten Daten abrufen\n","    scraped_data = scraper.get_scraped_data()\n","\n","    # 2. Sichten der Texte und Filtern\n","    #    Hier k√∂nntest du jetzt z. B. manuell pr√ºfen, welche URLs wichtig sind.\n","    #    Wir geben einfach mal alle URLs aus:\n","    print(\"\\n--- Gesammelte Seiten und Inhalte (gek√ºrzt) ---\")\n","    for url, text in scraped_data.items():\n","        print(f\"\\nURL: {url}\")\n","        # Beispiel: Nur ersten 200 Zeichen zeigen\n","        print(f\"Text: {text[:200]}...\")\n","\n","\n","\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, √ºberspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","\n","\n","\n","\n","\n","    # 3. SEO-Analyse starten (f√ºr gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","\n","        # 3.1 Chunken, um zu gro√üe Anfragen zu vermeiden\n","        text_chunks = chunk_text(page_text, max_tokens=2000)\n","\n","        print(f\"\\n=== Analyzing {url} ===\")\n","        all_analyses = []\n","        for i, chunk in enumerate(text_chunks):\n","            print(f\" - Sende Chunk {i+1}/{len(text_chunks)} an Chatbot ...\")\n","\n","            # Prompt definieren (SEO)\n","            system_prompt = \"Du bist ein hochqualifizierter SEO-Experte.\"\n","            user_prompt = (\n","                \"1. Untersuche den folgenden Text auf Keyword-Optimierung, Lesbarkeit und m√∂gliche SEO-Verbesserungen. \"\n","                \"2. Formuliere den Text entsprechend optimaler SEO Sichtbarkeit um. Der Tonfall soll weiterhin nett und freundlich sein und einen warmen, einf√ºhlsamen Eindruck machen.\"\n","                \"3. Als Ausgabe gebe deine optimierte version wieder und erl√§uterungen zur optimierung. gebe den abschnitten die √ºberschriften 'SEO optimierter Text' und 'Erl√§uterungen'! Benutze keine Formatierungszeichen wie '###' oder '#'! \"\n","\n","                \"Hier ist der Text chunk: \\n\\n\"\n","                f\"{chunk}\"\n","            )\n","\n","            # ChatGPT aufrufen\n","            cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","            analysis = cb.chat_with_streaming()\n","            all_analyses.append(analysis)\n","\n","            # Warte kurz (Rate Limits, API-Kosten etc.)\n","            time.sleep(1)\n","\n","        # 3.2 Fertige Analyse (alle Chunks zusammen)\n","        combined_analysis = \"\\n\".join(all_analyses)\n","\n","\n","        combined_analysis_list.append(combined_analysis)\n","        # print(f\"\\n--- SEO-Analyse f√ºr {url} ---\")\n","        # print(combined_analysis)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"nF-cfM35dhxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def extract_seo_optimized_texts(analysis_list):\n","    \"\"\"\n","    Sucht in jedem String der analysis_list nach Passagen unter der √úberschrift\n","    'SEO optimierter Text' und gibt diese als Liste zur√ºck.\n","    \"\"\"\n","    # Regex-Pattern (mit Dotall-Flag (?s), damit '.' auch Zeilenumbr√ºche matcht):\n","    # - 'SEO optimierter Text' gefolgt von beliebigem Whitespace.\n","    # - anschlie√üend erfassen wir (.*?), also den 'Text' bis zum Lookahead\n","    #   auf eine Zeile, die mit 'Erl√§uterungen' beginnt oder auf das String-Ende ($).\n","    pattern = re.compile(r\"(?s)SEO optimierter Text\\s*(.*?)(?=\\nErl√§uterungen|$)\")\n","\n","    seo_texts = []\n","\n","    for analysis_output in analysis_list:\n","        # Alle Fundstellen (falls mehrfach pro String)\n","        matches = pattern.findall(analysis_output)\n","        for match in matches:\n","            # Whitespace trimmen\n","            seo_text = match.strip()\n","            # Falls n√∂tig, weitere Bereinigungen an seo_text vornehmen\n","            seo_texts.append(seo_text)\n","\n","    return seo_texts\n","\n","# Beispielaufruf:\n","if __name__ == \"__main__\":\n","    # Angenommen, du hast im main() bereits `combined_analysis_list` gef√ºllt:\n","    # combined_analysis_list = [ ... alle LLM-Outputs ... ]\n","\n","    # Dann rufst du:\n","    seo_optimized_texts = extract_seo_optimized_texts(combined_analysis_list)\n","\n","    # Jetzt enth√§lt seo_optimized_texts ausschlie√ülich die Inhalte unter\n","    # \"SEO optimierter Text\".\n","    # Dies kannst du nun f√ºr deinen RAG-Workflow (Punkt 1) verwenden.\n","    print(\"\\n--- Extrahierte SEO-optimierte Texte ---\")\n","    for i, seo_text in enumerate(seo_optimized_texts):\n","        print(f\"\\nOptimierter Text f√ºr {filtered_urls[i]}:\\n{seo_text}\")\n"],"metadata":{"id":"OXaY96ApbuX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_1 = []\n","for i, text in  enumerate(seo_optimized_texts):\n","    list_1.append(f\"{filtered_urls[i]} optimierter Text: {text}\")"],"metadata":{"id":"qKQhdSZGD1t5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(systemprompt=f'Sei ein Lektor f√ºr SEO optimierte Texte. Du bist spezialisiert auf Grammatikfehler, eingedeutschte W√∂rter und Rechtschreibfehler', prompt=f\"√úberpr√ºfe die folgenden Texte auf Fehler und korrigiere diese. Achte ebenfalls auf Schw√§chen in der Ausdrucksweise oder fehlende Satzzeichen! Gebe als Ausgabe die Texte zur√ºck (einschlie√ülich dem link), mit deinen Verbesserungen falls notwendig. {list_1}\")\n","print(cb.chat())"],"metadata":{"id":"dH6bT3ScB6rH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthop√§din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\""],"metadata":{"id":"fH4mRdPXceeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install langchain faiss-cpu\n"],"metadata":{"id":"4DOE2XX9dmow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -U langchain-community"],"metadata":{"id":"x-BBLqldKMce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tiktoken"],"metadata":{"id":"-QDhcWZAKUsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import openai\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.docstore.document import Document\n","\n","# 0) Vector Index (FAISS) initialisieren\n","#    (Sp√§ter im Code k√∂nnen wir den Index persistent speichern/neu laden)\n","# os.environ[\"OPENAI_API_KEY\"] = \"DEIN_OPENAI_API_KEY\"\n","\n","embeddings = OpenAIEmbeddings()\n","\n","# Beispiel-Fehler als \"Dokument\" f√ºr den Vector Store\n","# \"page_content\" = Text, \"metadata\" = beliebige Zusatzinfos\n","known_error_text = \"\"\"\n","Fehler: \"Klaren Aligner\" wird f√§lschlicherweise als Eigenname verwendet,\n","         obwohl es grammatisch richtig \"klaren Alignern\" sein sollte.\n","\n","Richtige Anwendung:\n","- Sagen: \"Entdecken Sie die Vorteile von klaren Alignern.\"\n","- Oder: \"Klare Aligner sind die ...\"\n","\n","Zus√§tzliche Hinweise:\n","- Beim Eindeutschen englischer Fachbegriffe auf die Pluralbildung achten.\n","\"\"\"\n","\n","doc = Document(\n","    page_content=known_error_text,\n","    metadata={\"error_type\": \"grammar/de-english\", \"example_id\": \"klaren-aligner\"}\n",")\n","\n","# Vektorindex erzeugen und das \"bekannte Fehler\"-Dokument ablegen\n","vector_store = FAISS.from_documents([doc], embeddings)\n"],"metadata":{"id":"PHubUMS4Jirw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.docstore.document import Document\n","\n","# 1. Neuer Fehler: \"Kauen- und Sprechprobleme\" statt \"Kau- und Sprechprobleme\"\n","doc1_text = \"\"\"\n","Fehler: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","Richtig: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kau- und Sprechprobleme effektiv behandeln.\"\n","\n","Grund:\n","- Falsche Rechtschreibung/Zusammensetzung bei \"Kauen-\".\n","- Richtig ist \"Kau- und Sprechprobleme\".\n","\"\"\"\n","\n","doc1 = Document(\n","    page_content=doc1_text,\n","    metadata={\n","        \"error_type\": \"grammar/spelling\",\n","        \"example_id\": \"kauen-sprechprobleme\"\n","    }\n",")\n","\n","# 2. Neuer Fehler: falsche Formulierung bei Sprachen\n","doc2_text = \"\"\"\n","Fehler: \"Als in Kenia geborene Kieferorthop√§din ... spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","Richtig: \"Als in Kenia geborene Kieferorthop√§din ... spricht neben Deutsch auch Englisch und Swahili und verf√ºgt √ºber Grundkenntnisse in Arabisch und Spanisch.\"\n","\n","Grund:\n","- Bessere Formulierung, um '√ºber Grundkenntnisse' mit 'verf√ºgt √ºber Grundkenntnisse' zu vereinen.\n","- Straffere und klarere Satzstruktur.\n","\"\"\"\n","\n","doc2 = Document(\n","    page_content=doc2_text,\n","    metadata={\n","        \"error_type\": \"grammar/style\",\n","        \"example_id\": \"languages-phrase\"\n","    }\n",")\n","\n","\n","# Angenommen, du hast bereits:\n","# embeddings = OpenAIEmbeddings()\n","# vector_store = FAISS.from_documents([some_initial_docs], embeddings)\n","#\n","# -> Dann f√ºgen wir jetzt doc1 und doc2 hinzu:\n","\n","vector_store.add_documents([doc1, doc2])\n"],"metadata":{"id":"ViQDjBE8WqAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u"},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def chunk_text(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    F√ºr echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in seo_optimized_texts:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import TokenTextSplitter\n","\n","def chunk_text_langchain(text, max_tokens=500, overlap=50):\n","    \"\"\"\n","    Teilt den Text anhand der Tokenanzahl auf. Nutzt daf√ºr LangChain's TokenTextSplitter.\n","    - max_tokens: maximale Tokens pro Chunk\n","    - overlap: wie viele Tokens √úberschneidung zum vorherigen Chunk\n","    \"\"\"\n","    splitter = TokenTextSplitter(\n","        encoding_name=OpenAIEmbeddings(),  # oder passend zu deinem Modell (z.B. \"gpt-3.5-turbo\")\n","        chunk_size=max_tokens,         # maximale Anzahl Tokens pro Chunk\n","        chunk_overlap=overlap          # Tokens, die sich mit dem vorigen Chunk √ºberschneiden (Kontext)\n","    )\n","\n","    chunks = splitter.split_text(text)\n","    return chunks\n","\n","# Beispielanwendung:\n","# seo_text = \"\"\"Hier Dein langer Text, den du chunken willst ...\"\"\"\n","# chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","# print(chunked)\n","\n","\n","\n","chunked_texts = []\n","for seo_text in seo_optimized_texts:\n","    # Chunking pro SEO-Text\n","    chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","    chunked_texts.append(text_chunks)\n"],"metadata":{"id":"FspYaRcb_hOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunked_texts"],"metadata":{"id":"xt9uwzh5AjKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk √§hneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenf√ºgen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","retrieved_context = get_context_from_vector_store(test_chunk)\n","print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu pr√ºfen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu ber√ºcksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu pr√ºfenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu pr√ºfen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur Pr√ºfung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text gr√ºndlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu ver√§ndern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zus√§tzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein g√ºltiges JSON zur√ºckgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine g√ºltige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_corrected_texts = []\n","all_new_mistakes = []\n","\n","for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","    corrected_text_chunks = []\n","\n","    for chunk in text_chunks:\n","        # 3a) Kontext abfragen\n","        context = get_context_from_vector_store(chunk)\n","\n","\n","        # 4a) Prompt ChatGPT (Korrektur)\n","        result = proofread_text_with_context(chunk, context)\n","\n","        corrected_text = result[\"corrected_text\"]\n","        new_mistakes = result[\"new_mistakes_found\"]\n","\n","        # Sammeln\n","        corrected_text_chunks.append(corrected_text)\n","        all_new_mistakes.extend(new_mistakes)\n","\n","    # Pro SEO-Text f√ºgen wir die korrigierten Chunks zusammen.\n","    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _ in all_corrected_texts:\n","  print(_)"],"metadata":{"id":"qIUE9Dfl5YzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3"},"execution_count":null,"outputs":[]}]}