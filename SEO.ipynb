{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["o38bsnD-EWcq","vNVT-lr5jejz","FA2RIeko9reT","qGy_OuMyIC9C","HraJ3_5SJHSv"],"authorship_tag":"ABX9TyPtQSVskD7a7Ng/Ua4JM2k3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["#%%capture\n","import os\n","from google.colab import drive\n","from google.colab import userdata\n","drive.mount('/content/drive',\n","            force_remount=True\n","            )\n","\n","\n","notebookname = 'SEO.ipynb'\n","\n","class github:\n","    def __init__(self, github_pat, github_email, github_username, github_repo, gdrive_notebook_folder, notebook_name):\n","        self.github_pat = userdata.get(github_pat)\n","        self.github_email = userdata.get(github_email)\n","        self.github_username = userdata.get(github_username)\n","        self.github_repo = userdata.get(github_repo)\n","        self.gdrive_notebook_folder = userdata.get(gdrive_notebook_folder)\n","        self.notebook_name = notebook_name\n","\n","    def clone_repo(self):\n","        # Source file path in Google Drive\n","        source_file_path = f\"/content/drive/MyDrive/{self.gdrive_notebook_folder}/{self.notebook_name}\"\n","\n","        # Repository details\n","        repo_url = f'https://{self.github_pat}@github.com/{self.github_username}/{self.github_repo}.git'\n","\n","        # Clone the private repository\n","        !git clone {repo_url} cloned-repo\n","        os.chdir('cloned-repo')  # Switch to the cloned repository\n","\n","        # Ensure the file exists in Google Drive\n","        if os.path.exists(source_file_path):\n","            # Copy the notebook into the cloned repository\n","            !cp \"{source_file_path}\" ./\n","        else:\n","            print(f\"The file {source_file_path} was not found.\")\n","            return  # Exit if the file doesn't exist\n","\n","        # Git configuration\n","        !git config user.email \"{self.github_email}\"\n","        !git config user.name \"{self.github_username}\"\n","\n","        # Add the file to Git\n","        !git add \"{self.notebook_name}\"\n","\n","        # Commit the changes\n","        !git commit -m \"Added {self.notebook_name} from Google Drive\"\n","\n","        # Push to the repository\n","        !git push origin main\n","\n","        # Wechsle zur√ºck ins √ºbergeordnete Verzeichnis und l√∂sche cloned-repo\n","        os.chdir('..')\n","        !rm -rf cloned-repo\n","        print(\"cloned-repo wurde wieder gel√∂scht.\")\n","\n","\n","\n","# Clone, add, and push the notebook\n","clone_2 = github('github_pat', 'github_email', 'github_username', 'github_repo_seo', 'gdrive_seo_folder', notebookname)\n","clone_2.clone_repo()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-LWoIwDfU3Z","executionInfo":{"status":"ok","timestamp":1737483784763,"user_tz":-60,"elapsed":35628,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"be2151f6-a69c-4236-9b5c-094ed766ddc4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Cloning into 'cloned-repo'...\n","remote: Enumerating objects: 117, done.\u001b[K\n","remote: Counting objects: 100% (117/117), done.\u001b[K\n","remote: Compressing objects: 100% (86/86), done.\u001b[K\n","remote: Total 117 (delta 33), reused 106 (delta 28), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (117/117), 1.15 MiB | 3.66 MiB/s, done.\n","Resolving deltas: 100% (33/33), done.\n","[main d77bae4] Added SEO.ipynb from Google Drive\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite SEO.ipynb (99%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 942 bytes | 942.00 KiB/s, done.\n","Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/MarkH0705/SEO_Analyses.git\n","   e1338e7..d77bae4  main -> main\n","cloned-repo wurde wieder gel√∂scht.\n"]}]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, Comment\n","from urllib.parse import urljoin, urlparse\n","import chardet\n","\n","\n","class WebsiteScraper:\n","    \"\"\"\n","    Diese Klasse k√ºmmert sich ausschlie√ülich um das Sammeln und Extrahieren\n","    von Texten aus einer Website.\n","    \"\"\"\n","\n","    def __init__(self, start_url=\"https://www.rue-zahnspange.de\", max_pages=50):\n","        \"\"\"\n","        :param start_url: Die Start-URL der Website, z.B. \"https://www.example.com\"\n","        :param max_pages: Maximale Anzahl Seiten, die gecrawlt werden.\n","        \"\"\"\n","        self.start_url = start_url\n","        self.max_pages = max_pages\n","\n","        # Hier speichern wir {URL: reiner_Text}\n","        self.scraped_data = {}\n","\n","    def scrape_website(self):\n","        \"\"\"\n","        Startet den Crawl-Vorgang, gefolgt von der Extraktion des Textes\n","        und dem Sammeln interner Links.\n","        \"\"\"\n","        visited = set()\n","        to_visit = [self.start_url]\n","        domain = urlparse(self.start_url).netloc\n","\n","        while to_visit and len(visited) < self.max_pages:\n","            url = to_visit.pop(0)\n","            if url in visited:\n","                continue\n","            visited.add(url)\n","\n","            try:\n","                response = requests.get(url, timeout=10)\n","\n","                # Rohdaten holen und Encoding per chardet bestimmen\n","                raw_data = response.content\n","                detected = chardet.detect(raw_data)\n","                encoding = \"utf-8\"\n","                text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","                # Nur weiterverarbeiten, wenn HTML-Content\n","                if (response.status_code == 200\n","                        and \"text/html\" in response.headers.get(\"Content-Type\", \"\")):\n","                    soup = BeautifulSoup(text_data, \"html.parser\")\n","\n","                    # Text extrahieren\n","                    text = self._extract_text_from_soup(soup)\n","                    self.scraped_data[url] = text\n","\n","                    # Interne Links sammeln\n","                    for link in soup.find_all(\"a\", href=True):\n","                        absolute_link = urljoin(url, link[\"href\"])\n","                        if urlparse(absolute_link).netloc == domain:\n","                            if (absolute_link not in visited\n","                                    and absolute_link not in to_visit):\n","                                to_visit.append(absolute_link)\n","\n","            except requests.RequestException as e:\n","                print(f\"Fehler beim Abrufen von {url}:\\n{e}\")\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        entfernt Script-/Style-/Noscript-Tags und Kommentare.\n","        \"\"\"\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        texts = []\n","        for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\", \"faq4_question\", \"faq4_answer\"]):\n","            txt = tag.get_text(strip=True)\n","            if txt:\n","                texts.append(txt)\n","\n","        return \"\\n\".join(texts)\n","\n","\n","\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        aber NICHT die, die in .faq4_question oder .faq4_answer stecken.\n","        Au√üerdem extrahiert er separat die FAQ-Fragen und -Antworten\n","        (faq4_question / faq4_answer).\n","        \"\"\"\n","        # 1) Script/Style entfernen\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        # 2) Kommentare entfernen\n","        from bs4 import Comment\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        # 3) Normale Texte (p, h1, h2, h3, li), ABER nicht innerhalb von .faq4_question / .faq4_answer\n","        texts = []\n","        all_normal_tags = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])\n","        for tag in all_normal_tags:\n","            # Pr√ºfen, ob das Tag einen Vorfahren (Parent) hat mit Klasse faq4_question oder faq4_answer\n","            # Wenn ja, ignorieren wir es\n","            if tag.find_parent(class_=\"faq4_question\") or tag.find_parent(class_=\"faq4_answer\"):\n","                continue\n","\n","            txt = tag.get_text(strip=True)\n","            if txt:\n","                texts.append(txt)\n","\n","        # 4) FAQ-Bereiche (Fragen + Antworten)\n","        #    a) Alle Frage-Elemente mit Klasse .faq4_question\n","        #    b) Alle Antwort-Elemente mit Klasse .faq4_answer\n","        #    Wir gehen davon aus, dass Frage i zum Antwort i passt.\n","\n","        questions = soup.select(\".faq4_question\")\n","        answers = soup.select(\".faq4_answer\")\n","\n","        # 5) Zusammenf√ºhren (Frage + Antwort)\n","        for q, a in zip(questions, answers):\n","            q_text = q.get_text(strip=True)\n","            a_text = a.get_text(strip=True)\n","            if q_text and a_text:\n","                combined = f\"Frage: {q_text}\\nAntwort: {a_text}\"\n","                texts.append(combined)\n","\n","        # 6) Als String zur√ºckgeben\n","        return \"\\n\".join(texts)\n","\n","\n","\n","\n","\n","    def get_scraped_data(self):\n","        \"\"\"\n","        Gibt das Dictionary {URL: Text} zur√ºck.\n","        Du kannst damit arbeiten, Seiten filtern, etc.\n","        \"\"\"\n","        return self.scraped_data\n"],"metadata":{"id":"rrOBAe72aF6D","executionInfo":{"status":"ok","timestamp":1737483785785,"user_tz":-60,"elapsed":1025,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# üìäSEO Analysis"],"metadata":{"id":"FA2RIeko9reT"}},{"cell_type":"code","source":["pip install git+https://github.com/sethblack/python-seo-analyzer.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FqjBpIRJZC5B","executionInfo":{"status":"ok","timestamp":1737483795301,"user_tz":-60,"elapsed":9519,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"1972cc90-32e5-474c-e531-5b5e9965cfa9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/sethblack/python-seo-analyzer.git\n","  Cloning https://github.com/sethblack/python-seo-analyzer.git to /tmp/pip-req-build-hj5w6o8m\n","  Running command git clone --filter=blob:none --quiet https://github.com/sethblack/python-seo-analyzer.git /tmp/pip-req-build-hj5w6o8m\n","  Resolved https://github.com/sethblack/python-seo-analyzer.git to commit cfb38f5b39803ec1dc597158c9226ecd9bc07511\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (4.12.3)\n","Requirement already satisfied: certifi>=2024.8.30 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (2024.12.14)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (3.1.5)\n","Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (5.3.0)\n","Requirement already satisfied: markupsafe>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (3.0.2)\n","Collecting trafilatura>=2.0.0 (from pyseoanalyzer==2024.12.12)\n","  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12) (2.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->pyseoanalyzer==2024.12.12) (2.6)\n","Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (3.4.1)\n","Collecting courlan>=1.3.2 (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n","Collecting htmldate>=1.9.2 (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n","Collecting justext>=3.0.1 (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading jusText-3.0.1-py2.py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (2.16.0)\n","Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n","Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n","Collecting python-dateutil>=2.9.0.post0 (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (2024.2)\n","Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (2024.11.6)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (5.2)\n","Collecting lxml-html-clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12)\n","  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12) (1.17.0)\n","Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n","Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n","Downloading jusText-3.0.1-py2.py3-none-any.whl (837 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: pyseoanalyzer\n","  Building wheel for pyseoanalyzer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyseoanalyzer: filename=pyseoanalyzer-2024.12.12-py3-none-any.whl size=18575 sha256=2268ff5c1d4e5af2033045b7ee469e29e9e1ef5bb6d332b126d6254e06a314e9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qtyuo4q7/wheels/ae/fb/22/dd94b93adf36aa542e9aacbd300fef8e4f9037af0d68f75cff\n","Successfully built pyseoanalyzer\n","Installing collected packages: tld, python-dateutil, lxml-html-clean, dateparser, courlan, justext, htmldate, trafilatura, pyseoanalyzer\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.8.2\n","    Uninstalling python-dateutil-2.8.2:\n","      Successfully uninstalled python-dateutil-2.8.2\n","Successfully installed courlan-1.3.2 dateparser-1.2.0 htmldate-1.9.3 justext-3.0.1 lxml-html-clean-0.4.1 pyseoanalyzer-2024.12.12 python-dateutil-2.9.0.post0 tld-0.13 trafilatura-2.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dateutil"]},"id":"dbc2feefee62455d876e13cecaf55593"}},"metadata":{}}]},{"cell_type":"code","source":["pip install langchain_anthropic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qckMLpKPbkME","executionInfo":{"status":"ok","timestamp":1737483805778,"user_tz":-60,"elapsed":10484,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"95116cf1-0c9f-4836-b6b3-8154e699cebe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_anthropic\n","  Downloading langchain_anthropic-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n","Collecting anthropic<1,>=0.41.0 (from langchain_anthropic)\n","  Downloading anthropic-0.44.0-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic) (0.7.1)\n","Collecting langchain-core<0.4.0,>=0.3.30 (from langchain_anthropic)\n","  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic) (2.10.5)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.41.0->langchain_anthropic) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.41.0->langchain_anthropic) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.41.0->langchain_anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.41.0->langchain_anthropic) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.41.0->langchain_anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.41.0->langchain_anthropic) (4.12.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (1.33)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (0.2.10)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (24.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (9.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (2.27.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.41.0->langchain_anthropic) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.41.0->langchain_anthropic) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.41.0->langchain_anthropic) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.41.0->langchain_anthropic) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (3.10.14)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (1.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain_anthropic) (2.3.0)\n","Downloading langchain_anthropic-0.3.3-py3-none-any.whl (22 kB)\n","Downloading anthropic-0.44.0-py3-none-any.whl (208 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.6/208.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: anthropic, langchain-core, langchain_anthropic\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.29\n","    Uninstalling langchain-core-0.3.29:\n","      Successfully uninstalled langchain-core-0.3.29\n","Successfully installed anthropic-0.44.0 langchain-core-0.3.31 langchain_anthropic-0.3.3\n"]}]},{"cell_type":"code","source":["pip install python-dotenv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ntrn5pexB1Px","executionInfo":{"status":"ok","timestamp":1737483807971,"user_tz":-60,"elapsed":2196,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"08f0aca4-8f2a-4937-b8fc-9d6030ada11b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","object address  : 0x7a10d26bb340\n","object refcount : 2\n","object type     : 0x9d5ea0\n","object type name: KeyboardInterrupt\n","object repr     : KeyboardInterrupt()\n","lost sys.stderr\n","^C\n"]}]},{"cell_type":"code","source":["import dotenv\n","from pyseoanalyzer import analyze\n","\n","\n","url = \"https://www.rue-zahnspange.de\"\n","report = analyze(url)\n","\n","# 'report' enth√§lt nun s√§mtliche Analyseergebnisse\n","print(report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"vK6jH0qmasx0","executionInfo":{"status":"error","timestamp":1737483807971,"user_tz":-60,"elapsed":19,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"4d92c9d4-b720-4294-ed51-18b91b1c9206"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'dotenv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0dc05285ad3e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyseoanalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.rue-zahnspange.de\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"-xmnF1Yyphji","executionInfo":{"status":"aborted","timestamp":1737483807971,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oAeeJEjYphb1","executionInfo":{"status":"aborted","timestamp":1737483807971,"user_tz":-60,"elapsed":18,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4VkvKAOfphUi","executionInfo":{"status":"aborted","timestamp":1737483807972,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tats√§chlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Sortieren nach H√§ufigkeit absteigend\n","df_keywords.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Nur die Top 10 Keywords anzeigen\n","df_top10 = df_keywords.head(66)\n","\n","# Einfaches Balkendiagramm\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_top10['word'], df_top10[\"count\"], color=\"skyblue\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.title(\"Top 10 Keywords\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"H√§ufigkeit\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"nW_LgHd1Wtie","executionInfo":{"status":"aborted","timestamp":1737483807972,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tats√§chlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Neue Spalte mit Kleinbuchstaben erzeugen\n","df_keywords[\"word_lower\"] = df_keywords[\"word\"].str.lower()\n","\n","# Die Keywords, nach denen du suchen m√∂chtest (Case-insensitive)\n","interesting_keywords = [\"zahnspange\", \"Invisalign\", \"kieferorthop√§die\", \"BEHANDLUNG\", \"Kosten\" , \"z√§hne\", \"brackets\", \"Unsichtbar\", \"Kinder\", \"Jugendliche\", \"Patienten\", \"l√§cheln\"]\n","# Auch diese in Kleinbuchstaben umwandeln\n","interesting_keywords_lower = [kw.lower() for kw in interesting_keywords]\n","\n","# DataFrame nach den gew√ºnschten Keywords filtern (Case-insensitive)\n","df_subset = df_keywords.loc[df_keywords[\"word_lower\"].isin(interesting_keywords_lower)].copy()\n","\n","# Sortieren nach H√§ufigkeit, damit das Diagramm √ºbersichtlicher wird\n","df_subset.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Balkendiagramm\n","plt.figure(figsize=(8, 5))\n","# Plotten kannst du z.B. weiterhin den Originalwert \"word\" (falls du im Diagramm\n","# die urspr√ºngliche Schreibweise sehen willst)\n","plt.bar(df_subset[\"word\"], df_subset[\"count\"], color=\"steelblue\")\n","plt.title(\"H√§ufigkeit ausgew√§hlter Keywords (Case-Insensitive)\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"Anzahl\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"p3q7KVe8j5Gx","executionInfo":{"status":"aborted","timestamp":1737483807972,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pages = report.get(\"pages\", [])\n","\n","df_pages = pd.DataFrame(pages)\n","\n","# Beispiel: Title-L√§nge berechnen\n","df_pages[\"title_length\"] = df_pages[\"title\"].apply(lambda x: len(x) if isinstance(x, str) else 0)\n","\n","# Balkendiagramm Title-L√§nge\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","plt.xticks(rotation=90)\n","plt.title(\"L√§nge der Title-Tags pro Seite\")\n","plt.xlabel(\"URL\")\n","plt.ylabel(\"Title-L√§nge (Zeichen)\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"bH80vXrlj5D5","executionInfo":{"status":"aborted","timestamp":1737483807972,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1 Zeile, 2 Spalten\n","\n","# (a) Keywords\n","axes[0].bar(df_top10[\"word\"].head(22), df_top10[\"count\"].head(22), color=\"skyblue\")\n","axes[0].set_title(\"Top 10 Keywords\")\n","axes[0].set_xticklabels(df_top10[\"word\"], rotation=45, ha=\"right\")\n","\n","# (b) Title-L√§ngen\n","axes[1].bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","axes[1].set_title(\"Title-L√§nge pro Seite\")\n","axes[1].set_xticklabels(df_pages[\"url\"], rotation=90)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"WGcvHhIAj5BO","executionInfo":{"status":"aborted","timestamp":1737483807972,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I94K8prmj4-Z","executionInfo":{"status":"aborted","timestamp":1737483807972,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCO3w_TOj47i","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ü§ñ chatbot"],"metadata":{"id":"E0k0HmUZHmkg"}},{"cell_type":"code","source":["import openai\n","import time\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","class Chatbot:\n","    \"\"\"\n","    Diese Chatbot-Klasse nutzt die neue Methode client.chat.completions.create()\n","    aus openai>=1.0.0 √ºber openai.OpenAI().\n","    \"\"\"\n","\n","    def __init__(self, systemprompt, prompt):\n","        self.client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n","        self.systemprompt = systemprompt\n","        self.prompt = prompt\n","        self.context = [{\"role\": \"system\", \"content\": systemprompt}]\n","        self.model = \"gpt-4o-mini-2024-07-18\"  # Beispiel-Modell\n","\n","    def chat(self):\n","        \"\"\"\n","        Sendet den Prompt an das Chat-Interface und gibt den kompletten Antwort-String zur√ºck.\n","        \"\"\"\n","        self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model,\n","                messages=self.context\n","            )\n","            response_content = response.choices[0].message.content\n","            self.context.append({\"role\": \"assistant\", \"content\": response_content})\n","            return response_content\n","        except Exception as e:\n","            print(f\"Fehler bei der OpenAI-Anfrage: {e}\")\n","            return \"\"\n","\n","\n","    def chat_with_streaming(self):\n","            \"\"\"\n","            Interagiert mit OpenAI Chat Completion API und streamt die Antwort.\n","            \"\"\"\n","            # Nachricht zur Konversation hinzuf√ºgen\n","            self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","\n","            # print(f\"User: {self.prompt}\")\n","            # print(\"AI: \", end=\"\", flush=True)\n","\n","            try:\n","                # Streaming-Option aktivieren\n","                response = self.client.chat.completions.create(\n","                    model=self.model,\n","                    messages=self.context,\n","                    stream=True\n","                )\n","\n","                streamed_content = \"\"  # Zum Speichern der gestreamten Antwort\n","\n","                for chunk in response:\n","                    # Debugging: Anzeigen, was tats√§chlich in jedem Chunk enthalten ist\n","                    delta = chunk.choices[0].delta\n","                    content = getattr(delta, \"content\", \"\")\n","\n","                    if content:  # Verarbeite nur nicht-leere Inhalte\n","                        print(content, end=\"\", flush=True)\n","                        streamed_content += content\n","\n","                print()  # Neue Zeile am Ende\n","\n","                # Gestreamte Antwort zur Konversation hinzuf√ºgen\n","                self.context.append({\"role\": \"assistant\", \"content\": streamed_content})\n","\n","                # Return the streamed content\n","                return streamed_content # This line was added\n","\n","            except Exception as e:\n","                print(f\"\\nDEBUG: An error occurred during streaming: {e}\")\n","                # Return empty string in case of error\n","                return \"\" # This line was added\n"],"metadata":{"id":"j6ZZeUIRdYhy","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üÜé NLP"],"metadata":{"id":"qGy_OuMyIC9C"}},{"cell_type":"code","source":["def chunk_text(text, max_tokens=10000):\n","    \"\"\"\n","    Teilt den Text in Bl√∂cke auf, damit er nicht zu lang\n","    f√ºr die OpenAI-API wird.\n","    Hier sehr vereinfacht: 1 Token ~ 4 Zeichen.\n","    \"\"\"\n","    chunks = []\n","    approx_char_limit = max_tokens * 4\n","    start = 0\n","    while start < len(text):\n","        end = start + approx_char_limit\n","        chunk = text[start:end]\n","        chunks.append(chunk)\n","        start = end\n","    return chunks\n"],"metadata":{"id":"3oUna7Bodcp9","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆkeywords"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["systemprompt_keywords = (\"\"\"\n","    Du bist ein intelligentes KI-System, das auf die Generierung von SEO-Keywords spezialisiert ist.\n","    Der Benutzer wird dir den kompletten Text von einer gescrapten website aus einem webcrawler vorgeben.\n","    Deine Aufgabe ist es, den Text zu interpretieren und eine Liste von SEO-Keywords basierend auf diesem Input zu erstellen.\n","    Du sollst zu dem Text passende SEO-Keywords finden. Auf Basis dieser keywords soll sp√§ter der Text optimiert werden.\n","    Stelle sicher, dass die Keywords:\n","\n","    Thematisch relevant sind,\n","    Hohe Suchintention abdecken (Short-Tail und Long-Tail Keywords),\n","    Varianten mit Synonymen oder verwandten Begriffen enthalten,\n","    Keywords mit lokaler Ausrichtung enthalten, falls relevant (z. B. \"Friseur in Berlin\").\n","\n","\n","    Struktur f√ºr die Antwort:\n","\n","    Erstelle eine serialisierte Liste von SEO-Keywords.\n","\n","    Beispiel f√ºr einen User-Eingabe:\n","\n","    \"Ein Blog √ºber gesunde Ern√§hrung f√ºr Sportler. Ein Sportler muss auf seine Ern√§hrung ganz besonders achten.\"\n","    Beispiel f√ºr eine Ausgabe:\n","\n","\n","\n","    [gesund, Ern√§hrung, Sportler, Ern√§hrungstipps, Sportern√§hrung Rezepte, Fitness, Sport]\n","\n","\n","\n","\"\"\")\n","\n","\n","def user_prompt_keywords(text):\n","   return f\"\"\"\n","          Du bist online Marketing Experte und arbeitest f√ºr ein erfolgreiches Unternehemen mit SEO.\n","          Bitte generiere SEO-Keywords f√ºr den folgenden Text:\n","\n","          {text}\n","\n","          Deine Aufgabe ist es, thematisch relevante, serialisierte SEO-Keywords zu erstellen, die sowohl Short-Tail als auch Long-Tail Keywords enthalten.\n","          Achte darauf, dass die Keywords Synonyme und verwandte Begriffe ber√ºcksichtigen und f√ºr Suchmaschinenoptimierung geeignet sind.\n","\n","          Strukturiere deine Antwort folgenderma√üen:\n","          Gib eine Liste von Keywords in python zur√ºck. Gebe sonst nichts zur√ºck, keine EInleitung, keine √úberschrift, keine Zusammenfassung,\n","          nichts ausser der Liste in python.\n","\n","\n","          Danke! Mein Job h√§ngt davon ab!\n","          \"\"\"\n"],"metadata":{"id":"Hjy6NgflvCTz","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["page_text_list = []\n","def prep_text_and_get_keywords():\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","    scraped_data = scraper.get_scraped_data()\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","    filtered_urls = []\n","\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, √ºberspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","\n","            # 3. SEO-Analyse starten (f√ºr gefilterte Seiten)\n","        for url in filtered_urls:\n","            # Die gesamte Seite analysieren\n","            page_text = scraped_data[url]\n","            page_text_list.append(page_text)\n","\n","\n","\n","    keyword_list =[]\n","    for text in page_text_list:\n","\n","      cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))\n","\n","      keyword_list.append(cb.chat())\n","\n","    return keyword_list\n"],"metadata":{"id":"cwzCp7r2z3Dp","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_raw = prep_text_and_get_keywords()"],"metadata":{"id":"4-zqgH9o0F7g","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und willst dem user helfen!\", f\"\"\"Sei ein online marketing spezialist mit einem Talent f√ºr SEO. Du analysierst Texte\n","und erstellst SEO-Keywords f√ºr ein international erfolgreiches Unternehmen.\n","Anlysiere diesen string und extrahiere die wesentlichen 20 SEO-keywords:\n","{keywords_raw} Gebe nur die SEO keywords als Liste zur√ºck, sonst nichts. Keine Einleitung, keine Zusammenfassung, nur die Liste wie zB [keywort_1, keywort_2]\n","\"\"\")\n","keywords_final = cb.chat()"],"metadata":{"id":"KuJZ1Vz-s-Rt","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üöß main SEO optimize"],"metadata":{"id":"CdWeu44YIs5k"}},{"cell_type":"code","source":["combined_analysis_list = []\n","filtered_urls = []\n","\n","def main():\n","    # 1. SCRAPING\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","\n","    # Alle gescrapten Daten abrufen\n","    scraped_data = scraper.get_scraped_data()\n","\n","    # 2. Sichten der Texte und Filtern\n","    #    Hier k√∂nntest du jetzt z. B. manuell pr√ºfen, welche URLs wichtig sind.\n","    #    Wir geben einfach mal alle URLs aus:\n","    print(\"\\n--- Gesammelte Seiten und Inhalte (gek√ºrzt) ---\")\n","    for url, text in scraped_data.items():\n","        print(f\"\\nURL: {url}\")\n","        # Beispiel: Nur ersten 200 Zeichen zeigen\n","        print(f\"Text: {text[:200]}...\")\n","\n","\n","\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, √ºberspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","        print(f\"send text to LLM:  {url}\")\n","\n","\n","\n","\n","\n","    # 3. SEO-Analyse starten (f√ºr gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","\n","        # 3.1 Chunken, um zu gro√üe Anfragen zu vermeiden\n","        text_chunks = chunk_text(page_text, max_tokens=10000)\n","\n","        print(f\"\\n=== Analyzing {url} ===\")\n","        all_analyses = []\n","        for i, chunk in enumerate(text_chunks):\n","            print(f\" - Sende Chunk {i+1}/{len(text_chunks)} an Chatbot ...\")\n","\n","            # Prompt definieren (SEO)\n","            system_prompt = \"Du bist ein hochqualifizierter SEO-Experte. Du arbeitest f√ºr erfolgreiche online marketing experten!\"\n","            user_prompt = (f\"\"\"\n","                1. Untersuche den folgenden Text auf Keyword-Optimierung, Lesbarkeit und m√∂gliche SEO-Verbesserungen. Achte auf die Stadt, um die Information zu verwenden!\n","                Wichtige SEO Keywords sind:\n","                {keywords_final}\n","\n","                Die Praxis liegt in Essen, im S√ºden in R√ºttenscheid. Der Name der Praxis ist 'R√ú Zahnspange'.\n","\n","\n","                2. Formuliere den Text entsprechend optimaler SEO Sichtbarkeit um. Der Tonfall soll weiterhin nett und freundlich sein und einen warmen, einf√ºhlsamen Eindruck machen.\n","                Wir wollen die Patientenzahl der Praxis steigern aber die Kommunikation darf nicht aufdringlich wirken.\n","                Es soll trotzdem weiterhin ein hoher medizinisch- fachlicher Standard gehalten werden und Professionalit√§t und Exzellenz soll vermittelt werden!\n","                Die Fragen und Antworten in den FAQs sollen ebenfalls optimiert werden. Es d√ºrfen keine Fragen oder sonstige Textabschnitte weggelassen werden! Wir wollen grunds√§tzlich den Text nicht substanzielle ver√§ndern,\n","                sondern eine bereits bestehende website optimieren. das bedeutet auch, dass die Anzahl der W√∂rter etwa gleich bleiben sollte, um das bestehende Format der website nicht zu sehr zu beeinflussen.\n","                Falls notwendig, f√ºge Meta-Titel und longtail keywords hinzu!\n","                3. Als Ausgabe gebe eine detaillierte, ausf√ºhrliche und umfassende Analyse des SEO Status des Textes aus, √úberschrift: Analyse. Gebe dann deine optimierte Version aus, √úberschrift: SEO. Schliesse ab mit detaillierten und ausf√ºhrlichen Erl√§uterungen, warum du die √Ñnderungen durchgef√ºhrt hast, √úberschrift: Erkl√§rung.\n","                Die √ºberschriften sind sehr wichtig und m√ºssen √ºber den Abschnitten stehen!\n","                Benutze keine Formatierungszeichen wie ###, # oder **! Mein Job h√§ngt davon ab!\n","                \"\"\"\n","                \"Hier ist der Text chunk: \\n\\n\"\n","                f\"{chunk}\"\n","            )\n","\n","            # ChatGPT aufrufen\n","            cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","            analysis = cb.chat_with_streaming()\n","            all_analyses.append(analysis)\n","\n","            # Warte kurz (Rate Limits, API-Kosten etc.)\n","            time.sleep(1)\n","\n","        # 3.2 Fertige Analyse (alle Chunks zusammen)\n","        combined_analysis = \"\\n\".join(all_analyses)\n","\n","\n","        combined_analysis_list.append(combined_analysis)\n","        # print(f\"\\n--- SEO-Analyse f√ºr {url} ---\")\n","        # print(combined_analysis)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"nF-cfM35dhxQ","executionInfo":{"status":"aborted","timestamp":1737483807973,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"F√ºr Kieferorthop√§den sind vor allem Keywords mit Ortsbezug (‚ÄûKieferorthop√§de + Stadt‚Äú) und behandlungsspezifische Begriffe (‚ÄûZahnspange‚Äú, ‚ÄûZahnfehlstellung‚Äú, ‚ÄûInvisalign‚Äú) entscheidend. Zus√§tzlich sollte man sich auf h√§ufige Fragen (Long-Tail-Keywords) konzentrieren und regelm√§√üige Fach- und Ratgeber-Artikel ver√∂ffentlichen, um auch in der organischen Suche besser gefunden zu werden.\""],"metadata":{"id":"_jtOwjSxJTNP","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"source":["import re\n","import json\n","\n","def extract_sections_to_json(texts, keys):\n","    \"\"\"Extrahiert Abschnitte aus mehreren Texten und konvertiert sie in JSON.\"\"\"\n","\n","    all_sections = []  # Liste f√ºr alle Abschnitte\n","\n","    for text in texts:\n","        pattern = r\"(Analyse|SEO|Erkl√§rung)\\n(.*?)(?=\\n(?:Analyse|SEO|Erkl√§rung|$))\"\n","        matches = re.findall(pattern, text, re.DOTALL | re.MULTILINE | re.UNICODE)\n","        sections_dict = {}\n","        for heading, content in matches:\n","            sections_dict[heading] = content.strip()\n","        all_sections.append(sections_dict)  # Abschnitte zur Liste hinzuf√ºgen\n","\n","    # Kombinieren der Abschnitte mit Keys\n","    final_json_data = {}  # Dictionary f√ºr das finale JSON\n","    for i, sections_dict in enumerate(all_sections):\n","        key = keys[i]  # Key aus der Liste holen\n","        final_json_data[key] = sections_dict  # Abschnitte zum Dictionary hinzuf√ºgen\n","\n","    json_data = json.dumps(final_json_data, indent=4, ensure_ascii=False)\n","    return json_data\n","\n","\n","keys = filtered_urls  # Keys f√ºr die Texte\n","\n","json_output = extract_sections_to_json(combined_analysis_list, keys)\n","print(json_output)"],"cell_type":"code","metadata":{"id":"aTZgOCUKe2_s","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthop√§din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\""],"metadata":{"id":"fH4mRdPXceeP","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install langchain faiss-cpu\n"],"metadata":{"id":"4DOE2XX9dmow","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -U langchain-community"],"metadata":{"id":"x-BBLqldKMce","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tiktoken"],"metadata":{"id":"-QDhcWZAKUsX","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import openai\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.docstore.document import Document\n","\n","# 0) Vector Index (FAISS) initialisieren\n","#    (Sp√§ter im Code k√∂nnen wir den Index persistent speichern/neu laden)\n","# os.environ[\"OPENAI_API_KEY\"] = \"DEIN_OPENAI_API_KEY\"\n","\n","embeddings = OpenAIEmbeddings()\n","\n","# Beispiel-Fehler als \"Dokument\" f√ºr den Vector Store\n","# \"page_content\" = Text, \"metadata\" = beliebige Zusatzinfos\n","known_error_text = \"\"\"\n","Fehler: \"Klaren Aligner\" wird f√§lschlicherweise als Eigenname verwendet,\n","         obwohl es grammatisch richtig \"klaren Alignern\" sein sollte.\n","\n","Richtige Anwendung:\n","- Sagen: \"Entdecken Sie die Vorteile von klaren Alignern.\"\n","- Oder: \"Klare Aligner sind die ...\"\n","\n","Zus√§tzliche Hinweise:\n","- Beim Eindeutschen englischer Fachbegriffe auf die Pluralbildung achten.\n","\"\"\"\n","\n","doc = Document(\n","    page_content=known_error_text,\n","    metadata={\"error_type\": \"grammar/de-english\", \"example_id\": \"klaren-aligner\"}\n",")\n","\n","# Vektorindex erzeugen und das \"bekannte Fehler\"-Dokument ablegen\n","vector_store = FAISS.from_documents([doc], embeddings)\n"],"metadata":{"id":"PHubUMS4Jirw","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.docstore.document import Document\n","\n","# 1. Neuer Fehler: \"Kauen- und Sprechprobleme\" statt \"Kau- und Sprechprobleme\"\n","doc1_text = \"\"\"\n","Fehler: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","Richtig: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kau- und Sprechprobleme effektiv behandeln.\"\n","\n","Grund:\n","- Falsche Rechtschreibung/Zusammensetzung bei \"Kauen-\".\n","- Richtig ist \"Kau- und Sprechprobleme\".\n","\"\"\"\n","\n","doc1 = Document(\n","    page_content=doc1_text,\n","    metadata={\n","        \"error_type\": \"grammar/spelling\",\n","        \"example_id\": \"kauen-sprechprobleme\"\n","    }\n",")\n","\n","# 2. Neuer Fehler: falsche Formulierung bei Sprachen\n","doc2_text = \"\"\"\n","Fehler: \"Als in Kenia geborene Kieferorthop√§din ... spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","Richtig: \"Als in Kenia geborene Kieferorthop√§din ... spricht neben Deutsch auch Englisch und Swahili und verf√ºgt √ºber Grundkenntnisse in Arabisch und Spanisch.\"\n","\n","Grund:\n","- Bessere Formulierung, um '√ºber Grundkenntnisse' mit 'verf√ºgt √ºber Grundkenntnisse' zu vereinen.\n","- Straffere und klarere Satzstruktur.\n","\"\"\"\n","\n","doc2 = Document(\n","    page_content=doc2_text,\n","    metadata={\n","        \"error_type\": \"grammar/style\",\n","        \"example_id\": \"languages-phrase\"\n","    }\n",")\n","\n","\n","# Angenommen, du hast bereits:\n","# embeddings = OpenAIEmbeddings()\n","# vector_store = FAISS.from_documents([some_initial_docs], embeddings)\n","#\n","# -> Dann f√ºgen wir jetzt doc1 und doc2 hinzu:\n","\n","vector_store.add_documents([doc1, doc2])\n"],"metadata":{"id":"ViQDjBE8WqAn","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u","executionInfo":{"status":"aborted","timestamp":1737483807974,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn","executionInfo":{"status":"aborted","timestamp":1737483807975,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def chunk_text_2(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    F√ºr echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in seo_optimized_texts:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text_2(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import TokenTextSplitter\n","\n","def chunk_text_langchain(text, max_tokens=500, overlap=50):\n","    \"\"\"\n","    Teilt den Text anhand der Tokenanzahl auf. Nutzt daf√ºr LangChain's TokenTextSplitter.\n","    - max_tokens: maximale Tokens pro Chunk\n","    - overlap: wie viele Tokens √úberschneidung zum vorherigen Chunk\n","    \"\"\"\n","    splitter = TokenTextSplitter(\n","        encoding_name=OpenAIEmbeddings(),  # oder passend zu deinem Modell (z.B. \"gpt-3.5-turbo\")\n","        chunk_size=max_tokens,         # maximale Anzahl Tokens pro Chunk\n","        chunk_overlap=overlap          # Tokens, die sich mit dem vorigen Chunk √ºberschneiden (Kontext)\n","    )\n","\n","    chunks = splitter.split_text(text)\n","    return chunks\n","\n","# Beispielanwendung:\n","# seo_text = \"\"\"Hier Dein langer Text, den du chunken willst ...\"\"\"\n","# chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","# print(chunked)\n","\n","\n","\n","chunked_texts = []\n","for seo_text in seo_optimized_texts:\n","    # Chunking pro SEO-Text\n","    chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","    chunked_texts.append(text_chunks)\n"],"metadata":{"id":"FspYaRcb_hOm","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunked_texts"],"metadata":{"id":"xt9uwzh5AjKy","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk √§hneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenf√ºgen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","# test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","# retrieved_context = get_context_from_vector_store(test_chunk)\n","# print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu pr√ºfen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu ber√ºcksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu pr√ºfenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu pr√ºfen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur Pr√ºfung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text gr√ºndlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu ver√§ndern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zus√§tzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein g√ºltiges JSON zur√ºckgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine g√ºltige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":17,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_corrected_texts = []\n","all_new_mistakes = []\n","\n","for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","    corrected_text_chunks = []\n","\n","    for chunk in text_chunks:\n","        # 3a) Kontext abfragen\n","        context = get_context_from_vector_store(chunk)\n","\n","\n","        # 4a) Prompt ChatGPT (Korrektur)\n","        result = proofread_text_with_context(chunk, context)\n","\n","        corrected_text = result[\"corrected_text\"]\n","        new_mistakes = result[\"new_mistakes_found\"]\n","\n","        # Sammeln\n","        corrected_text_chunks.append(corrected_text)\n","        all_new_mistakes.extend(new_mistakes)\n","\n","    # Pro SEO-Text f√ºgen wir die korrigierten Chunks zusammen.\n","    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _ in all_corrected_texts:\n","  print(_)"],"metadata":{"id":"qIUE9Dfl5YzS","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3","executionInfo":{"status":"aborted","timestamp":1737483807976,"user_tz":-60,"elapsed":16,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]}]}