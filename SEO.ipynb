{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["o38bsnD-EWcq","vNVT-lr5jejz","FA2RIeko9reT","E0k0HmUZHmkg","qGy_OuMyIC9C","qK7cW2yRWDnV","cgPAMHODtAuZ","CdWeu44YIs5k","BatTg4ZKvpZV","4v6-jGMeO9Zt","HraJ3_5SJHSv"],"authorship_tag":"ABX9TyOLftkFAPCN9ADQd+b0MVh1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 🏁 Install requirements"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["from google.colab import drive, userdata\n","notebook_folder = userdata.get('gdrive_seo_folder')\n","\n","drive.mount('/content/drive',\n","            force_remount=True\n","            )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXxs_PE-OzbK","executionInfo":{"status":"ok","timestamp":1739032845975,"user_tz":-60,"elapsed":4322,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"77fd29c8-82c2-43de-b160-12745da9e343"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["folder_path = \"/content/drive/MyDrive/\" + notebook_folder  # Beispiel für den Ordnernamen\n","\n","# Dynamisch den vollständigen Dateipfad erstellen\n","requirements_file = f\"{folder_path}/requirements.txt\""],"metadata":{"id":"OBGV2dN4LEgD","executionInfo":{"status":"ok","timestamp":1739032978173,"user_tz":-60,"elapsed":42,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# !apt-get install -y pandoc\n","!pip install -r '{requirements_file}'\n","!python -m spacy download de_core_news_sm\n","!playwright install"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjUp2rFaRqqz","executionInfo":{"status":"ok","timestamp":1739033035586,"user_tz":-60,"elapsed":54367,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"6e387c4f-9087-4d16-fb8e-61d23166ac68"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/sethblack/python-seo-analyzer.git (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1))\n","  Cloning https://github.com/sethblack/python-seo-analyzer.git to /tmp/pip-req-build-txiy6wk7\n","  Running command git clone --filter=blob:none --quiet https://github.com/sethblack/python-seo-analyzer.git /tmp/pip-req-build-txiy6wk7\n","  Resolved https://github.com/sethblack/python-seo-analyzer.git to commit cfb38f5b39803ec1dc597158c9226ecd9bc07511\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: langchain_anthropic in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.3.7)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 3)) (1.0.1)\n","Collecting pypandoc (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4))\n","  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n","Collecting playwright (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 5))\n","  Downloading playwright-1.50.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (4.13.3)\n","Requirement already satisfied: certifi>=2024.8.30 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2025.1.31)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (3.1.5)\n","Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (5.3.0)\n","Requirement already satisfied: markupsafe>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (3.0.2)\n","Requirement already satisfied: trafilatura>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.3.0)\n","Requirement already satisfied: anthropic<1,>=0.45.0 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.45.2)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.3.34)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.10.6)\n","Collecting pyee<13,>=12 (from playwright->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 5))\n","  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 5)) (3.1.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (4.12.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.6)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.3.5)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (24.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.27.2)\n","Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (3.4.1)\n","Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.3.2)\n","Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.9.3)\n","Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (3.0.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.17.0)\n","Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (0.13)\n","Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.2.1)\n","Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.9.0.post0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.45.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.10.15)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.23.0)\n","Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2025.1)\n","Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2024.11.6)\n","Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (5.2)\n","Requirement already satisfied: lxml-html-clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.17.0)\n","Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n","Downloading playwright-1.50.0-py3-none-manylinux1_x86_64.whl (45.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyee-12.1.1-py3-none-any.whl (15 kB)\n","Installing collected packages: pypandoc, pyee, playwright\n","Successfully installed playwright-1.50.0 pyee-12.1.1 pypandoc-1.15\n","Collecting de-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.1.31)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Downloading Chromium 133.0.6943.16 (playwright build v1155)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-linux.zip\u001b[22m\n","\u001b[1G163.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 0% 335.3s\u001b[0K\u001b[1G163.5 MiB [] 0% 336.9s\u001b[0K\u001b[1G163.5 MiB [] 0% 212.5s\u001b[0K\u001b[1G163.5 MiB [] 0% 195.6s\u001b[0K\u001b[1G163.5 MiB [] 0% 152.9s\u001b[0K\u001b[1G163.5 MiB [] 0% 116.3s\u001b[0K\u001b[1G163.5 MiB [] 0% 123.2s\u001b[0K\u001b[1G163.5 MiB [] 0% 98.6s\u001b[0K\u001b[1G163.5 MiB [] 0% 77.2s\u001b[0K\u001b[1G163.5 MiB [] 0% 73.5s\u001b[0K\u001b[1G163.5 MiB [] 0% 59.6s\u001b[0K\u001b[1G163.5 MiB [] 0% 47.4s\u001b[0K\u001b[1G163.5 MiB [] 0% 43.5s\u001b[0K\u001b[1G163.5 MiB [] 1% 35.0s\u001b[0K\u001b[1G163.5 MiB [] 1% 28.1s\u001b[0K\u001b[1G163.5 MiB [] 1% 25.1s\u001b[0K\u001b[1G163.5 MiB [] 2% 20.9s\u001b[0K\u001b[1G163.5 MiB [] 2% 17.4s\u001b[0K\u001b[1G163.5 MiB [] 3% 14.2s\u001b[0K\u001b[1G163.5 MiB [] 3% 14.0s\u001b[0K\u001b[1G163.5 MiB [] 4% 12.2s\u001b[0K\u001b[1G163.5 MiB [] 4% 10.5s\u001b[0K\u001b[1G163.5 MiB [] 5% 9.4s\u001b[0K\u001b[1G163.5 MiB [] 5% 9.7s\u001b[0K\u001b[1G163.5 MiB [] 5% 9.1s\u001b[0K\u001b[1G163.5 MiB [] 6% 8.9s\u001b[0K\u001b[1G163.5 MiB [] 6% 8.6s\u001b[0K\u001b[1G163.5 MiB [] 7% 8.4s\u001b[0K\u001b[1G163.5 MiB [] 7% 8.1s\u001b[0K\u001b[1G163.5 MiB [] 8% 7.5s\u001b[0K\u001b[1G163.5 MiB [] 8% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 9% 6.6s\u001b[0K\u001b[1G163.5 MiB [] 10% 6.4s\u001b[0K\u001b[1G163.5 MiB [] 10% 6.3s\u001b[0K\u001b[1G163.5 MiB [] 11% 6.1s\u001b[0K\u001b[1G163.5 MiB [] 12% 5.7s\u001b[0K\u001b[1G163.5 MiB [] 12% 5.5s\u001b[0K\u001b[1G163.5 MiB [] 13% 5.3s\u001b[0K\u001b[1G163.5 MiB [] 13% 5.4s\u001b[0K\u001b[1G163.5 MiB [] 14% 5.3s\u001b[0K\u001b[1G163.5 MiB [] 14% 5.2s\u001b[0K\u001b[1G163.5 MiB [] 15% 5.1s\u001b[0K\u001b[1G163.5 MiB [] 15% 4.9s\u001b[0K\u001b[1G163.5 MiB [] 16% 4.7s\u001b[0K\u001b[1G163.5 MiB [] 17% 4.6s\u001b[0K\u001b[1G163.5 MiB [] 17% 4.5s\u001b[0K\u001b[1G163.5 MiB [] 18% 4.5s\u001b[0K\u001b[1G163.5 MiB [] 18% 4.3s\u001b[0K\u001b[1G163.5 MiB [] 19% 4.2s\u001b[0K\u001b[1G163.5 MiB [] 19% 4.1s\u001b[0K\u001b[1G163.5 MiB [] 20% 4.1s\u001b[0K\u001b[1G163.5 MiB [] 21% 4.0s\u001b[0K\u001b[1G163.5 MiB [] 21% 3.9s\u001b[0K\u001b[1G163.5 MiB [] 22% 3.8s\u001b[0K\u001b[1G163.5 MiB [] 23% 3.7s\u001b[0K\u001b[1G163.5 MiB [] 24% 3.6s\u001b[0K\u001b[1G163.5 MiB [] 24% 3.5s\u001b[0K\u001b[1G163.5 MiB [] 25% 3.5s\u001b[0K\u001b[1G163.5 MiB [] 26% 3.4s\u001b[0K\u001b[1G163.5 MiB [] 27% 3.3s\u001b[0K\u001b[1G163.5 MiB [] 28% 3.3s\u001b[0K\u001b[1G163.5 MiB [] 29% 3.2s\u001b[0K\u001b[1G163.5 MiB [] 30% 3.1s\u001b[0K\u001b[1G163.5 MiB [] 31% 3.0s\u001b[0K\u001b[1G163.5 MiB [] 32% 2.9s\u001b[0K\u001b[1G163.5 MiB [] 32% 3.0s\u001b[0K\u001b[1G163.5 MiB [] 33% 2.9s\u001b[0K\u001b[1G163.5 MiB [] 34% 2.8s\u001b[0K\u001b[1G163.5 MiB [] 35% 2.7s\u001b[0K\u001b[1G163.5 MiB [] 36% 2.7s\u001b[0K\u001b[1G163.5 MiB [] 36% 2.6s\u001b[0K\u001b[1G163.5 MiB [] 37% 2.6s\u001b[0K\u001b[1G163.5 MiB [] 38% 2.5s\u001b[0K\u001b[1G163.5 MiB [] 40% 2.4s\u001b[0K\u001b[1G163.5 MiB [] 41% 2.4s\u001b[0K\u001b[1G163.5 MiB [] 42% 2.3s\u001b[0K\u001b[1G163.5 MiB [] 43% 2.2s\u001b[0K\u001b[1G163.5 MiB [] 44% 2.2s\u001b[0K\u001b[1G163.5 MiB [] 45% 2.1s\u001b[0K\u001b[1G163.5 MiB [] 46% 2.1s\u001b[0K\u001b[1G163.5 MiB [] 47% 2.0s\u001b[0K\u001b[1G163.5 MiB [] 48% 2.0s\u001b[0K\u001b[1G163.5 MiB [] 49% 1.9s\u001b[0K\u001b[1G163.5 MiB [] 50% 1.9s\u001b[0K\u001b[1G163.5 MiB [] 51% 1.9s\u001b[0K\u001b[1G163.5 MiB [] 51% 1.8s\u001b[0K\u001b[1G163.5 MiB [] 52% 1.8s\u001b[0K\u001b[1G163.5 MiB [] 53% 1.8s\u001b[0K\u001b[1G163.5 MiB [] 54% 1.7s\u001b[0K\u001b[1G163.5 MiB [] 55% 1.7s\u001b[0K\u001b[1G163.5 MiB [] 56% 1.6s\u001b[0K\u001b[1G163.5 MiB [] 57% 1.6s\u001b[0K\u001b[1G163.5 MiB [] 58% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 59% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 60% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 60% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 61% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 62% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 63% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 64% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 65% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 66% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 67% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 68% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 69% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 70% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 71% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 72% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 73% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 74% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 75% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 76% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 77% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 78% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 78% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 79% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 80% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 81% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 81% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 82% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 83% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 84% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 85% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 86% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 87% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 88% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 89% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 90% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 91% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 92% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 92% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 93% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 94% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 95% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 97% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 98% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 100% 0.0s\u001b[0K\n","Chromium 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium-1155\n","Downloading Chromium Headless Shell 133.0.6943.16 (playwright build v1155)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-headless-shell-linux.zip\u001b[22m\n","\u001b[1G99.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 0% 201.4s\u001b[0K\u001b[1G99.9 MiB [] 0% 204.5s\u001b[0K\u001b[1G99.9 MiB [] 0% 135.1s\u001b[0K\u001b[1G99.9 MiB [] 0% 123.9s\u001b[0K\u001b[1G99.9 MiB [] 0% 96.6s\u001b[0K\u001b[1G99.9 MiB [] 0% 73.6s\u001b[0K\u001b[1G99.9 MiB [] 0% 77.7s\u001b[0K\u001b[1G99.9 MiB [] 0% 62.1s\u001b[0K\u001b[1G99.9 MiB [] 0% 49.8s\u001b[0K\u001b[1G99.9 MiB [] 0% 46.3s\u001b[0K\u001b[1G99.9 MiB [] 0% 36.9s\u001b[0K\u001b[1G99.9 MiB [] 1% 29.5s\u001b[0K\u001b[1G99.9 MiB [] 1% 27.6s\u001b[0K\u001b[1G99.9 MiB [] 1% 21.7s\u001b[0K\u001b[1G99.9 MiB [] 2% 17.4s\u001b[0K\u001b[1G99.9 MiB [] 2% 15.9s\u001b[0K\u001b[1G99.9 MiB [] 3% 12.6s\u001b[0K\u001b[1G99.9 MiB [] 4% 10.0s\u001b[0K\u001b[1G99.9 MiB [] 5% 8.9s\u001b[0K\u001b[1G99.9 MiB [] 6% 7.7s\u001b[0K\u001b[1G99.9 MiB [] 7% 6.8s\u001b[0K\u001b[1G99.9 MiB [] 8% 6.1s\u001b[0K\u001b[1G99.9 MiB [] 8% 5.9s\u001b[0K\u001b[1G99.9 MiB [] 10% 5.2s\u001b[0K\u001b[1G99.9 MiB [] 10% 5.1s\u001b[0K\u001b[1G99.9 MiB [] 12% 4.5s\u001b[0K\u001b[1G99.9 MiB [] 13% 4.2s\u001b[0K\u001b[1G99.9 MiB [] 14% 3.9s\u001b[0K\u001b[1G99.9 MiB [] 15% 3.9s\u001b[0K\u001b[1G99.9 MiB [] 16% 3.6s\u001b[0K\u001b[1G99.9 MiB [] 17% 3.4s\u001b[0K\u001b[1G99.9 MiB [] 18% 3.3s\u001b[0K\u001b[1G99.9 MiB [] 20% 3.1s\u001b[0K\u001b[1G99.9 MiB [] 21% 3.0s\u001b[0K\u001b[1G99.9 MiB [] 21% 2.9s\u001b[0K\u001b[1G99.9 MiB [] 22% 2.9s\u001b[0K\u001b[1G99.9 MiB [] 23% 2.9s\u001b[0K\u001b[1G99.9 MiB [] 24% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 25% 2.6s\u001b[0K\u001b[1G99.9 MiB [] 27% 2.5s\u001b[0K\u001b[1G99.9 MiB [] 29% 2.3s\u001b[0K\u001b[1G99.9 MiB [] 30% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 31% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 32% 2.1s\u001b[0K\u001b[1G99.9 MiB [] 34% 2.0s\u001b[0K\u001b[1G99.9 MiB [] 35% 2.0s\u001b[0K\u001b[1G99.9 MiB [] 36% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 37% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 38% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 40% 1.7s\u001b[0K\u001b[1G99.9 MiB [] 41% 1.7s\u001b[0K\u001b[1G99.9 MiB [] 42% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 43% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 44% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 45% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 46% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 47% 1.4s\u001b[0K\u001b[1G99.9 MiB [] 49% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 50% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 51% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 52% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 53% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 54% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 55% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 57% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 58% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 59% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 60% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 61% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 62% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 63% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 64% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 65% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 66% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 67% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 69% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 70% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 71% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 72% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 73% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 75% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 76% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 77% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 78% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 79% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 80% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 81% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 82% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 83% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 84% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 85% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 86% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 87% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 88% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 89% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 90% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 91% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 93% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 97% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 100% 0.0s\u001b[0K\n","Chromium Headless Shell 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1155\n","Downloading Firefox 134.0 (playwright build v1471)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1471/firefox-ubuntu-22.04.zip\u001b[22m\n","\u001b[1G88.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 206.5s\u001b[0K\u001b[1G88.7 MiB [] 0% 173.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 115.3s\u001b[0K\u001b[1G88.7 MiB [] 0% 105.5s\u001b[0K\u001b[1G88.7 MiB [] 0% 82.1s\u001b[0K\u001b[1G88.7 MiB [] 0% 62.4s\u001b[0K\u001b[1G88.7 MiB [] 0% 66.5s\u001b[0K\u001b[1G88.7 MiB [] 0% 53.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 42.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 40.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 32.6s\u001b[0K\u001b[1G88.7 MiB [] 1% 25.8s\u001b[0K\u001b[1G88.7 MiB [] 1% 23.8s\u001b[0K\u001b[1G88.7 MiB [] 1% 20.1s\u001b[0K\u001b[1G88.7 MiB [] 2% 15.3s\u001b[0K\u001b[1G88.7 MiB [] 3% 13.7s\u001b[0K\u001b[1G88.7 MiB [] 3% 11.2s\u001b[0K\u001b[1G88.7 MiB [] 4% 9.4s\u001b[0K\u001b[1G88.7 MiB [] 5% 7.6s\u001b[0K\u001b[1G88.7 MiB [] 6% 7.5s\u001b[0K\u001b[1G88.7 MiB [] 7% 6.7s\u001b[0K\u001b[1G88.7 MiB [] 8% 6.0s\u001b[0K\u001b[1G88.7 MiB [] 8% 5.7s\u001b[0K\u001b[1G88.7 MiB [] 9% 5.4s\u001b[0K\u001b[1G88.7 MiB [] 10% 4.9s\u001b[0K\u001b[1G88.7 MiB [] 11% 4.5s\u001b[0K\u001b[1G88.7 MiB [] 12% 4.6s\u001b[0K\u001b[1G88.7 MiB [] 12% 4.4s\u001b[0K\u001b[1G88.7 MiB [] 13% 4.2s\u001b[0K\u001b[1G88.7 MiB [] 14% 4.1s\u001b[0K\u001b[1G88.7 MiB [] 14% 4.0s\u001b[0K\u001b[1G88.7 MiB [] 15% 4.0s\u001b[0K\u001b[1G88.7 MiB [] 16% 3.9s\u001b[0K\u001b[1G88.7 MiB [] 17% 3.6s\u001b[0K\u001b[1G88.7 MiB [] 17% 3.5s\u001b[0K\u001b[1G88.7 MiB [] 18% 3.4s\u001b[0K\u001b[1G88.7 MiB [] 19% 3.4s\u001b[0K\u001b[1G88.7 MiB [] 19% 3.3s\u001b[0K\u001b[1G88.7 MiB [] 20% 3.3s\u001b[0K\u001b[1G88.7 MiB [] 21% 3.2s\u001b[0K\u001b[1G88.7 MiB [] 22% 3.0s\u001b[0K\u001b[1G88.7 MiB [] 23% 2.9s\u001b[0K\u001b[1G88.7 MiB [] 24% 2.8s\u001b[0K\u001b[1G88.7 MiB [] 25% 2.7s\u001b[0K\u001b[1G88.7 MiB [] 26% 2.7s\u001b[0K\u001b[1G88.7 MiB [] 26% 2.6s\u001b[0K\u001b[1G88.7 MiB [] 28% 2.5s\u001b[0K\u001b[1G88.7 MiB [] 29% 2.4s\u001b[0K\u001b[1G88.7 MiB [] 30% 2.3s\u001b[0K\u001b[1G88.7 MiB [] 31% 2.2s\u001b[0K\u001b[1G88.7 MiB [] 32% 2.2s\u001b[0K\u001b[1G88.7 MiB [] 33% 2.1s\u001b[0K\u001b[1G88.7 MiB [] 35% 2.0s\u001b[0K\u001b[1G88.7 MiB [] 36% 1.9s\u001b[0K\u001b[1G88.7 MiB [] 37% 1.9s\u001b[0K\u001b[1G88.7 MiB [] 39% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 40% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 41% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 42% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 43% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 44% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 45% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 47% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 48% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 49% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 50% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 52% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 54% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 55% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 56% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 57% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 59% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 60% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 62% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 64% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 65% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 67% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 68% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 69% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 70% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 71% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 73% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 74% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 75% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 77% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 78% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 79% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 81% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 82% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 83% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 84% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 85% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 86% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 89% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 100% 0.0s\u001b[0K\n","Firefox 134.0 (playwright build v1471) downloaded to /root/.cache/ms-playwright/firefox-1471\n","Downloading Webkit 18.2 (playwright build v2123)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2123/webkit-ubuntu-22.04.zip\u001b[22m\n","\u001b[1G89.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 0% 176.9s\u001b[0K\u001b[1G89.2 MiB [] 0% 179.7s\u001b[0K\u001b[1G89.2 MiB [] 0% 118.9s\u001b[0K\u001b[1G89.2 MiB [] 0% 110.8s\u001b[0K\u001b[1G89.2 MiB [] 0% 86.4s\u001b[0K\u001b[1G89.2 MiB [] 0% 68.2s\u001b[0K\u001b[1G89.2 MiB [] 0% 69.6s\u001b[0K\u001b[1G89.2 MiB [] 0% 53.7s\u001b[0K\u001b[1G89.2 MiB [] 0% 43.2s\u001b[0K\u001b[1G89.2 MiB [] 0% 41.1s\u001b[0K\u001b[1G89.2 MiB [] 0% 33.2s\u001b[0K\u001b[1G89.2 MiB [] 1% 26.7s\u001b[0K\u001b[1G89.2 MiB [] 1% 24.5s\u001b[0K\u001b[1G89.2 MiB [] 1% 19.7s\u001b[0K\u001b[1G89.2 MiB [] 2% 15.9s\u001b[0K\u001b[1G89.2 MiB [] 2% 14.7s\u001b[0K\u001b[1G89.2 MiB [] 3% 14.0s\u001b[0K\u001b[1G89.2 MiB [] 3% 11.3s\u001b[0K\u001b[1G89.2 MiB [] 4% 10.0s\u001b[0K\u001b[1G89.2 MiB [] 5% 7.9s\u001b[0K\u001b[1G89.2 MiB [] 6% 7.1s\u001b[0K\u001b[1G89.2 MiB [] 8% 5.6s\u001b[0K\u001b[1G89.2 MiB [] 10% 5.0s\u001b[0K\u001b[1G89.2 MiB [] 11% 4.6s\u001b[0K\u001b[1G89.2 MiB [] 11% 4.5s\u001b[0K\u001b[1G89.2 MiB [] 13% 4.0s\u001b[0K\u001b[1G89.2 MiB [] 14% 3.7s\u001b[0K\u001b[1G89.2 MiB [] 16% 3.4s\u001b[0K\u001b[1G89.2 MiB [] 16% 3.3s\u001b[0K\u001b[1G89.2 MiB [] 17% 3.2s\u001b[0K\u001b[1G89.2 MiB [] 18% 3.0s\u001b[0K\u001b[1G89.2 MiB [] 20% 2.9s\u001b[0K\u001b[1G89.2 MiB [] 21% 2.7s\u001b[0K\u001b[1G89.2 MiB [] 22% 2.6s\u001b[0K\u001b[1G89.2 MiB [] 23% 2.5s\u001b[0K\u001b[1G89.2 MiB [] 25% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 25% 2.4s\u001b[0K\u001b[1G89.2 MiB [] 27% 2.2s\u001b[0K\u001b[1G89.2 MiB [] 28% 2.1s\u001b[0K\u001b[1G89.2 MiB [] 29% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 31% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 31% 2.1s\u001b[0K\u001b[1G89.2 MiB [] 33% 1.9s\u001b[0K\u001b[1G89.2 MiB [] 36% 1.8s\u001b[0K\u001b[1G89.2 MiB [] 38% 1.7s\u001b[0K\u001b[1G89.2 MiB [] 39% 1.7s\u001b[0K\u001b[1G89.2 MiB [] 40% 1.6s\u001b[0K\u001b[1G89.2 MiB [] 42% 1.5s\u001b[0K\u001b[1G89.2 MiB [] 43% 1.5s\u001b[0K\u001b[1G89.2 MiB [] 45% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 46% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 47% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 49% 1.3s\u001b[0K\u001b[1G89.2 MiB [] 51% 1.2s\u001b[0K\u001b[1G89.2 MiB [] 53% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 55% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 57% 1.0s\u001b[0K\u001b[1G89.2 MiB [] 59% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 60% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 62% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 64% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 66% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 68% 0.7s\u001b[0K\u001b[1G89.2 MiB [] 69% 0.7s\u001b[0K\u001b[1G89.2 MiB [] 70% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 72% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 73% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 75% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 76% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 77% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 79% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 80% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 82% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 83% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 85% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 87% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 88% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 89% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 90% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 91% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 93% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 94% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 95% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 96% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 97% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 99% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 100% 0.0s\u001b[0K\n","Webkit 18.2 (playwright build v2123) downloaded to /root/.cache/ms-playwright/webkit-2123\n","Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n","\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 1% 3.9s\u001b[0K\u001b[1G2.3 MiB [] 2% 4.1s\u001b[0K\u001b[1G2.3 MiB [] 4% 2.7s\u001b[0K\u001b[1G2.3 MiB [] 6% 2.4s\u001b[0K\u001b[1G2.3 MiB [] 9% 1.8s\u001b[0K\u001b[1G2.3 MiB [] 13% 1.4s\u001b[0K\u001b[1G2.3 MiB [] 14% 1.4s\u001b[0K\u001b[1G2.3 MiB [] 19% 1.1s\u001b[0K\u001b[1G2.3 MiB [] 25% 0.8s\u001b[0K\u001b[1G2.3 MiB [] 29% 0.7s\u001b[0K\u001b[1G2.3 MiB [] 39% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 52% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 59% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 79% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n","FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n","Playwright Host validation warning: \n","╔══════════════════════════════════════════════════════╗\n","║ Host system is missing dependencies to run browsers. ║\n","║ Missing libraries:                                   ║\n","║     libgtk-4.so.1                                    ║\n","║     libgraphene-1.0.so.0                             ║\n","║     libwoff2dec.so.1.0.2                             ║\n","║     libgstgl-1.0.so.0                                ║\n","║     libgstcodecparsers-1.0.so.0                      ║\n","║     libavif.so.13                                    ║\n","║     libharfbuzz-icu.so.0                             ║\n","║     libenchant-2.so.2                                ║\n","║     libsecret-1.so.0                                 ║\n","║     libhyphen.so.0                                   ║\n","║     libmanette-0.2.so.0                              ║\n","╚══════════════════════════════════════════════════════╝\n","    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n","    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:865:43)\n","    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:963:7)\n","    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:952:43)\n","    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"]}]},{"cell_type":"markdown","source":["# 🖼frameworks"],"metadata":{"id":"Ew8QRBDOSq9H"}},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, Comment\n","from urllib.parse import urljoin, urlparse\n","import chardet\n","\n","import dotenv\n","from pyseoanalyzer import analyze\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import openai\n","import time\n","\n","import re\n","import json\n","\n","from collections import Counter\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import ast\n","\n","import numpy as np\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","import string\n","\n","import seaborn as sns\n","\n","from wordcloud import WordCloud\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from jinja2 import Template\n","\n","import asyncio\n","from playwright.async_api import async_playwright\n","\n","import pypandoc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"SjcMjba1T1Cc","executionInfo":{"status":"error","timestamp":1739032876827,"user_tz":-60,"elapsed":9882,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"85d67ef2-01f5-4381-83ce-69abef829ab9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'playwright'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-2a9ce4a57ae8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplaywright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masync_playwright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpypandoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'playwright'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["# ⛩ push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["#%%capture\n","\n","\n","\n","\n","notebookname = 'SEO.ipynb'\n","\n","class github:\n","    def __init__(self, github_pat, github_email, github_username, github_repo, gdrive_notebook_folder, notebook_name):\n","        self.github_pat = userdata.get(github_pat)\n","        self.github_email = userdata.get(github_email)\n","        self.github_username = userdata.get(github_username)\n","        self.github_repo = userdata.get(github_repo)\n","        self.gdrive_notebook_folder = userdata.get(gdrive_notebook_folder)\n","        self.notebook_name = notebook_name\n","\n","    def clone_repo(self):\n","        # Source file path in Google Drive\n","        source_file_path = f\"/content/drive/MyDrive/{self.gdrive_notebook_folder}/{self.notebook_name}\"\n","\n","        # Repository details\n","        repo_url = f'https://{self.github_pat}@github.com/{self.github_username}/{self.github_repo}.git'\n","\n","        # Clone the private repository\n","        !git clone {repo_url} cloned-repo\n","        os.chdir('cloned-repo')  # Switch to the cloned repository\n","\n","        # Ensure the file exists in Google Drive\n","        if os.path.exists(source_file_path):\n","            # Copy the notebook into the cloned repository\n","            !cp \"{source_file_path}\" ./\n","        else:\n","            print(f\"The file {source_file_path} was not found.\")\n","            return  # Exit if the file doesn't exist\n","\n","        # Git configuration\n","        !git config user.email \"{self.github_email}\"\n","        !git config user.name \"{self.github_username}\"\n","\n","        # Add the file to Git\n","        !git add \"{self.notebook_name}\"\n","\n","        # Commit the changes\n","        !git commit -m \"Added {self.notebook_name} from Google Drive\"\n","\n","        # Push to the repository\n","        !git push origin main\n","\n","        # Wechsle zurück ins übergeordnete Verzeichnis und lösche cloned-repo\n","        os.chdir('..')\n","        !rm -rf cloned-repo\n","        print(\"cloned-repo wurde wieder gelöscht.\")\n","\n","\n","\n","# Clone, add, and push the notebook\n","clone_2 = github('github_pat', 'github_email', 'github_username', 'github_repo_seo', 'gdrive_seo_folder', notebookname)\n","clone_2.clone_repo()\n"],"metadata":{"id":"p-LWoIwDfU3Z","executionInfo":{"status":"aborted","timestamp":1739032876922,"user_tz":-60,"elapsed":91,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🕸 scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["class WebsiteScraper:\n","    \"\"\"\n","    Diese Klasse kümmert sich ausschließlich um das Sammeln und Extrahieren\n","    von Texten aus einer Website.\n","    \"\"\"\n","\n","    def __init__(self, start_url=\"https://www.rue-zahnspange.de\", max_pages=50):\n","        \"\"\"\n","        :param start_url: Die Start-URL der Website, z.B. \"https://www.example.com\"\n","        :param max_pages: Maximale Anzahl Seiten, die gecrawlt werden.\n","        \"\"\"\n","        self.start_url = start_url\n","        self.max_pages = max_pages\n","\n","        # Hier speichern wir {URL: reiner_Text}\n","        self.scraped_data = {}\n","\n","    def scrape_website(self):\n","        \"\"\"\n","        Startet den Crawl-Vorgang, gefolgt von der Extraktion des Textes\n","        und dem Sammeln interner Links.\n","        \"\"\"\n","        visited = set()\n","        to_visit = [self.start_url]\n","        domain = urlparse(self.start_url).netloc\n","\n","        while to_visit and len(visited) < self.max_pages:\n","            url = to_visit.pop(0)\n","            if url in visited:\n","                continue\n","            visited.add(url)\n","\n","            try:\n","                response = requests.get(url, timeout=10)\n","\n","                # Rohdaten holen und Encoding per chardet bestimmen\n","                raw_data = response.content\n","                detected = chardet.detect(raw_data)\n","                # Wenn chardet etwas erkennt, nehmen wir das. Sonst Standard \"utf-8\".\n","                encoding = \"utf-8\"\n","                text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","                # Nur weiterverarbeiten, wenn HTML-Content\n","                if (response.status_code == 200\n","                    and \"text/html\" in response.headers.get(\"Content-Type\", \"\")):\n","                    soup = BeautifulSoup(text_data, \"html.parser\")\n","\n","                    # Text extrahieren\n","                    text = self._extract_text_from_soup(soup)\n","                    self.scraped_data[url] = text\n","\n","                    # Interne Links sammeln\n","                    for link in soup.find_all(\"a\", href=True):\n","                        absolute_link = urljoin(url, link[\"href\"])\n","                        if urlparse(absolute_link).netloc == domain:\n","                            if (absolute_link not in visited\n","                                and absolute_link not in to_visit):\n","                                to_visit.append(absolute_link)\n","\n","            except requests.RequestException as e:\n","                print(f\"Fehler beim Abrufen von {url}:\\n{e}\")\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        aber NICHT die, die in .faq4_question oder .faq4_answer stecken.\n","        Außerdem extrahiert er separat die FAQ-Fragen und -Antworten\n","        (faq4_question / faq4_answer), damit wir beide Zeilenumbrüche\n","        dort ebenfalls erhalten.\n","        \"\"\"\n","\n","        # 1) Script/Style/Noscript entfernen\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        # 2) Kommentare entfernen\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        # 3) Normale Texte (p, h1, h2, h3, li), ABER nicht innerhalb von .faq4_question / .faq4_answer\n","        texts = []\n","        all_normal_tags = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])\n","        for tag in all_normal_tags:\n","            # Prüfen, ob das Tag einen Vorfahren hat mit Klasse faq4_question oder faq4_answer\n","            if tag.find_parent(class_=\"faq4_question\") or tag.find_parent(class_=\"faq4_answer\"):\n","                continue\n","\n","            # Hier wichtig: separator=\"\\n\", strip=False, damit wir Zeilenumbrüche behalten\n","            txt = tag.get_text(separator=\"\\n\", strip=False)\n","            # Evtl. willst du doppelte Leerzeilen bereinigen. Das kannst du optional tun.\n","            if txt.strip():\n","                texts.append(txt.strip(\"\\r\\n\"))\n","\n","        # 4) FAQ-Bereiche (Fragen + Antworten)\n","        questions = soup.select(\".faq4_question\")\n","        answers = soup.select(\".faq4_answer\")\n","\n","        # 5) Zusammenführen (Frage + Antwort)\n","        for q, a in zip(questions, answers):\n","            q_text = q.get_text(separator=\"\\n\", strip=False)\n","            a_text = a.get_text(separator=\"\\n\", strip=False)\n","            q_text = q_text.strip(\"\\r\\n\")\n","            a_text = a_text.strip(\"\\r\\n\")\n","            if q_text and a_text:\n","                combined = f\"Frage: {q_text}\\nAntwort: {a_text}\"\n","                texts.append(combined)\n","\n","        # 6) Als String zurückgeben. Wir trennen die einzelnen Elemente durch \"\\n\\n\"\n","        #    (kannst du je nach Wunsch anpassen)\n","        return \"\\n\\n\".join(texts)\n","\n","    def get_scraped_data(self):\n","        \"\"\"\n","        Gibt das Dictionary {URL: Text} zurück.\n","        Du kannst damit arbeiten, Seiten filtern, etc.\n","        \"\"\"\n","        return self.scraped_data\n"],"metadata":{"id":"lCvWJLpBajRY","executionInfo":{"status":"aborted","timestamp":1739032876924,"user_tz":-60,"elapsed":7,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📊SEO Analysis 1"],"metadata":{"id":"FA2RIeko9reT"}},{"cell_type":"code","source":["\n","\n","\n","url = \"https://www.rue-zahnspange.de\"\n","report = analyze(url)\n","\n","# 'report' enthält nun sämtliche Analyseergebnisse\n","print(report)\n"],"metadata":{"id":"vK6jH0qmasx0","executionInfo":{"status":"aborted","timestamp":1739032876926,"user_tz":-60,"elapsed":8,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-xmnF1Yyphji","executionInfo":{"status":"aborted","timestamp":1739032876927,"user_tz":-60,"elapsed":7,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oAeeJEjYphb1","executionInfo":{"status":"aborted","timestamp":1739032876928,"user_tz":-60,"elapsed":7,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4VkvKAOfphUi","executionInfo":{"status":"aborted","timestamp":1739032876930,"user_tz":-60,"elapsed":35762,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tatsächlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Sortieren nach Häufigkeit absteigend\n","df_keywords.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Nur die Top 10 Keywords anzeigen\n","df_top10 = df_keywords.head(44)\n","\n","# Einfaches Balkendiagramm\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_top10['word'], df_top10[\"count\"], color=\"skyblue\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.title(\"Top 10 Keywords\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"Häufigkeit\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"nW_LgHd1Wtie","executionInfo":{"status":"aborted","timestamp":1739032876932,"user_tz":-60,"elapsed":35758,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(list(df_keywords['word']))"],"metadata":{"id":"1WWQvbUIPIwd","executionInfo":{"status":"aborted","timestamp":1739032876934,"user_tz":-60,"elapsed":35756,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ci_GtO9qPIt9","executionInfo":{"status":"aborted","timestamp":1739032876935,"user_tz":-60,"elapsed":35751,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N5oTB61wPIrj","executionInfo":{"status":"aborted","timestamp":1739032876966,"user_tz":-60,"elapsed":1,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VcE0WgKKPIo5","executionInfo":{"status":"aborted","timestamp":1739032876973,"user_tz":-60,"elapsed":5,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tatsächlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Neue Spalte mit Kleinbuchstaben erzeugen\n","df_keywords[\"word_lower\"] = df_keywords[\"word\"].str.lower()\n","\n","# Die Keywords, nach denen du suchen möchtest (Case-insensitive)\n","interesting_keywords = [\"zahnspange\", \"Invisalign\", \"kieferorthopädie\", \"BEHANDLUNG\", \"Kosten\" , \"zähne\", \"brackets\", \"Unsichtbar\", \"Kinder\", \"Jugendliche\", \"Patienten\", \"lächeln\", \"zeitnahen\"]\n","# Auch diese in Kleinbuchstaben umwandeln\n","interesting_keywords_lower = [kw.lower() for kw in interesting_keywords]\n","\n","# DataFrame nach den gewünschten Keywords filtern (Case-insensitive)\n","df_subset = df_keywords.loc[df_keywords[\"word_lower\"].isin(interesting_keywords_lower)].copy()\n","\n","# Sortieren nach Häufigkeit, damit das Diagramm übersichtlicher wird\n","df_subset.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Balkendiagramm\n","plt.figure(figsize=(8, 5))\n","# Plotten kannst du z.B. weiterhin den Originalwert \"word\" (falls du im Diagramm\n","# die ursprüngliche Schreibweise sehen willst)\n","plt.bar(df_subset[\"word\"], df_subset[\"count\"], color=\"steelblue\")\n","plt.title(\"Häufigkeit ausgewählter Keywords (Case-Insensitive)\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"Anzahl\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"p3q7KVe8j5Gx","executionInfo":{"status":"aborted","timestamp":1739032876978,"user_tz":-60,"elapsed":35778,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pages = report.get(\"pages\", [])\n","\n","df_pages = pd.DataFrame(pages)\n","\n","# Beispiel: Title-Länge berechnen\n","df_pages[\"title_length\"] = df_pages[\"title\"].apply(lambda x: len(x) if isinstance(x, str) else 0)\n","\n","# Balkendiagramm Title-Länge\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","plt.xticks(rotation=90)\n","plt.title(\"Länge der Title-Tags pro Seite\")\n","plt.xlabel(\"URL\")\n","plt.ylabel(\"Title-Länge (Zeichen)\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"bH80vXrlj5D5","executionInfo":{"status":"aborted","timestamp":1739032876980,"user_tz":-60,"elapsed":35774,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1 Zeile, 2 Spalten\n","\n","# (a) Keywords\n","axes[0].bar(df_top10[\"word\"].head(22), df_top10[\"count\"].head(22), color=\"skyblue\")\n","axes[0].set_title(\"Top 10 Keywords\")\n","axes[0].set_xticklabels(df_top10[\"word\"], rotation=45, ha=\"right\")\n","\n","# (b) Title-Längen\n","axes[1].bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","axes[1].set_title(\"Title-Länge pro Seite\")\n","axes[1].set_xticklabels(df_pages[\"url\"], rotation=90)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"WGcvHhIAj5BO","executionInfo":{"status":"aborted","timestamp":1739032877063,"user_tz":-60,"elapsed":35852,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I94K8prmj4-Z","executionInfo":{"status":"aborted","timestamp":1739032877064,"user_tz":-60,"elapsed":35847,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCO3w_TOj47i","executionInfo":{"status":"aborted","timestamp":1739032877071,"user_tz":-60,"elapsed":35849,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🤖 chatbot"],"metadata":{"id":"E0k0HmUZHmkg"}},{"cell_type":"code","source":["\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","class Chatbot:\n","    \"\"\"\n","    Diese Chatbot-Klasse nutzt die neue Methode client.chat.completions.create()\n","    aus openai>=1.0.0 über openai.OpenAI().\n","    \"\"\"\n","\n","    def __init__(self, systemprompt, prompt):\n","        self.client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n","        self.systemprompt = systemprompt\n","        self.prompt = prompt\n","        self.context = [{\"role\": \"system\", \"content\": systemprompt}]\n","        self.model = \"gpt-4o-mini-2024-07-18\"\n","\n","    def chat(self):\n","        \"\"\"\n","        Sendet den Prompt an das Chat-Interface und gibt den kompletten Antwort-String zurück.\n","        \"\"\"\n","        self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model,\n","                messages=self.context\n","            )\n","            response_content = response.choices[0].message.content\n","            self.context.append({\"role\": \"assistant\", \"content\": response_content})\n","            return response_content\n","        except Exception as e:\n","            print(f\"Fehler bei der OpenAI-Anfrage: {e}\")\n","            return \"\"\n","\n","\n","    def chat_with_streaming(self):\n","            \"\"\"\n","            Interagiert mit OpenAI Chat Completion API und streamt die Antwort.\n","            \"\"\"\n","            # Nachricht zur Konversation hinzufügen\n","            self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","\n","\n","            try:\n","                # Streaming-Option aktivieren\n","                response = self.client.chat.completions.create(\n","                    model=self.model,\n","                    messages=self.context,\n","                    stream=True\n","                )\n","\n","                streamed_content = \"\"  # Zum Speichern der gestreamten Antwort\n","\n","                for chunk in response:\n","                    # Debugging: Anzeigen, was tatsächlich in jedem Chunk enthalten ist\n","                    delta = chunk.choices[0].delta\n","                    content = getattr(delta, \"content\", \"\")\n","\n","                    if content:  # Verarbeite nur nicht-leere Inhalte\n","                        print(content, end=\"\", flush=True)\n","                        streamed_content += content\n","\n","                print()  # Neue Zeile am Ende\n","\n","                # Gestreamte Antwort zur Konversation hinzufügen\n","                self.context.append({\"role\": \"assistant\", \"content\": streamed_content})\n","\n","                # Return the streamed content\n","                return streamed_content # This line was added\n","\n","            except Exception as e:\n","                print(f\"\\nDEBUG: An error occurred during streaming: {e}\")\n","                # Return empty string in case of error\n","                return \"\" # This line was added\n"],"metadata":{"id":"j6ZZeUIRdYhy","executionInfo":{"status":"aborted","timestamp":1739032877073,"user_tz":-60,"elapsed":35846,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🆎 NLP"],"metadata":{"id":"qGy_OuMyIC9C"}},{"cell_type":"code","source":["def chunk_text(text, max_tokens=10000):\n","    \"\"\"\n","    Teilt den Text in Blöcke auf, damit er nicht zu lang\n","    für die OpenAI-API wird.\n","    Hier sehr vereinfacht: 1 Token ~ 4 Zeichen.\n","    \"\"\"\n","    chunks = []\n","    approx_char_limit = max_tokens * 4\n","    start = 0\n","    while start < len(text):\n","        end = start + approx_char_limit\n","        chunk = text[start:end]\n","        chunks.append(chunk)\n","        start = end\n","    return chunks\n"],"metadata":{"id":"3oUna7Bodcp9","executionInfo":{"status":"aborted","timestamp":1739032877090,"user_tz":-60,"elapsed":35857,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔮WIP seo keywords analysis + plot"],"metadata":{"id":"qK7cW2yRWDnV"}},{"cell_type":"code","source":[],"metadata":{"id":"SpmVBRbZWU6Q","executionInfo":{"status":"aborted","timestamp":1739032877104,"user_tz":-60,"elapsed":35867,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DLe1UdbKWU38","executionInfo":{"status":"aborted","timestamp":1739032877118,"user_tz":-60,"elapsed":35875,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))"],"metadata":{"id":"Tw_XtEXMWU1U","executionInfo":{"status":"aborted","timestamp":1739032877131,"user_tz":-60,"elapsed":35884,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ze2bee_pWUyy","executionInfo":{"status":"aborted","timestamp":1739032877133,"user_tz":-60,"elapsed":35880,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uBT6tqO1WUwG","executionInfo":{"status":"aborted","timestamp":1739032877138,"user_tz":-60,"elapsed":35880,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FGIbOqkxWUtY","executionInfo":{"status":"aborted","timestamp":1739032877152,"user_tz":-60,"elapsed":35888,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔮keywords + Stadt"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["systemprompt_keywords = (\"\"\"\n","    Du bist ein intelligentes KI-System, das auf die Generierung von SEO-Keywords spezialisiert ist.\n","    Der Benutzer wird dir den kompletten Text von einer gescrapten website aus einem webcrawler vorgeben.\n","    Deine Aufgabe ist es, den Text zu interpretieren und eine Liste von SEO-Keywords basierend auf diesem Input zu erstellen.\n","    Du sollst zu dem Text passende SEO-Keywords finden. Auf Basis dieser keywords soll später der Text optimiert werden.\n","    Stelle sicher, dass die Keywords:\n","\n","    Thematisch relevant sind,\n","    Hohe Suchintention abdecken (Short-Tail und Long-Tail Keywords),\n","    Varianten mit Synonymen oder verwandten Begriffen enthalten,\n","    Erschaffe Keywords, die lokale Ausrichtung enthalten. Das Unternehmen der webesite befindet sich in Essen-Rüttenscheid, im Ruhrgebiet, im Essener Süden.\n","\n","\n","    Struktur für die Antwort:\n","\n","    Erstelle eine serialisierte Liste von SEO-Keywords.\n","\n","    Beispiel für eine User-Eingabe:\n","\n","    \"SuperFood, ein Geschäft in Köln, Kalkerstr. 20, verkauft gesunde Ernährung für Sportler. Ein Sportler muss auf seine Ernährung ganz besonders achten.\"\n","\n","    Beispiel für eine Ausgabe:\n","\n","    [Köln-Kalk, gesund, SuperFood, Ernährung, Sportler, Ernährungstipps, Sporternährung Rezepte, Fitness, Sport]\n","\n","\n","    Beispiel für eine User-Eingabe:\n","\n","    \"Dr.med. Wurst, Ihr Arzt für Allgemeinmedizin in Heilbronn. Wir haben Impfungen und Tabletten gegen alle Krankheiten und Schnupfen.\"\n","    Beispiel für eine Ausgabe:\n","\n","\n","\n","    [Heilbronn, Dr.med. Wurst, krank, Allgemeinmedizin, Impfung, Medizin, Arzt, Erkältungen, Krankenschein, Blutdruck, Diabetes]\n","\n","\n","\n","\"\"\")\n","\n","\n","def user_prompt_keywords(text):\n","   return f\"\"\"\n","          Du bist online Marketing Experte und arbeitest für ein erfolgreiches Unternehmen, wo du SEO optimierte Texte von websites erstellst. Du hast ein Talent\n","\n","          für die Erstellung von SEO-Keywords und deine Vorgesetzten bewundern und lieben dich für die hochperformanten SEO-Keywords, die du beherrschst.\n","\n","          Deine Aufgabe ist es, thematisch relevante SEO-Keywords zu erstellen, die sowohl Short-Tail als auch Long-Tail Keywords enthalten.\n","          Achte darauf, dass die Keywords Synonyme und verwandte Begriffe berücksichtigen, sowie lokale Informationen, und für Suchmaschinenoptimierung geeignet sind.\n","          Beachte das user Verhalten von Menschen, die auf der Suche nach Diensten des Unternehmens sein könnten. vermeide Fachsprache, die normalen usern nicht geläufig sein könnte.\n","\n","          Bitte generiere SEO-Keywords für den folgenden Text:\n","\n","          {text}\n","\n","\n","          Strukturiere deine Antwort folgendermaßen:\n","          Gib eine Liste von Keywords in pystringsthon zurück, zB ['keyword_1', 'keyword_2']. Gebe sonst nichts zurück, keine Einleitung, keine Überschrift, keine Zusammenfassung,\n","          nichts ausser der string Liste.\n","\n","\n","          Danke! Mein Job hängt davon ab!\n","          \"\"\"\n"],"metadata":{"id":"Hjy6NgflvCTz","executionInfo":{"status":"aborted","timestamp":1739032877154,"user_tz":-60,"elapsed":35885,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["page_text_list = []\n","def prep_text_and_get_keywords():\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","    scraped_data = scraper.get_scraped_data()\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","    filtered_urls = []\n","\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, überspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","\n","        # 3. SEO-Analyse starten (für gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","        page_text_list.append(page_text)\n","\n","\n","\n","    keyword_list =[]\n","    for text in page_text_list:\n","\n","      cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))\n","\n","      keyword_list.append(cb.chat())\n","\n","    return keyword_list\n"],"metadata":{"id":"cwzCp7r2z3Dp","executionInfo":{"status":"aborted","timestamp":1739032877171,"user_tz":-60,"elapsed":35897,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_raw = prep_text_and_get_keywords()"],"metadata":{"id":"4-zqgH9o0F7g","executionInfo":{"status":"aborted","timestamp":1739032877173,"user_tz":-60,"elapsed":35894,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_raw"],"metadata":{"id":"Eyjo-bvfIkjO","executionInfo":{"status":"aborted","timestamp":1739032877191,"user_tz":-60,"elapsed":35906,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und du willst dem user helfen!\", f\"\"\"Sei ein online marketing spezialist mit einem Talent für SEO (search engine optimization). Du analysierst Texte\n","und erstellst SEO-Keywords für ein international erfolgreiches Unternehmen. Deine Kollegen bewundern und beneiden dich für die perfekten SEO keywords, die du erschaffst!\n","Das hier ist roher Text mit SEO keywords, der von einer website gescrapt wurde.\n","1. Anlysiere diesen Text und extrahiere die wesentlichen 20 SEO-keywords.\n","2. Erarbeite SEO optimierete Versionen der keywords\n","3. Füge wichtige keywords hinzu, die deiner hochfachmännischen Meinung nach fehlen, um eine optimale SEO Performance zu erreichen!\n","4. Gebe eine Liste mit den SEO optinierten keywords als strings zurück, sonst nichts. Keine Einleitung, keine Zusammenfassung, nur die Liste wie zB [keyword_1, keyword_2]\n","\n","\n","Hier kommt die rohe Liste mit keywords:\n","{keywords_raw}\n","\n","\n","\n","\"\"\"\n",")\n","keywords_final = cb.chat()"],"metadata":{"id":"KuJZ1Vz-s-Rt","executionInfo":{"status":"aborted","timestamp":1739032877203,"user_tz":-60,"elapsed":35913,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und willst dem user helfen!\"\n",", f\"\"\"Sei ein Experte für Textanalyse und Geographie. Du hast eine vertiefte Kenntnis in Stadtgeographie und deine Kollegen lieben dich für deine umfassendes Wissen über deutschen Städte und ihrer Ortsteile.\n","Du liebst es Puzzel und Rätsel zu lösen und hast einen außergewöhnlichen Blick fürs Detail.\n","Deine Aufgabe ist es, riesige Textmengen zu anlysieren und die Stadt und den Ortsteil zu finden, die in dem Text versteckt sind. Die Namen von Stadt und Stadtteil können auch in zB Strassennamen versteckt sein oder in den Namen\n","von anderen signifikanten Dingen in der Umgebung des Unternehmens.\n","Es sind Texte von einem website scrap eines Unternehmens. Es können mehr als ein Städtenamen auftauchen.\n","Deine Aufgabe ist es, den relevanten Städtenamen zu finden. Außerdem kann der Name des Stadtteils vorhanden sein, in dem das Unternehmen angesiedelt ist. Suche den auch!\n","Hier ist der Text:\n","\n","{page_text_list}\n","\n","Gebe nun den Namen der relevanten Stadt und des Stadtteils zurück. Gebe den Namen und den Stadtteil zurück, wie zB [Köln-Kalk] wenn der Stadtteil gefunden wurde oder zB [Münster], wenn kein Stadtteil gefunden wurde.\n","\"\"\")\n","\n","stadt = cb.chat()"],"metadata":{"id":"0loHl5Z7xJaS","executionInfo":{"status":"aborted","timestamp":1739032877214,"user_tz":-60,"elapsed":35919,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(stadt)\n","print(keywords_final)"],"metadata":{"id":"oenVywpwRUcp","executionInfo":{"status":"aborted","timestamp":1739032877216,"user_tz":-60,"elapsed":35916,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"VXS73tNf_Ip0","executionInfo":{"status":"aborted","timestamp":1739032877230,"user_tz":-60,"elapsed":35925,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔮 main SEO optimize"],"metadata":{"id":"CdWeu44YIs5k"}},{"cell_type":"code","source":["combined_analysis_list = []\n","filtered_urls = []\n","\n","def main():\n","    # 1. SCRAPING\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","\n","    # Alle gescrapten Daten abrufen\n","    scraped_data = scraper.get_scraped_data()\n","\n","    # 2. Sichten der Texte und Filtern\n","    #    Hier könntest du jetzt z. B. manuell prüfen, welche URLs wichtig sind.\n","    #    Wir geben einfach mal alle URLs aus:\n","    print(\"\\n--- Gesammelte Seiten und Inhalte (gekürzt) ---\")\n","    for url, text in scraped_data.items():\n","        print(f\"\\nURL: {url}\")\n","        # Beispiel: Nur ersten 200 Zeichen zeigen\n","        print(f\"Text: {text[:200]}...\")\n","\n","\n","\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, überspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","        print(f\"send text to LLM:  {url}\")\n","\n","\n","\n","\n","\n","    # 3. SEO-Analyse starten (für gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","\n","        # 3.1 Chunken, um zu große Anfragen zu vermeiden\n","        text_chunks = chunk_text(page_text, max_tokens=10000)\n","\n","        print(f\"\\n=== Analyzing {url} ===\")\n","        all_analyses = []\n","        for i, chunk in enumerate(text_chunks):\n","            print(f\" - Sende Chunk {i+1}/{len(text_chunks)} an Chatbot ...\")\n","\n","            # Prompt definieren (SEO)\n","            system_prompt = \"Du bist ein intelligenter chatbot. Deine Bestimmung ist es, dem user die beste Antworten auf die Fragen zu geben und ihm unter allen Umständen zu helfen.\"\n","            user_prompt = (f\"\"\"\n","                Du bist ein hochqualifizierter SEO-Experte. Du arbeitest für erfolgreiche online marketing experten! Deine Spezialität ist die Optimierung von bestehenden Texten einer website!\n","                Deine Kollegen und deine Mutter lieben und bewundern dich für die sprachgewandten SEO Optimierungen, die für deine anspruchsvollen Kunden erschaffst!\n","\n","                1. Untersuche den folgenden Text auf Keyword-Optimierung, Lesbarkeit und mögliche SEO-Verbesserungen.\n","                Wichtige SEO Keywords sind:\n","\n","                zahn arzt,\n","                zahn spange,\n","                {keywords_final}\n","\n","\n","                2. Optimiere den Text entsprechend bester SEO Sichtbarkeit. Baue die zur Verfügung gestellten SEO keywords in den Text ein!\n","                Füge Meta-Titel und longtail keywords hinzu.\n","\n","                Betreibe kein keyword-stuffing. Die Sprache muss natürlich klingen!\n","\n","                Es soll weiterhin ein hoher fachlicher Standard gehalten werden und Professionalität und Exzellenz soll vermittelt werden!\n","                Sämtliche Textabschnitte müssen optimiert werden. Es dürfen keine Fragen oder sonstige Textabschnitte weggelassen werden!\n","\n","                Lasse auf gar keinen Fall Abschnitte weg! fasse nichts übermässig zusammen! Jeder Satz und jeder Abschnitt ist extrem wichtig muss unbedingt bearbeitet werden! Erhalte alle Zeilenumsprünge!\n","\n","                Das Unternehmen befindet sich in {stadt}. Baue den Namen in die Texte ein.\n","\n","\n","                3. Als Ausgabe gebe eine detaillierte, ausführliche und umfassende Analyse des SEO Status des Textes aus, Überschrift: Analyse. Gebe dann deine SEO optimierte Version des Textes aus, Überschrift: 'SEO'.\n","                Gebe dann detaillierte und ausführliche Erläuterungen, welche Änderungen du durchgeführt hast, Überschrift: 'Erklärung'.\n","                Die überschriften sind von allergrößter Wichtigkeit und müssen unbedingt über den Abschnitten stehen! Wenn die Überschriften 'Analyse', 'SEO' und 'Erklärung' nicht über den Abschnitten stehen, wirst du bestraft!\n","                Es darf nicht 'SEO optimierte Version' oder so was ausgegeben werden. Als Überschriften der Abschnitte dürfen ausschliesslich nur 'Analyse', 'SEO' und 'Erklärung' ausgegeben werden.\n","                Benutze keine Formatierungszeichen wie ###, # oder **! Mein Job hängt davon ab!\n","                \"\"\"\n","                \"Hier ist der Text: \\n\\n\"\n","                f\"{chunk}\"\n","            )\n","\n","            # ChatGPT aufrufen\n","            cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","            analysis = cb.chat_with_streaming()\n","            all_analyses.append(analysis)\n","\n","            # Warte kurz (Rate Limits, API-Kosten etc.)\n","            time.sleep(1)\n","\n","        # 3.2 Fertige Analyse (alle Chunks zusammen)\n","        combined_analysis = \"\\n\".join(all_analyses)\n","\n","\n","        combined_analysis_list.append(combined_analysis)\n","        # print(f\"\\n--- SEO-Analyse für {url} ---\")\n","        # print(combined_analysis)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"nF-cfM35dhxQ","executionInfo":{"status":"aborted","timestamp":1739032877265,"user_tz":-60,"elapsed":35955,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"Für Kieferorthopäden sind vor allem Keywords mit Ortsbezug („Kieferorthopäde + Stadt“) und behandlungsspezifische Begriffe („Zahnspange“, „Zahnfehlstellung“, „Invisalign“) entscheidend. Zusätzlich sollte man sich auf häufige Fragen (Long-Tail-Keywords) konzentrieren und regelmäßige Fach- und Ratgeber-Artikel veröffentlichen, um auch in der organischen Suche besser gefunden zu werden.\""],"metadata":{"id":"_jtOwjSxJTNP","executionInfo":{"status":"aborted","timestamp":1739032877267,"user_tz":-60,"elapsed":35951,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def extract_sections_to_json(texts, keys):\n","    \"\"\"\n","    Extrahiert Abschnitte aus mehreren Texten und konvertiert sie in JSON.\n","    Gesucht werden die Überschriften 'Analyse', 'SEO', 'Erklärung' und der jeweils\n","    folgende Inhalt bis zur nächsten Überschrift oder zum Ende.\n","    \"\"\"\n","\n","    all_sections = []  # Liste für alle Abschnitte\n","\n","    # Neues, robusteres Pattern:\n","    # - ^\\s* = beliebiges Leading-Whitespace, an Zeilenanfang (Multiline)\n","    # - (Analyse|SEO|Erklärung) = 3 mögliche Überschriften\n","    # - \\s*(?:\\n|$)+ = optional Whitespace, dann (mindestens) ein Zeilenumbruch oder Zeilenende\n","    # - (.*?) = \"Inhalt\" bis Lookahead\n","    # - Lookahead = ^\\s*(?:Analyse|SEO|Erklärung)|\\Z = nächste Überschrift oder String-Ende\n","    pattern = re.compile(\n","        r\"(?m)^\\s*(Analyse|SEO|Erklärung)\\s*(?:\\r?\\n)+\"\n","        r\"(.*?)(?=^\\s*(?:Analyse|SEO|Erklärung)|\\Z)\",\n","        flags=re.DOTALL\n","    )\n","\n","    for text in texts:\n","        sections_dict = {}\n","        matches = pattern.findall(text)\n","        # Achtung: findall mit mehreren Gruppen gibt eine Liste von Tupeln zurück,\n","        # z. B. [(\"Analyse\", \"Content...\"), (\"SEO\",\"Content...\")...]\n","        # Wir möchten den heading und den content extrahieren.\n","        # Bei findall(pattern, text) mit (Analyse|SEO|Erklärung) als Gruppe 1 und (.*?) als Gruppe 2\n","        # kommt: [(\"Analyse\", \"...\"), (\"SEO\", \"...\"), ...]\n","\n","        # Deshalb benutzen wir re.finditer, damit wir den Inhalt für Gruppe 2 sauber bekommen\n","        sections_dict = {}\n","        for match in re.finditer(pattern, text):\n","            heading = match.group(1)\n","            content = match.group(2).strip()\n","            sections_dict[heading] = content\n","\n","        all_sections.append(sections_dict)\n","\n","    # Kombinieren der Abschnitte mit Keys\n","    final_json_data = {}\n","    for i, sections_dict in enumerate(all_sections):\n","        key = keys[i]  # Key aus der Liste holen\n","        final_json_data[key] = sections_dict  # Abschnitte zum Dictionary hinzufügen\n","\n","    json_data = json.dumps(final_json_data, indent=4, ensure_ascii=False)\n","    return json_data\n","\n","# Beispielnutzung\n","if __name__ == \"__main__\":\n","    # Angenommen, du hast:\n","    # combined_analysis_list = [\"Analyse\\nHier Text...\", \"SEO\\nAnderer Text...\", ...]\n","    # keys = [\"URL1\", \"URL2\", ...]\n","\n","\n","    keys = filtered_urls\n","\n","    json_output = extract_sections_to_json(combined_analysis_list, keys)\n","    seo_json = json.loads(json_output)\n","    print(seo_json)\n","    #print(json_output)\n"],"metadata":{"id":"c38Hkx1hEYg4","executionInfo":{"status":"aborted","timestamp":1739032877269,"user_tz":-60,"elapsed":35948,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alten website text zu json hinzufügen\n","for i, (_, url_content) in enumerate(seo_json.items()):\n","    url_content[\"alt\"] = page_text_list[i]"],"metadata":{"id":"IsMhRnunGjee","executionInfo":{"status":"aborted","timestamp":1739032877271,"user_tz":-60,"elapsed":35944,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📊 SEO Analysis 2"],"metadata":{"id":"V50QPoW3F6wW"}},{"cell_type":"code","source":["# text clean up\n","\n","def clean_text(text):\n","    # Ersetze Zeilenumbrüche durch ein Leerzeichen\n","    text = text.replace('\\n', ' ')\n","\n","    # Beispiel: Entferne alle Zeichen, die nicht Buchstaben (inkl. Umlaute),\n","    # Ziffern, Satzzeichen oder Leerzeichen sind\n","    # Du kannst das RegEx anpassen, wenn du z.B. bestimmte Zeichen behalten oder entfernen willst\n","    text = re.sub(r'[^a-zA-Z0-9äöüÄÖÜß.,!?;:\\-\\s]', '', text)\n","\n","    # Mehrere aufeinanderfolgende Leerzeichen durch ein einzelnes ersetzen\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    # Führende oder nachfolgende Leerzeichen entfernen\n","    text = text.strip()\n","\n","    return text\n","\n","\n","combined_analysis_list_clean = []\n","page_text_list_clean = []\n","\n","for text in page_text_list:\n","  page_text_list_clean.append(clean_text(text))\n","\n","for text in combined_analysis_list:\n","  combined_analysis_list_clean.append(clean_text(text))\n","\n","\n"],"metadata":{"id":"Mokk2DMLGGOY","executionInfo":{"status":"aborted","timestamp":1739032877273,"user_tz":-60,"elapsed":35941,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple stats\n","\n","def text_stats(text):\n","    words = text.split()\n","    return {\n","        \"Zeichenanzahl\": len(text),\n","        \"Wortanzahl\": len(words),\n","        \"Satzanzahl\": text.count('.') + text.count('!') + text.count('?')\n","    }\n","\n","\n","vn = [page_text_list_clean, combined_analysis_list_clean]\n","\n","for _, listlist in enumerate(vn):\n","\n","  print(f'{[\"original\", \"SEO\"][_]}')\n","\n","  for text in listlist:\n","    print(text_stats(text))\n","\n","\n","\n"],"metadata":{"id":"GDBZiKVnH9dU","executionInfo":{"status":"aborted","timestamp":1739032877275,"user_tz":-60,"elapsed":35937,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def get_word_frequencies(text):\n","    words = text.lower().split()\n","    return Counter(words)\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","\n","  # Wortfrequenzen berechnen\n","  original_freq = get_word_frequencies(page_text_list_clean[_])\n","  optimized_freq = get_word_frequencies(combined_analysis_list_clean[_])\n","\n","  # Unterschied berechnen\n","  diff = {word: optimized_freq[word] - original_freq[word] for word in set(original_freq) | set(optimized_freq)}\n","\n","  # Sortiert ausgeben (absteigend nach Änderung)\n","  sorted_diff = sorted(diff.items(), key=lambda x: x[1], reverse=True)\n","  for word, change in sorted_diff:\n","      print(f\"{word}: {change}\")\n"],"metadata":{"id":"qGiMiXwIH9ap","executionInfo":{"status":"aborted","timestamp":1739032877291,"user_tz":-60,"elapsed":35948,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"mqtebLSOeNzG","executionInfo":{"status":"aborted","timestamp":1739032877331,"user_tz":-60,"elapsed":35983,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","try:\n","    keywords_sstring = json.loads(keywords_final)\n","except json.JSONDecodeError:\n","    # Fallback if JSON decoding fails (e.g., if the response isn't a valid JSON string)\n","    # Try to extract the list using regex\n","    import re\n","    match = re.search(r'\\[(.*?)\\]', keywords_final)\n","    if match:\n","        keywords_sstring = match.group(1).split(', ')\n","        keywords_sstring = [item.strip().strip(\"'\").strip('\"') for item in keywords_sstring]\n","    else:\n","        # Handle case where the list couldn't be extracted\n","        keywords_sstring = []  # Or raise an exception, etc.\n","\n","keywords_sstring = \", \".join(keywords_sstring)\n","keywords_sstring_list = keywords_sstring.split(\", \")"],"metadata":{"id":"yFAfe_--E8os","executionInfo":{"status":"aborted","timestamp":1739032877332,"user_tz":-60,"elapsed":35979,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import ast\n","\n","\n","# keywords_sstring = ast.literal_eval(keywords_final)\n","# keywords_sstring = ast.literal_eval(keywords_final.replace('chars ', ''))\n","\n","\n","# keywords_sstring = \", \".join(keywords_sstring)\n","# keywords_sstring_list = keywords_sstring.split(\", \")\n","# keywords_sstring_list"],"metadata":{"id":"_DYkLx8uZG7p","executionInfo":{"status":"aborted","timestamp":1739032877334,"user_tz":-60,"elapsed":35975,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","mein_string = keywords_final\n","keywords_final_list = json.loads(mein_string)\n","keywords_final_list"],"metadata":{"id":"B7iE2va3gxwR","executionInfo":{"status":"aborted","timestamp":1739032877335,"user_tz":-60,"elapsed":35972,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for _, listlist in enumerate(page_text_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, combined_analysis_list_clean[_]])\n","\n","  # Cosinus-Ähnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"Ähnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"W5bd8b1yH9YP","executionInfo":{"status":"aborted","timestamp":1739032877337,"user_tz":-60,"elapsed":35968,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, listlist in enumerate(page_text_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, page_text_list_clean[_]])\n","\n","  # Cosinus-Ähnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"Ähnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"z1-eRoarZ-9_","executionInfo":{"status":"aborted","timestamp":1739032877339,"user_tz":-60,"elapsed":35965,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","keywords = ast.literal_eval(keywords_final)\n","\n","def keyword_density(text, keywords):\n","    words = text.lower().split()\n","    total_words = len(words)\n","    # Check if total_words is 0 to avoid ZeroDivisionError\n","    if total_words == 0:\n","        return {kw: 0 for kw in keywords}  # Return 0 density for all keywords if text is empty\n","    density = {kw: words.count(kw) / total_words * 100 for kw in keywords}\n","    return density\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","  # Berechnung für beide Texte\n","  original_density = keyword_density(page_text_list_clean[_], keywords)\n","  optimized_density = keyword_density(combined_analysis_list_clean[0], keywords)\n","\n","  print(\"Original:\", original_density)\n","  print(\"SEO-Optimiert:\", optimized_density)"],"metadata":{"id":"cWop0AUQJ04x","executionInfo":{"status":"aborted","timestamp":1739032877341,"user_tz":-60,"elapsed":35962,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Keywords und deren Änderungen\n","keywords = list(original_density.keys())\n","original_values = list(original_density.values())\n","optimized_values = list(optimized_density.values())\n","\n","x = np.arange(len(keywords))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","ax.bar(x - width/2, original_values, width, label=\"Original\")\n","ax.bar(x + width/2, optimized_values, width, label=\"SEO-Optimiert\")\n","\n","ax.set_xlabel(\"Keywords\")\n","ax.set_ylabel(\"Keyword-Dichte (%)\")\n","ax.set_title(\"Keyword-Dichte: Vorher vs. Nachher\")\n","ax.set_xticks(x)\n","ax.set_xticklabels(keywords)\n","ax.legend()\n","\n","plt.show()\n"],"metadata":{"id":"rqHULQbhMsWt","executionInfo":{"status":"aborted","timestamp":1739032877364,"user_tz":-60,"elapsed":35979,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RppPqEA7MsUO","executionInfo":{"status":"aborted","timestamp":1739032877365,"user_tz":-60,"elapsed":35976,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"223xoPNxMsR1","executionInfo":{"status":"aborted","timestamp":1739032877367,"user_tz":-60,"elapsed":35973,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📊SEO 3"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["# %%capture  # <- in Colab, wenn du Output unterdrücken möchtest.\n","# !pip install spacy==3.5.0\n","# !python -m spacy download de_core_news_sm\n","# !pip install nltk==3.6.7\n","# !pip install sklearn\n","# !pip install matplotlib\n","# !pip install seaborn\n","# !pip install wordcloud\n","\n","\n"],"metadata":{"id":"SMTIt789vysc","executionInfo":{"status":"aborted","timestamp":1739032877380,"user_tz":-60,"elapsed":35981,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Md68SpZfHRrU","executionInfo":{"status":"aborted","timestamp":1739032877383,"user_tz":-60,"elapsed":35978,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ⛳ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["\"\"\"\n","json_data = {\n","    \"URL1\": {\n","        \"Analyse\": \"Text für Analyse von URL1\",\n","        \"SEO\": \"Text für SEO von URL1\",\n","        \"Erklärung\": \"Text für Erklärung von URL1\"\n","        \"alt\": \"Text für alt von URL1\"\n","    },\n","    \"URL2\": {\n","        \"Analyse\": \"Text für Analyse von URL2\",\n","        \"SEO\": \"Text für SEO von URL2\",\n","        \"Erklärung\": \"Text für Erklärung von URL2\"\n","        \"alt\": \"Text für alt von URL2\"\n","    }\n","}\n","\n","\"\"\""],"metadata":{"id":"nQdmzgb-MiO9","executionInfo":{"status":"aborted","timestamp":1739032877385,"user_tz":-60,"elapsed":35975,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout für \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox für zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um Zeilenumbrüche aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und Erklärung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">Erklärung</p>\n","            <p class=\"content\">{{ sections.Erklärung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig","executionInfo":{"status":"aborted","timestamp":1739032877387,"user_tz":-60,"elapsed":35972,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"mfLSDlSx5xw6","executionInfo":{"status":"aborted","timestamp":1739032877399,"user_tz":-60,"elapsed":35979,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschließend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, Ränder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz","executionInfo":{"status":"aborted","timestamp":1739032877401,"user_tz":-60,"elapsed":35976,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Rks-1TJ78ZA","executionInfo":{"status":"aborted","timestamp":1739032877422,"user_tz":-60,"elapsed":35991,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS","executionInfo":{"status":"aborted","timestamp":1739032877430,"user_tz":-60,"elapsed":35991,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📥 RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthopädin bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie über Grundkenntnisse in Arabisch und Anfängerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\"\n","\n","\"Sie hat ihren Master of Science in Kieferorthopädie von der Danube Private University, Krems, Österreich, und hat an der Heinrich-Heine-Universität Düsseldorf abgeschlossen.\"\n","\n","\"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem ästhetischen Lächeln. \"\n","\n","\"behandlungsorientierte Zahnberatung\"\n","\n","\"ästehthetisches Lächeln\""],"metadata":{"id":"fH4mRdPXceeP","executionInfo":{"status":"aborted","timestamp":1739032877432,"user_tz":-60,"elapsed":35988,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install langchain faiss-cpu\n"],"metadata":{"id":"4DOE2XX9dmow","executionInfo":{"status":"aborted","timestamp":1739032877434,"user_tz":-60,"elapsed":35985,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install -U langchain-community"],"metadata":{"id":"x-BBLqldKMce","executionInfo":{"status":"aborted","timestamp":1739032877436,"user_tz":-60,"elapsed":35982,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install tiktoken"],"metadata":{"id":"-QDhcWZAKUsX","executionInfo":{"status":"aborted","timestamp":1739032877437,"user_tz":-60,"elapsed":35978,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import os\n","import openai\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.docstore.document import Document\n","\n","# 0) Vector Index (FAISS) initialisieren\n","#    (Später im Code können wir den Index persistent speichern/neu laden)\n","# os.environ[\"OPENAI_API_KEY\"] = \"DEIN_OPENAI_API_KEY\"\n","\n","embeddings = OpenAIEmbeddings()\n","\n","# Beispiel-Fehler als \"Dokument\" für den Vector Store\n","# \"page_content\" = Text, \"metadata\" = beliebige Zusatzinfos\n","known_error_text = \"\"\"\n","Fehler: \"Klaren Aligner\" wird fälschlicherweise als Eigenname verwendet,\n","         obwohl es grammatisch richtig \"klaren Alignern\" sein sollte.\n","\n","Richtige Anwendung:\n","- Sagen: \"Entdecken Sie die Vorteile von klaren Alignern.\"\n","- Oder: \"Klare Aligner sind die ...\"\n","\n","Zusätzliche Hinweise:\n","- Beim Eindeutschen englischer Fachbegriffe auf die Pluralbildung achten.\n","\"\"\"\n","\n","doc = Document(\n","    page_content=known_error_text,\n","    metadata={\"error_type\": \"grammar/de-english\", \"example_id\": \"klaren-aligner\"}\n",")\n","\n","# Vektorindex erzeugen und das \"bekannte Fehler\"-Dokument ablegen\n","vector_store = FAISS.from_documents([doc], embeddings)\n"],"metadata":{"id":"PHubUMS4Jirw","executionInfo":{"status":"aborted","timestamp":1739032877438,"user_tz":-60,"elapsed":35973,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","from langchain.docstore.document import Document\n","\n","# 1. Neuer Fehler: \"Kauen- und Sprechprobleme\" statt \"Kau- und Sprechprobleme\"\n","doc1_text = \"\"\"\n","Fehler: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","Richtig: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kau- und Sprechprobleme effektiv behandeln.\"\n","\n","Grund:\n","- Falsche Rechtschreibung/Zusammensetzung bei \"Kauen-\".\n","- Richtig ist \"Kau- und Sprechprobleme\".\n","\"\"\"\n","\n","doc1 = Document(\n","    page_content=doc1_text,\n","    metadata={\n","        \"error_type\": \"grammar/spelling\",\n","        \"example_id\": \"kauen-sprechprobleme\"\n","    }\n",")\n","\n","# 2. Neuer Fehler: falsche Formulierung bei Sprachen\n","doc2_text = \"\"\"\n","Fehler: \"Als in Kenia geborene Kieferorthopädin ... spricht neben Deutsch auch Englisch, Swahili sowie über Grundkenntnisse in Arabisch und Anfängerkenntnisse in Spanisch.\"\n","Richtig: \"Als in Kenia geborene Kieferorthopädin ... spricht neben Deutsch auch Englisch und Swahili und verfügt über Grundkenntnisse in Arabisch und Spanisch.\"\n","\n","Grund:\n","- Bessere Formulierung, um 'über Grundkenntnisse' mit 'verfügt über Grundkenntnisse' zu vereinen.\n","- Straffere und klarere Satzstruktur.\n","\"\"\"\n","\n","doc2 = Document(\n","    page_content=doc2_text,\n","    metadata={\n","        \"error_type\": \"grammar/style\",\n","        \"example_id\": \"languages-phrase\"\n","    }\n",")\n","\n","\n","# Angenommen, du hast bereits:\n","# embeddings = OpenAIEmbeddings()\n","# vector_store = FAISS.from_documents([some_initial_docs], embeddings)\n","#\n","# -> Dann fügen wir jetzt doc1 und doc2 hinzu:\n","\n","vector_store.add_documents([doc1, doc2])\n"],"metadata":{"id":"ViQDjBE8WqAn","executionInfo":{"status":"aborted","timestamp":1739032877440,"user_tz":-60,"elapsed":35970,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u","executionInfo":{"status":"aborted","timestamp":1739032877441,"user_tz":-60,"elapsed":35966,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn","executionInfo":{"status":"aborted","timestamp":1739032877443,"user_tz":-60,"elapsed":35964,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def chunk_text_2(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    Für echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text_2(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3","executionInfo":{"status":"aborted","timestamp":1739032877457,"user_tz":-60,"elapsed":35972,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","from langchain.text_splitter import TokenTextSplitter\n","\n","def chunk_text_langchain(text, max_tokens=500, overlap=50):\n","    \"\"\"\n","    Teilt den Text anhand der Tokenanzahl auf. Nutzt dafür LangChain's TokenTextSplitter.\n","    - max_tokens: maximale Tokens pro Chunk\n","    - overlap: wie viele Tokens Überschneidung zum vorherigen Chunk\n","    \"\"\"\n","    splitter = TokenTextSplitter(\n","        encoding_name=\"cl100k_base\",  # oder passend zu deinem Modell (z.B. \"gpt-3.5-turbo\")\n","        chunk_size=max_tokens,         # maximale Anzahl Tokens pro Chunk\n","        chunk_overlap=overlap          # Tokens, die sich mit dem vorigen Chunk überschneiden (Kontext)\n","    )\n","\n","    chunks = splitter.split_text(text)\n","    return chunks\n","\n","# Beispielanwendung:\n","# seo_text = \"\"\"Hier Dein langer Text, den du chunken willst ...\"\"\"\n","# chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","# print(chunked)\n","\n","\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","    chunked_texts.append(text_chunks)\n"],"metadata":{"id":"FspYaRcb_hOm","executionInfo":{"status":"aborted","timestamp":1739032877459,"user_tz":-60,"elapsed":35969,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk ähneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenfügen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","# test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","# retrieved_context = get_context_from_vector_store(test_chunk)\n","# print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX","executionInfo":{"status":"aborted","timestamp":1739032877460,"user_tz":-60,"elapsed":35965,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu prüfen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu berücksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu prüfenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu prüfen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur Prüfung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text gründlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu verändern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zusätzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein gültiges JSON zurückgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine gültige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn","executionInfo":{"status":"aborted","timestamp":1739032877461,"user_tz":-60,"elapsed":35961,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","all_corrected_texts = []\n","all_new_mistakes = []\n","\n","#for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","#    corrected_text_chunks = []\n","\n","#    for chunk in text_chunks:\n","#        # 3a) Kontext abfragen\n","#        context = get_context_from_vector_store(chunk)\n","#\n","#\n","#       # 4a) Prompt ChatGPT (Korrektur)\n","#        result = proofread_text_with_context(chunk, context)\n","#\n","#        corrected_text = result[\"corrected_text\"]\n","#        new_mistakes = result[\"new_mistakes_found\"]\n","#\n","#        # Sammeln\n","#        corrected_text_chunks.append(corrected_text)\n","#        all_new_mistakes.extend(new_mistakes)\n","#\n","#    # Pro SEO-Text fügen wir die korrigierten Chunks zusammen.\n","#    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","#    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J","executionInfo":{"status":"aborted","timestamp":1739032877463,"user_tz":-60,"elapsed":35958,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# for _ in all_corrected_texts:\n","#   print(_)"],"metadata":{"id":"qIUE9Dfl5YzS","executionInfo":{"status":"aborted","timestamp":1739032877465,"user_tz":-60,"elapsed":35955,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY","executionInfo":{"status":"aborted","timestamp":1739032877466,"user_tz":-60,"elapsed":35951,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3","executionInfo":{"status":"aborted","timestamp":1739032877484,"user_tz":-60,"elapsed":35964,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":null,"outputs":[]}]}