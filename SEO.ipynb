{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["TPz7fgRaRAUv","Ew8QRBDOSq9H","o38bsnD-EWcq","vNVT-lr5jejz","FA2RIeko9reT","E0k0HmUZHmkg","qGy_OuMyIC9C","qK7cW2yRWDnV","cgPAMHODtAuZ","CdWeu44YIs5k","V50QPoW3F6wW","4v6-jGMeO9Zt","HraJ3_5SJHSv"],"authorship_tag":"ABX9TyPJagEhdp3yiJkBCzFONp+G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üèÅ Install requirements"],"metadata":{"id":"TPz7fgRaRAUv"}},{"cell_type":"code","source":["from google.colab import drive, userdata\n","notebook_folder = userdata.get('gdrive_seo_folder')\n","\n","drive.mount('/content/drive',\n","            force_remount=True\n","            )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXxs_PE-OzbK","executionInfo":{"status":"ok","timestamp":1739703924439,"user_tz":-60,"elapsed":5056,"user":{"displayName":"Markus","userId":"00979932223459614021"}},"outputId":"d6a18294-6a27-403c-9cb9-b6eef6cbb281"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["folder_path = \"/content/drive/MyDrive/\" + notebook_folder  # Beispiel f√ºr den Ordnernamen\n","\n","# Dynamisch den vollst√§ndigen Dateipfad erstellen\n","requirements_file = f\"{folder_path}/requirements.txt\""],"metadata":{"id":"OBGV2dN4LEgD","executionInfo":{"status":"ok","timestamp":1739703924439,"user_tz":-60,"elapsed":5,"user":{"displayName":"Markus","userId":"00979932223459614021"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install -r '{requirements_file}'\n","!python -m spacy download de_core_news_sm\n","!playwright install\n","!apt-get update\n","!apt-get install -y pandoc\n","# !pip install --upgrade pydantic langchain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjUp2rFaRqqz","outputId":"0ad29305-2f57-4604-88c8-242ec5986e9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/sethblack/python-seo-analyzer.git (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1))\n","  Cloning https://github.com/sethblack/python-seo-analyzer.git to /tmp/pip-req-build-nupuonxf\n","  Running command git clone --filter=blob:none --quiet https://github.com/sethblack/python-seo-analyzer.git /tmp/pip-req-build-nupuonxf\n","  Resolved https://github.com/sethblack/python-seo-analyzer.git to commit cfb38f5b39803ec1dc597158c9226ecd9bc07511\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: spacy==3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.5.0)\n","Requirement already satisfied: nltk==3.6.7 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 3)) (3.6.7)\n","Requirement already satisfied: langchain_anthropic in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.1.23)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 5)) (1.0.1)\n","Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 6)) (1.15)\n","Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 7)) (1.50.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (0.13.2)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 9)) (1.9.4)\n","Requirement already satisfied: pydantic==1.10.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 10)) (1.10.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (4.67.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (2.32.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.6.7->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 3)) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.6.7->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 3)) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.6.7->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 3)) (2024.11.6)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==1.10.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 10)) (4.12.2)\n","Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (4.13.3)\n","Requirement already satisfied: certifi>=2024.8.30 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2025.1.31)\n","Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (5.3.1)\n","Requirement already satisfied: markupsafe>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (3.0.2)\n","Requirement already satisfied: trafilatura>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.3.0)\n","Requirement already satisfied: anthropic<1,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.45.2)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.7.1)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /usr/local/lib/python3.11/dist-packages (from langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.2.43)\n","Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 7)) (12.1.1)\n","Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 7)) (3.1.1)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (2.2.2)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (3.10.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 9)) (11.1.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.6)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.1.147)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (8.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (2025.1)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.10.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (0.1.5)\n","Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.3.2)\n","Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.9.3)\n","Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (3.0.1)\n","Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (2.17.0)\n","Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (0.13)\n","Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (1.2.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.30.0->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.26->langchain_anthropic->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 4)) (1.0.0)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.5.0->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 2)) (1.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 8)) (1.17.0)\n","Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (5.2)\n","Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura>=2.0.0->pyseoanalyzer==2024.12.12->-r /content/drive/MyDrive/Colab Notebooks/SEO/requirements.txt (line 1)) (0.4.1)\n","2025-02-16 11:05:49.246385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1739703949.287087    1461 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1739703949.299375    1461 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-02-16 11:05:49.347653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-02-16 11:06:01.363955: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","Collecting de-core-news-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from de-core-news-sm==3.5.0) (3.5.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.11.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.67.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.3.0)\n","Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.1.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2025.1.31)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.1.5)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.8)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.2.1)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.5.0\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n","Downloading Chromium 133.0.6943.16 (playwright build v1155)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-linux.zip\u001b[22m\n","\u001b[1G163.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 0% 125.0s\u001b[0K\u001b[1G163.5 MiB [] 0% 160.4s\u001b[0K\u001b[1G163.5 MiB [] 0% 49.4s\u001b[0K\u001b[1G163.5 MiB [] 0% 48.4s\u001b[0K\u001b[1G163.5 MiB [] 0% 44.5s\u001b[0K\u001b[1G163.5 MiB [] 0% 34.3s\u001b[0K\u001b[1G163.5 MiB [] 0% 29.0s\u001b[0K\u001b[1G163.5 MiB [] 0% 25.7s\u001b[0K\u001b[1G163.5 MiB [] 1% 22.8s\u001b[0K\u001b[1G163.5 MiB [] 1% 18.0s\u001b[0K\u001b[1G163.5 MiB [] 1% 18.8s\u001b[0K\u001b[1G163.5 MiB [] 1% 17.2s\u001b[0K\u001b[1G163.5 MiB [] 1% 15.2s\u001b[0K\u001b[1G163.5 MiB [] 2% 14.3s\u001b[0K\u001b[1G163.5 MiB [] 2% 14.5s\u001b[0K\u001b[1G163.5 MiB [] 2% 14.4s\u001b[0K\u001b[1G163.5 MiB [] 2% 14.5s\u001b[0K\u001b[1G163.5 MiB [] 2% 14.4s\u001b[0K\u001b[1G163.5 MiB [] 2% 14.3s\u001b[0K\u001b[1G163.5 MiB [] 3% 14.0s\u001b[0K\u001b[1G163.5 MiB [] 3% 13.7s\u001b[0K\u001b[1G163.5 MiB [] 3% 13.6s\u001b[0K\u001b[1G163.5 MiB [] 3% 13.3s\u001b[0K\u001b[1G163.5 MiB [] 3% 12.9s\u001b[0K\u001b[1G163.5 MiB [] 4% 12.5s\u001b[0K\u001b[1G163.5 MiB [] 4% 12.4s\u001b[0K\u001b[1G163.5 MiB [] 4% 11.7s\u001b[0K\u001b[1G163.5 MiB [] 5% 11.1s\u001b[0K\u001b[1G163.5 MiB [] 5% 11.2s\u001b[0K\u001b[1G163.5 MiB [] 5% 10.7s\u001b[0K\u001b[1G163.5 MiB [] 5% 10.4s\u001b[0K\u001b[1G163.5 MiB [] 6% 10.1s\u001b[0K\u001b[1G163.5 MiB [] 6% 10.2s\u001b[0K\u001b[1G163.5 MiB [] 6% 10.3s\u001b[0K\u001b[1G163.5 MiB [] 7% 10.2s\u001b[0K\u001b[1G163.5 MiB [] 7% 10.3s\u001b[0K\u001b[1G163.5 MiB [] 7% 10.4s\u001b[0K\u001b[1G163.5 MiB [] 8% 10.3s\u001b[0K\u001b[1G163.5 MiB [] 8% 10.2s\u001b[0K\u001b[1G163.5 MiB [] 8% 10.1s\u001b[0K\u001b[1G163.5 MiB [] 8% 10.2s\u001b[0K\u001b[1G163.5 MiB [] 9% 10.1s\u001b[0K\u001b[1G163.5 MiB [] 9% 10.2s\u001b[0K\u001b[1G163.5 MiB [] 9% 10.3s\u001b[0K\u001b[1G163.5 MiB [] 10% 10.3s\u001b[0K\u001b[1G163.5 MiB [] 10% 10.4s\u001b[0K\u001b[1G163.5 MiB [] 10% 10.5s\u001b[0K\u001b[1G163.5 MiB [] 10% 10.4s\u001b[0K\u001b[1G163.5 MiB [] 10% 10.5s\u001b[0K\u001b[1G163.5 MiB [] 11% 10.4s\u001b[0K\u001b[1G163.5 MiB [] 11% 10.3s\u001b[0K\u001b[1G163.5 MiB [] 11% 10.2s\u001b[0K\u001b[1G163.5 MiB [] 12% 10.1s\u001b[0K\u001b[1G163.5 MiB [] 12% 10.0s\u001b[0K\u001b[1G163.5 MiB [] 12% 9.9s\u001b[0K\u001b[1G163.5 MiB [] 13% 9.7s\u001b[0K\u001b[1G163.5 MiB [] 13% 9.5s\u001b[0K\u001b[1G163.5 MiB [] 13% 9.6s\u001b[0K\u001b[1G163.5 MiB [] 14% 9.5s\u001b[0K\u001b[1G163.5 MiB [] 14% 9.3s\u001b[0K\u001b[1G163.5 MiB [] 15% 9.4s\u001b[0K\u001b[1G163.5 MiB [] 15% 9.3s\u001b[0K\u001b[1G163.5 MiB [] 15% 9.4s\u001b[0K\u001b[1G163.5 MiB [] 16% 9.3s\u001b[0K\u001b[1G163.5 MiB [] 16% 9.2s\u001b[0K\u001b[1G163.5 MiB [] 16% 9.1s\u001b[0K\u001b[1G163.5 MiB [] 17% 9.0s\u001b[0K\u001b[1G163.5 MiB [] 17% 8.8s\u001b[0K\u001b[1G163.5 MiB [] 17% 8.9s\u001b[0K\u001b[1G163.5 MiB [] 17% 8.8s\u001b[0K\u001b[1G163.5 MiB [] 18% 8.6s\u001b[0K\u001b[1G163.5 MiB [] 18% 8.5s\u001b[0K\u001b[1G163.5 MiB [] 19% 8.4s\u001b[0K\u001b[1G163.5 MiB [] 19% 8.3s\u001b[0K\u001b[1G163.5 MiB [] 19% 8.2s\u001b[0K\u001b[1G163.5 MiB [] 20% 8.1s\u001b[0K\u001b[1G163.5 MiB [] 20% 8.0s\u001b[0K\u001b[1G163.5 MiB [] 21% 7.9s\u001b[0K\u001b[1G163.5 MiB [] 21% 7.8s\u001b[0K\u001b[1G163.5 MiB [] 22% 7.7s\u001b[0K\u001b[1G163.5 MiB [] 22% 7.6s\u001b[0K\u001b[1G163.5 MiB [] 23% 7.4s\u001b[0K\u001b[1G163.5 MiB [] 23% 7.2s\u001b[0K\u001b[1G163.5 MiB [] 24% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 24% 7.0s\u001b[0K\u001b[1G163.5 MiB [] 25% 7.0s\u001b[0K\u001b[1G163.5 MiB [] 25% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 26% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 27% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 27% 7.0s\u001b[0K\u001b[1G163.5 MiB [] 27% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 28% 7.1s\u001b[0K\u001b[1G163.5 MiB [] 29% 7.0s\u001b[0K\u001b[1G163.5 MiB [] 29% 6.9s\u001b[0K\u001b[1G163.5 MiB [] 30% 6.8s\u001b[0K\u001b[1G163.5 MiB [] 30% 6.7s\u001b[0K\u001b[1G163.5 MiB [] 31% 6.6s\u001b[0K\u001b[1G163.5 MiB [] 31% 6.5s\u001b[0K\u001b[1G163.5 MiB [] 32% 6.5s\u001b[0K\u001b[1G163.5 MiB [] 32% 6.4s\u001b[0K\u001b[1G163.5 MiB [] 33% 6.3s\u001b[0K\u001b[1G163.5 MiB [] 33% 6.2s\u001b[0K\u001b[1G163.5 MiB [] 34% 6.2s\u001b[0K\u001b[1G163.5 MiB [] 34% 6.1s\u001b[0K\u001b[1G163.5 MiB [] 34% 6.0s\u001b[0K\u001b[1G163.5 MiB [] 35% 6.0s\u001b[0K\u001b[1G163.5 MiB [] 35% 5.9s\u001b[0K\u001b[1G163.5 MiB [] 36% 5.8s\u001b[0K\u001b[1G163.5 MiB [] 36% 5.7s\u001b[0K\u001b[1G163.5 MiB [] 37% 5.6s\u001b[0K\u001b[1G163.5 MiB [] 37% 5.5s\u001b[0K\u001b[1G163.5 MiB [] 38% 5.4s\u001b[0K\u001b[1G163.5 MiB [] 39% 5.4s\u001b[0K\u001b[1G163.5 MiB [] 39% 5.3s\u001b[0K\u001b[1G163.5 MiB [] 40% 5.3s\u001b[0K\u001b[1G163.5 MiB [] 40% 5.2s\u001b[0K\u001b[1G163.5 MiB [] 40% 5.1s\u001b[0K\u001b[1G163.5 MiB [] 41% 5.1s\u001b[0K\u001b[1G163.5 MiB [] 41% 5.0s\u001b[0K\u001b[1G163.5 MiB [] 42% 5.0s\u001b[0K\u001b[1G163.5 MiB [] 42% 4.9s\u001b[0K\u001b[1G163.5 MiB [] 42% 4.8s\u001b[0K\u001b[1G163.5 MiB [] 43% 4.8s\u001b[0K\u001b[1G163.5 MiB [] 43% 4.7s\u001b[0K\u001b[1G163.5 MiB [] 44% 4.7s\u001b[0K\u001b[1G163.5 MiB [] 44% 4.6s\u001b[0K\u001b[1G163.5 MiB [] 45% 4.5s\u001b[0K\u001b[1G163.5 MiB [] 46% 4.4s\u001b[0K\u001b[1G163.5 MiB [] 47% 4.3s\u001b[0K\u001b[1G163.5 MiB [] 48% 4.2s\u001b[0K\u001b[1G163.5 MiB [] 48% 4.1s\u001b[0K\u001b[1G163.5 MiB [] 49% 4.1s\u001b[0K\u001b[1G163.5 MiB [] 49% 4.0s\u001b[0K\u001b[1G163.5 MiB [] 50% 3.9s\u001b[0K\u001b[1G163.5 MiB [] 51% 3.9s\u001b[0K\u001b[1G163.5 MiB [] 52% 3.8s\u001b[0K\u001b[1G163.5 MiB [] 53% 3.8s\u001b[0K\u001b[1G163.5 MiB [] 53% 3.7s\u001b[0K\u001b[1G163.5 MiB [] 54% 3.7s\u001b[0K\u001b[1G163.5 MiB [] 54% 3.6s\u001b[0K\u001b[1G163.5 MiB [] 55% 3.6s\u001b[0K\u001b[1G163.5 MiB [] 55% 3.5s\u001b[0K\u001b[1G163.5 MiB [] 56% 3.5s\u001b[0K\u001b[1G163.5 MiB [] 56% 3.4s\u001b[0K\u001b[1G163.5 MiB [] 57% 3.4s\u001b[0K\u001b[1G163.5 MiB [] 58% 3.3s\u001b[0K\u001b[1G163.5 MiB [] 59% 3.2s\u001b[0K\u001b[1G163.5 MiB [] 59% 3.1s\u001b[0K\u001b[1G163.5 MiB [] 60% 3.1s\u001b[0K\u001b[1G163.5 MiB [] 61% 3.0s\u001b[0K\u001b[1G163.5 MiB [] 62% 2.9s\u001b[0K\u001b[1G163.5 MiB [] 63% 2.8s\u001b[0K\u001b[1G163.5 MiB [] 64% 2.7s\u001b[0K\u001b[1G163.5 MiB [] 65% 2.7s\u001b[0K\u001b[1G163.5 MiB [] 65% 2.6s\u001b[0K\u001b[1G163.5 MiB [] 66% 2.6s\u001b[0K\u001b[1G163.5 MiB [] 66% 2.5s\u001b[0K\u001b[1G163.5 MiB [] 67% 2.5s\u001b[0K\u001b[1G163.5 MiB [] 67% 2.4s\u001b[0K\u001b[1G163.5 MiB [] 68% 2.4s\u001b[0K\u001b[1G163.5 MiB [] 69% 2.3s\u001b[0K\u001b[1G163.5 MiB [] 70% 2.2s\u001b[0K\u001b[1G163.5 MiB [] 71% 2.2s\u001b[0K\u001b[1G163.5 MiB [] 71% 2.1s\u001b[0K\u001b[1G163.5 MiB [] 72% 2.0s\u001b[0K\u001b[1G163.5 MiB [] 73% 2.0s\u001b[0K\u001b[1G163.5 MiB [] 73% 1.9s\u001b[0K\u001b[1G163.5 MiB [] 74% 1.9s\u001b[0K\u001b[1G163.5 MiB [] 75% 1.8s\u001b[0K\u001b[1G163.5 MiB [] 76% 1.7s\u001b[0K\u001b[1G163.5 MiB [] 77% 1.6s\u001b[0K\u001b[1G163.5 MiB [] 78% 1.6s\u001b[0K\u001b[1G163.5 MiB [] 78% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 79% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 79% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 80% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 81% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 81% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 82% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 82% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 83% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 84% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 85% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 86% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 86% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 87% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 88% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 89% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 89% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 90% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 90% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 91% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 92% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 92% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 93% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 93% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 94% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 95% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 96% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 97% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 98% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 99% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 100% 0.0s\u001b[0K\n","Chromium 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium-1155\n","Downloading Chromium Headless Shell 133.0.6943.16 (playwright build v1155)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-headless-shell-linux.zip\u001b[22m\n","\u001b[1G99.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 0% 38.0s\u001b[0K\u001b[1G99.9 MiB [] 0% 20.9s\u001b[0K\u001b[1G99.9 MiB [] 0% 14.0s\u001b[0K\u001b[1G99.9 MiB [] 0% 7.4s\u001b[0K\u001b[1G99.9 MiB [] 1% 5.4s\u001b[0K\u001b[1G99.9 MiB [] 1% 5.3s\u001b[0K\u001b[1G99.9 MiB [] 2% 5.0s\u001b[0K\u001b[1G99.9 MiB [] 2% 5.4s\u001b[0K\u001b[1G99.9 MiB [] 3% 4.7s\u001b[0K\u001b[1G99.9 MiB [] 3% 4.4s\u001b[0K\u001b[1G99.9 MiB [] 4% 4.1s\u001b[0K\u001b[1G99.9 MiB [] 5% 3.8s\u001b[0K\u001b[1G99.9 MiB [] 5% 3.5s\u001b[0K\u001b[1G99.9 MiB [] 6% 3.4s\u001b[0K\u001b[1G99.9 MiB [] 7% 3.4s\u001b[0K\u001b[1G99.9 MiB [] 7% 3.5s\u001b[0K\u001b[1G99.9 MiB [] 8% 3.3s\u001b[0K\u001b[1G99.9 MiB [] 8% 3.2s\u001b[0K\u001b[1G99.9 MiB [] 9% 3.2s\u001b[0K\u001b[1G99.9 MiB [] 10% 3.1s\u001b[0K\u001b[1G99.9 MiB [] 10% 3.2s\u001b[0K\u001b[1G99.9 MiB [] 10% 3.3s\u001b[0K\u001b[1G99.9 MiB [] 11% 3.2s\u001b[0K\u001b[1G99.9 MiB [] 11% 3.1s\u001b[0K\u001b[1G99.9 MiB [] 12% 3.0s\u001b[0K\u001b[1G99.9 MiB [] 13% 2.9s\u001b[0K\u001b[1G99.9 MiB [] 14% 2.8s\u001b[0K\u001b[1G99.9 MiB [] 15% 2.8s\u001b[0K\u001b[1G99.9 MiB [] 16% 2.8s\u001b[0K\u001b[1G99.9 MiB [] 17% 2.8s\u001b[0K\u001b[1G99.9 MiB [] 18% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 19% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 19% 2.6s\u001b[0K\u001b[1G99.9 MiB [] 20% 2.6s\u001b[0K\u001b[1G99.9 MiB [] 21% 2.5s\u001b[0K\u001b[1G99.9 MiB [] 22% 2.5s\u001b[0K\u001b[1G99.9 MiB [] 22% 2.6s\u001b[0K\u001b[1G99.9 MiB [] 23% 2.6s\u001b[0K\u001b[1G99.9 MiB [] 23% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 24% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 25% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 26% 2.7s\u001b[0K\u001b[1G99.9 MiB [] 27% 2.6s\u001b[0K\u001b[1G99.9 MiB [] 28% 2.5s\u001b[0K\u001b[1G99.9 MiB [] 29% 2.4s\u001b[0K\u001b[1G99.9 MiB [] 30% 2.4s\u001b[0K\u001b[1G99.9 MiB [] 30% 2.5s\u001b[0K\u001b[1G99.9 MiB [] 30% 2.4s\u001b[0K\u001b[1G99.9 MiB [] 31% 2.4s\u001b[0K\u001b[1G99.9 MiB [] 32% 2.4s\u001b[0K\u001b[1G99.9 MiB [] 33% 2.4s\u001b[0K\u001b[1G99.9 MiB [] 33% 2.3s\u001b[0K\u001b[1G99.9 MiB [] 34% 2.3s\u001b[0K\u001b[1G99.9 MiB [] 35% 2.3s\u001b[0K\u001b[1G99.9 MiB [] 35% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 36% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 37% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 38% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 38% 2.1s\u001b[0K\u001b[1G99.9 MiB [] 39% 2.1s\u001b[0K\u001b[1G99.9 MiB [] 40% 2.0s\u001b[0K\u001b[1G99.9 MiB [] 41% 2.0s\u001b[0K\u001b[1G99.9 MiB [] 42% 2.0s\u001b[0K\u001b[1G99.9 MiB [] 43% 2.0s\u001b[0K\u001b[1G99.9 MiB [] 43% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 44% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 45% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 46% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 47% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 48% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 49% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 50% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 51% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 52% 1.7s\u001b[0K\u001b[1G99.9 MiB [] 53% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 54% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 55% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 56% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 57% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 58% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 58% 1.4s\u001b[0K\u001b[1G99.9 MiB [] 59% 1.4s\u001b[0K\u001b[1G99.9 MiB [] 60% 1.4s\u001b[0K\u001b[1G99.9 MiB [] 60% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 61% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 62% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 63% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 64% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 65% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 66% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 67% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 68% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 68% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 69% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 70% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 71% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 72% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 73% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 74% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 75% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 76% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 77% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 78% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 79% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 80% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 80% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 81% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 82% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 83% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 83% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 84% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 85% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 86% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 86% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 87% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 88% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 89% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 89% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 90% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 91% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 92% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 92% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 94% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 96% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 97% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 98% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 100% 0.0s\u001b[0K\n","Chromium Headless Shell 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1155\n","Downloading Firefox 134.0 (playwright build v1471)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1471/firefox-ubuntu-22.04.zip\u001b[22m\n","\u001b[1G88.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 27.8s\u001b[0K\u001b[1G88.7 MiB [] 0% 13.4s\u001b[0K\u001b[1G88.7 MiB [] 0% 7.9s\u001b[0K\u001b[1G88.7 MiB [] 1% 6.2s\u001b[0K\u001b[1G88.7 MiB [] 1% 5.7s\u001b[0K\u001b[1G88.7 MiB [] 1% 4.8s\u001b[0K\u001b[1G88.7 MiB [] 2% 4.3s\u001b[0K\u001b[1G88.7 MiB [] 3% 4.0s\u001b[0K\u001b[1G88.7 MiB [] 3% 3.8s\u001b[0K\u001b[1G88.7 MiB [] 4% 3.5s\u001b[0K\u001b[1G88.7 MiB [] 5% 3.2s\u001b[0K\u001b[1G88.7 MiB [] 6% 2.9s\u001b[0K\u001b[1G88.7 MiB [] 7% 2.7s\u001b[0K\u001b[1G88.7 MiB [] 7% 2.9s\u001b[0K\u001b[1G88.7 MiB [] 8% 2.8s\u001b[0K\u001b[1G88.7 MiB [] 9% 2.8s\u001b[0K\u001b[1G88.7 MiB [] 10% 2.7s\u001b[0K\u001b[1G88.7 MiB [] 10% 2.8s\u001b[0K\u001b[1G88.7 MiB [] 11% 2.9s\u001b[0K\u001b[1G88.7 MiB [] 11% 3.0s\u001b[0K\u001b[1G88.7 MiB [] 11% 3.1s\u001b[0K\u001b[1G88.7 MiB [] 12% 3.1s\u001b[0K\u001b[1G88.7 MiB [] 12% 3.3s\u001b[0K\u001b[1G88.7 MiB [] 12% 3.4s\u001b[0K\u001b[1G88.7 MiB [] 13% 3.6s\u001b[0K\u001b[1G88.7 MiB [] 14% 3.5s\u001b[0K\u001b[1G88.7 MiB [] 15% 3.5s\u001b[0K\u001b[1G88.7 MiB [] 16% 3.4s\u001b[0K\u001b[1G88.7 MiB [] 16% 3.3s\u001b[0K\u001b[1G88.7 MiB [] 17% 3.4s\u001b[0K\u001b[1G88.7 MiB [] 18% 3.3s\u001b[0K\u001b[1G88.7 MiB [] 19% 3.2s\u001b[0K\u001b[1G88.7 MiB [] 20% 3.1s\u001b[0K\u001b[1G88.7 MiB [] 20% 3.0s\u001b[0K\u001b[1G88.7 MiB [] 21% 2.9s\u001b[0K\u001b[1G88.7 MiB [] 23% 2.7s\u001b[0K\u001b[1G88.7 MiB [] 24% 2.6s\u001b[0K\u001b[1G88.7 MiB [] 25% 2.5s\u001b[0K\u001b[1G88.7 MiB [] 26% 2.4s\u001b[0K\u001b[1G88.7 MiB [] 27% 2.4s\u001b[0K\u001b[1G88.7 MiB [] 28% 2.3s\u001b[0K\u001b[1G88.7 MiB [] 29% 2.2s\u001b[0K\u001b[1G88.7 MiB [] 30% 2.2s\u001b[0K\u001b[1G88.7 MiB [] 31% 2.1s\u001b[0K\u001b[1G88.7 MiB [] 32% 2.0s\u001b[0K\u001b[1G88.7 MiB [] 33% 2.0s\u001b[0K\u001b[1G88.7 MiB [] 34% 2.0s\u001b[0K\u001b[1G88.7 MiB [] 35% 1.9s\u001b[0K\u001b[1G88.7 MiB [] 36% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 37% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 38% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 39% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 40% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 41% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 42% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 43% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 44% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 45% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 46% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 47% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 48% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 49% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 51% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 51% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 52% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 54% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 55% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 56% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 57% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 58% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 59% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 61% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 62% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 63% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 64% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 65% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 66% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 67% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 68% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 69% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 70% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 72% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 73% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 73% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 74% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 75% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 76% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 77% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 78% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 79% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 80% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 81% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 82% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 84% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 85% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 89% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 100% 0.0s\u001b[0K\n","Firefox 134.0 (playwright build v1471) downloaded to /root/.cache/ms-playwright/firefox-1471\n","Downloading Webkit 18.2 (playwright build v2123)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2123/webkit-ubuntu-22.04.zip\u001b[22m\n","\u001b[1G89.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 0% 192.2s\u001b[0K\u001b[1G89.2 MiB [] 0% 168.6s\u001b[0K\u001b[1G89.2 MiB [] 0% 66.9s\u001b[0K\u001b[1G89.2 MiB [] 0% 38.7s\u001b[0K\u001b[1G89.2 MiB [] 0% 30.3s\u001b[0K\u001b[1G89.2 MiB [] 0% 23.1s\u001b[0K\u001b[1G89.2 MiB [] 0% 20.2s\u001b[0K\u001b[1G89.2 MiB [] 0% 18.9s\u001b[0K\u001b[1G89.2 MiB [] 1% 16.9s\u001b[0K\u001b[1G89.2 MiB [] 1% 16.0s\u001b[0K\u001b[1G89.2 MiB [] 1% 16.1s\u001b[0K\u001b[1G89.2 MiB [] 1% 15.7s\u001b[0K\u001b[1G89.2 MiB [] 1% 15.2s\u001b[0K\u001b[1G89.2 MiB [] 2% 15.0s\u001b[0K\u001b[1G89.2 MiB [] 2% 14.2s\u001b[0K\u001b[1G89.2 MiB [] 2% 13.6s\u001b[0K\u001b[1G89.2 MiB [] 2% 13.2s\u001b[0K\u001b[1G89.2 MiB [] 2% 12.8s\u001b[0K\u001b[1G89.2 MiB [] 3% 12.5s\u001b[0K\u001b[1G89.2 MiB [] 3% 12.0s\u001b[0K\u001b[1G89.2 MiB [] 3% 11.9s\u001b[0K\u001b[1G89.2 MiB [] 3% 10.8s\u001b[0K\u001b[1G89.2 MiB [] 4% 11.2s\u001b[0K\u001b[1G89.2 MiB [] 4% 11.0s\u001b[0K\u001b[1G89.2 MiB [] 4% 10.9s\u001b[0K\u001b[1G89.2 MiB [] 4% 10.8s\u001b[0K\u001b[1G89.2 MiB [] 4% 11.0s\u001b[0K\u001b[1G89.2 MiB [] 5% 10.7s\u001b[0K\u001b[1G89.2 MiB [] 5% 10.5s\u001b[0K\u001b[1G89.2 MiB [] 5% 10.4s\u001b[0K\u001b[1G89.2 MiB [] 5% 10.6s\u001b[0K\u001b[1G89.2 MiB [] 6% 10.2s\u001b[0K\u001b[1G89.2 MiB [] 6% 10.1s\u001b[0K\u001b[1G89.2 MiB [] 6% 9.7s\u001b[0K\u001b[1G89.2 MiB [] 7% 9.8s\u001b[0K\u001b[1G89.2 MiB [] 7% 9.9s\u001b[0K\u001b[1G89.2 MiB [] 7% 9.7s\u001b[0K\u001b[1G89.2 MiB [] 7% 9.5s\u001b[0K\u001b[1G89.2 MiB [] 8% 9.3s\u001b[0K\u001b[1G89.2 MiB [] 8% 9.2s\u001b[0K\u001b[1G89.2 MiB [] 8% 9.1s\u001b[0K\u001b[1G89.2 MiB [] 9% 8.9s\u001b[0K\u001b[1G89.2 MiB [] 9% 8.8s\u001b[0K\u001b[1G89.2 MiB [] 9% 8.7s\u001b[0K\u001b[1G89.2 MiB [] 10% 8.7s\u001b[0K\u001b[1G89.2 MiB [] 10% 8.5s\u001b[0K\u001b[1G89.2 MiB [] 10% 8.6s\u001b[0K\u001b[1G89.2 MiB [] 11% 8.6s\u001b[0K\u001b[1G89.2 MiB [] 11% 8.5s\u001b[0K\u001b[1G89.2 MiB [] 11% 8.7s\u001b[0K\u001b[1G89.2 MiB [] 12% 8.9s\u001b[0K\u001b[1G89.2 MiB [] 12% 8.8s\u001b[0K\u001b[1G89.2 MiB [] 12% 8.7s\u001b[0K\u001b[1G89.2 MiB [] 13% 8.6s\u001b[0K\u001b[1G89.2 MiB [] 14% 8.3s\u001b[0K\u001b[1G89.2 MiB [] 14% 8.2s\u001b[0K\u001b[1G89.2 MiB [] 14% 8.0s\u001b[0K\u001b[1G89.2 MiB [] 15% 7.9s\u001b[0K\u001b[1G89.2 MiB [] 15% 7.7s\u001b[0K\u001b[1G89.2 MiB [] 16% 7.5s\u001b[0K\u001b[1G89.2 MiB [] 16% 7.4s\u001b[0K\u001b[1G89.2 MiB [] 16% 7.3s\u001b[0K\u001b[1G89.2 MiB [] 16% 7.4s\u001b[0K\u001b[1G89.2 MiB [] 17% 7.2s\u001b[0K\u001b[1G89.2 MiB [] 17% 7.3s\u001b[0K\u001b[1G89.2 MiB [] 17% 7.2s\u001b[0K\u001b[1G89.2 MiB [] 18% 7.2s\u001b[0K\u001b[1G89.2 MiB [] 18% 7.1s\u001b[0K\u001b[1G89.2 MiB [] 19% 7.1s\u001b[0K\u001b[1G89.2 MiB [] 19% 7.0s\u001b[0K\u001b[1G89.2 MiB [] 19% 6.9s\u001b[0K\u001b[1G89.2 MiB [] 20% 6.9s\u001b[0K\u001b[1G89.2 MiB [] 20% 6.8s\u001b[0K\u001b[1G89.2 MiB [] 21% 6.7s\u001b[0K\u001b[1G89.2 MiB [] 21% 6.6s\u001b[0K\u001b[1G89.2 MiB [] 22% 6.4s\u001b[0K\u001b[1G89.2 MiB [] 23% 6.4s\u001b[0K\u001b[1G89.2 MiB [] 23% 6.3s\u001b[0K\u001b[1G89.2 MiB [] 23% 6.2s\u001b[0K\u001b[1G89.2 MiB [] 24% 6.1s\u001b[0K\u001b[1G89.2 MiB [] 25% 6.1s\u001b[0K\u001b[1G89.2 MiB [] 26% 6.0s\u001b[0K\u001b[1G89.2 MiB [] 26% 5.9s\u001b[0K\u001b[1G89.2 MiB [] 27% 5.8s\u001b[0K\u001b[1G89.2 MiB [] 28% 5.7s\u001b[0K\u001b[1G89.2 MiB [] 28% 5.6s\u001b[0K\u001b[1G89.2 MiB [] 29% 5.6s\u001b[0K\u001b[1G89.2 MiB [] 29% 5.5s\u001b[0K\u001b[1G89.2 MiB [] 30% 5.4s\u001b[0K\u001b[1G89.2 MiB [] 30% 5.3s\u001b[0K\u001b[1G89.2 MiB [] 31% 5.3s\u001b[0K\u001b[1G89.2 MiB [] 31% 5.2s\u001b[0K\u001b[1G89.2 MiB [] 32% 5.2s\u001b[0K\u001b[1G89.2 MiB [] 32% 5.1s\u001b[0K\u001b[1G89.2 MiB [] 33% 5.1s\u001b[0K\u001b[1G89.2 MiB [] 33% 5.0s\u001b[0K\u001b[1G89.2 MiB [] 34% 4.9s\u001b[0K\u001b[1G89.2 MiB [] 34% 4.8s\u001b[0K\u001b[1G89.2 MiB [] 35% 4.8s\u001b[0K\u001b[1G89.2 MiB [] 35% 4.7s\u001b[0K\u001b[1G89.2 MiB [] 36% 4.6s\u001b[0K\u001b[1G89.2 MiB [] 37% 4.5s\u001b[0K\u001b[1G89.2 MiB [] 37% 4.4s\u001b[0K\u001b[1G89.2 MiB [] 38% 4.4s\u001b[0K\u001b[1G89.2 MiB [] 38% 4.3s\u001b[0K\u001b[1G89.2 MiB [] 39% 4.3s\u001b[0K\u001b[1G89.2 MiB [] 39% 4.2s\u001b[0K\u001b[1G89.2 MiB [] 40% 4.2s\u001b[0K\u001b[1G89.2 MiB [] 40% 4.1s\u001b[0K\u001b[1G89.2 MiB [] 41% 4.1s\u001b[0K\u001b[1G89.2 MiB [] 42% 4.0s\u001b[0K\u001b[1G89.2 MiB [] 42% 3.9s\u001b[0K\u001b[1G89.2 MiB [] 43% 3.9s\u001b[0K\u001b[1G89.2 MiB [] 44% 3.8s\u001b[0K\u001b[1G89.2 MiB [] 45% 3.7s\u001b[0K\u001b[1G89.2 MiB [] 46% 3.6s\u001b[0K\u001b[1G89.2 MiB [] 47% 3.5s\u001b[0K\u001b[1G89.2 MiB [] 48% 3.5s\u001b[0K\u001b[1G89.2 MiB [] 48% 3.4s\u001b[0K\u001b[1G89.2 MiB [] 49% 3.3s\u001b[0K\u001b[1G89.2 MiB [] 50% 3.2s\u001b[0K\u001b[1G89.2 MiB [] 51% 3.1s\u001b[0K\u001b[1G89.2 MiB [] 52% 3.1s\u001b[0K\u001b[1G89.2 MiB [] 52% 3.0s\u001b[0K\u001b[1G89.2 MiB [] 53% 3.0s\u001b[0K\u001b[1G89.2 MiB [] 54% 2.8s\u001b[0K\u001b[1G89.2 MiB [] 55% 2.8s\u001b[0K\u001b[1G89.2 MiB [] 56% 2.7s\u001b[0K\u001b[1G89.2 MiB [] 57% 2.6s\u001b[0K\u001b[1G89.2 MiB [] 58% 2.5s\u001b[0K\u001b[1G89.2 MiB [] 60% 2.4s\u001b[0K\u001b[1G89.2 MiB [] 60% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 61% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 62% 2.2s\u001b[0K\u001b[1G89.2 MiB [] 63% 2.1s\u001b[0K\u001b[1G89.2 MiB [] 64% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 65% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 66% 1.9s\u001b[0K\u001b[1G89.2 MiB [] 67% 1.8s\u001b[0K\u001b[1G89.2 MiB [] 68% 1.8s\u001b[0K\u001b[1G89.2 MiB [] 68% 1.7s\u001b[0K\u001b[1G89.2 MiB [] 69% 1.7s\u001b[0K\u001b[1G89.2 MiB [] 70% 1.6s\u001b[0K\u001b[1G89.2 MiB [] 71% 1.6s\u001b[0K\u001b[1G89.2 MiB [] 72% 1.5s\u001b[0K\u001b[1G89.2 MiB [] 73% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 74% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 74% 1.3s\u001b[0K\u001b[1G89.2 MiB [] 75% 1.3s\u001b[0K\u001b[1G89.2 MiB [] 76% 1.3s\u001b[0K\u001b[1G89.2 MiB [] 77% 1.2s\u001b[0K\u001b[1G89.2 MiB [] 78% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 79% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 80% 1.0s\u001b[0K\u001b[1G89.2 MiB [] 81% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 82% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 83% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 85% 0.7s\u001b[0K\u001b[1G89.2 MiB [] 86% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 87% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 88% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 89% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 90% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 90% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 91% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 92% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 92% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 93% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 94% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 96% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 96% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 98% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 98% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 100% 0.0s\u001b[0K\n","Webkit 18.2 (playwright build v2123) downloaded to /root/.cache/ms-playwright/webkit-2123\n","Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n","\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 0.6s\u001b[0K\u001b[1G2.3 MiB [] 7% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 22% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 28% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 59% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 82% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n","FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n","Playwright Host validation warning: \n","‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n","‚ïë Host system is missing dependencies to run browsers. ‚ïë\n","‚ïë Missing libraries:                                   ‚ïë\n","‚ïë     libgtk-4.so.1                                    ‚ïë\n","‚ïë     libgraphene-1.0.so.0                             ‚ïë\n","‚ïë     libwoff2dec.so.1.0.2                             ‚ïë\n","‚ïë     libgstgl-1.0.so.0                                ‚ïë\n","‚ïë     libgstcodecparsers-1.0.so.0                      ‚ïë\n","‚ïë     libavif.so.13                                    ‚ïë\n","‚ïë     libharfbuzz-icu.so.0                             ‚ïë\n","‚ïë     libenchant-2.so.2                                ‚ïë\n","‚ïë     libsecret-1.so.0                                 ‚ïë\n","‚ïë     libhyphen.so.0                                   ‚ïë\n","‚ïë     libmanette-0.2.so.0                              ‚ïë\n","‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n","    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n","\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n","    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:865:43)\n","    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:963:7)\n","    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:952:43)\n","    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n","Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,315 kB]\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,911 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,655 kB]\n","Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,674 kB]\n","Fetched 17.5 MB in 2s (7,871 kB/s)\n"]}]},{"cell_type":"markdown","source":["# üñºframeworks"],"metadata":{"id":"Ew8QRBDOSq9H"}},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, Comment\n","from urllib.parse import urljoin, urlparse\n","import chardet\n","\n","import dotenv\n","# from pyseoanalyzer import analyze\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import openai\n","import time\n","\n","import re\n","import json\n","\n","from collections import Counter\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import ast\n","\n","import numpy as np\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","import string\n","\n","import seaborn as sns\n","\n","from wordcloud import WordCloud\n","\n","from jinja2 import Template\n","\n","import asyncio\n","from playwright.async_api import async_playwright\n","import spacy\n","import pypandoc"],"metadata":{"id":"SjcMjba1T1Cc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ© push to github"],"metadata":{"id":"o38bsnD-EWcq"}},{"cell_type":"code","source":["#%%capture\n","notebookname = 'SEO.ipynb'\n","\n","class github:\n","    def __init__(self, github_pat, github_email, github_username, github_repo, gdrive_notebook_folder, notebook_name):\n","        self.github_pat = userdata.get(github_pat)\n","        self.github_email = userdata.get(github_email)\n","        self.github_username = userdata.get(github_username)\n","        self.github_repo = userdata.get(github_repo)\n","        self.gdrive_notebook_folder = userdata.get(gdrive_notebook_folder)\n","        self.notebook_name = notebook_name\n","\n","    def clone_repo(self):\n","        # Source file path in Google Drive\n","        source_file_path = f\"/content/drive/MyDrive/{self.gdrive_notebook_folder}/{self.notebook_name}\"\n","\n","        # Repository details\n","        repo_url = f'https://{self.github_pat}@github.com/{self.github_username}/{self.github_repo}.git'\n","\n","        # Clone the private repository\n","        !git clone {repo_url} cloned-repo\n","        os.chdir('cloned-repo')  # Switch to the cloned repository\n","\n","        # Ensure the file exists in Google Drive\n","        if os.path.exists(source_file_path):\n","            # Copy the notebook into the cloned repository\n","            !cp \"{source_file_path}\" ./\n","        else:\n","            print(f\"The file {source_file_path} was not found.\")\n","            return  # Exit if the file doesn't exist\n","\n","        # Git configuration\n","        !git config user.email \"{self.github_email}\"\n","        !git config user.name \"{self.github_username}\"\n","\n","        # Add the file to Git\n","        !git add \"{self.notebook_name}\"\n","\n","        # Commit the changes\n","        !git commit -m \"Added {self.notebook_name} from Google Drive\"\n","\n","        # Push to the repository\n","        !git push origin main\n","\n","        # Wechsle zur√ºck ins √ºbergeordnete Verzeichnis und l√∂sche cloned-repo\n","        os.chdir('..')\n","        !rm -rf cloned-repo\n","        print(\"cloned-repo wurde wieder gel√∂scht.\")\n","\n","\n","\n","# Clone, add, and push the notebook\n","clone_2 = github('github_pat', 'github_email', 'github_username', 'github_repo_seo', 'gdrive_seo_folder', notebookname)\n","clone_2.clone_repo()\n"],"metadata":{"id":"p-LWoIwDfU3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üï∏ scrap"],"metadata":{"id":"vNVT-lr5jejz"}},{"cell_type":"code","source":["class WebsiteScraper:\n","    \"\"\"\n","    Diese Klasse k√ºmmert sich ausschlie√ülich um das Sammeln und Extrahieren\n","    von Texten aus einer Website.\n","    \"\"\"\n","\n","    def __init__(self, start_url=\"https://www.rue-zahnspange.de\", max_pages=50):\n","        \"\"\"\n","        :param start_url: Die Start-URL der Website, z.B. \"https://www.example.com\"\n","        :param max_pages: Maximale Anzahl Seiten, die gecrawlt werden.\n","        \"\"\"\n","        self.start_url = start_url\n","        self.max_pages = max_pages\n","\n","        # Hier speichern wir {URL: reiner_Text}\n","        self.scraped_data = {}\n","\n","    def scrape_website(self):\n","        \"\"\"\n","        Startet den Crawl-Vorgang, gefolgt von der Extraktion des Textes\n","        und dem Sammeln interner Links.\n","        \"\"\"\n","        visited = set()\n","        to_visit = [self.start_url]\n","        domain = urlparse(self.start_url).netloc\n","\n","        while to_visit and len(visited) < self.max_pages:\n","            url = to_visit.pop(0)\n","            if url in visited:\n","                continue\n","            visited.add(url)\n","\n","            try:\n","                response = requests.get(url, timeout=10)\n","\n","                # Rohdaten holen und Encoding per chardet bestimmen\n","                raw_data = response.content\n","                detected = chardet.detect(raw_data)\n","                # Wenn chardet etwas erkennt, nehmen wir das. Sonst Standard \"utf-8\".\n","                encoding = \"utf-8\"\n","                text_data = raw_data.decode(encoding, errors=\"replace\")\n","\n","                # Nur weiterverarbeiten, wenn HTML-Content\n","                if (response.status_code == 200\n","                    and \"text/html\" in response.headers.get(\"Content-Type\", \"\")):\n","                    soup = BeautifulSoup(text_data, \"html.parser\")\n","\n","                    # Text extrahieren\n","                    text = self._extract_text_from_soup(soup)\n","                    self.scraped_data[url] = text\n","\n","                    # Interne Links sammeln\n","                    for link in soup.find_all(\"a\", href=True):\n","                        absolute_link = urljoin(url, link[\"href\"])\n","                        if urlparse(absolute_link).netloc == domain:\n","                            if (absolute_link not in visited\n","                                and absolute_link not in to_visit):\n","                                to_visit.append(absolute_link)\n","\n","            except requests.RequestException as e:\n","                print(f\"Fehler beim Abrufen von {url}:\\n{e}\")\n","\n","    def _extract_text_from_soup(self, soup):\n","        \"\"\"\n","        Extrahiert aus <p>, <h1>, <h2>, <h3>, <li> reinen Text,\n","        aber NICHT die, die in .faq4_question oder .faq4_answer stecken.\n","        Au√üerdem extrahiert er separat die FAQ-Fragen und -Antworten\n","        (faq4_question / faq4_answer), damit wir beide Zeilenumbr√ºche\n","        dort ebenfalls erhalten.\n","        \"\"\"\n","\n","        # 1) Script/Style/Noscript entfernen\n","        for script_or_style in soup([\"script\", \"style\", \"noscript\"]):\n","            script_or_style.decompose()\n","\n","        # 2) Kommentare entfernen\n","        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n","            comment.extract()\n","\n","        # 3) Normale Texte (p, h1, h2, h3, li), ABER nicht innerhalb von .faq4_question / .faq4_answer\n","        texts = []\n","        all_normal_tags = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"])\n","        for tag in all_normal_tags:\n","            # Pr√ºfen, ob das Tag einen Vorfahren hat mit Klasse faq4_question oder faq4_answer\n","            if tag.find_parent(class_=\"faq4_question\") or tag.find_parent(class_=\"faq4_answer\"):\n","                continue\n","\n","            # Hier wichtig: separator=\"\\n\", strip=False, damit wir Zeilenumbr√ºche behalten\n","            txt = tag.get_text(separator=\"\\n\", strip=False)\n","            # Evtl. willst du doppelte Leerzeilen bereinigen. Das kannst du optional tun.\n","            if txt.strip():\n","                texts.append(txt.strip(\"\\r\\n\"))\n","\n","        # 4) FAQ-Bereiche (Fragen + Antworten)\n","        questions = soup.select(\".faq4_question\")\n","        answers = soup.select(\".faq4_answer\")\n","\n","        # 5) Zusammenf√ºhren (Frage + Antwort)\n","        for q, a in zip(questions, answers):\n","            q_text = q.get_text(separator=\"\\n\", strip=False)\n","            a_text = a.get_text(separator=\"\\n\", strip=False)\n","            q_text = q_text.strip(\"\\r\\n\")\n","            a_text = a_text.strip(\"\\r\\n\")\n","            if q_text and a_text:\n","                combined = f\"Frage: {q_text}\\nAntwort: {a_text}\"\n","                texts.append(combined)\n","\n","        # 6) Als String zur√ºckgeben. Wir trennen die einzelnen Elemente durch \"\\n\\n\"\n","        #    (kannst du je nach Wunsch anpassen)\n","        return \"\\n\\n\".join(texts)\n","\n","    def get_scraped_data(self):\n","        \"\"\"\n","        Gibt das Dictionary {URL: Text} zur√ºck.\n","        Du kannst damit arbeiten, Seiten filtern, etc.\n","        \"\"\"\n","        return self.scraped_data\n"],"metadata":{"id":"lCvWJLpBajRY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Bajdm9YW8Y0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìäSEO Analysis 1"],"metadata":{"id":"FA2RIeko9reT"}},{"cell_type":"code","source":["%%capture\n","\n","\n","url = \"https://www.rue-zahnspange.de\"\n","report = analyze(url)\n","\n","# 'report' enth√§lt nun s√§mtliche Analyseergebnisse\n","print(report)\n"],"metadata":{"id":"vK6jH0qmasx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-xmnF1Yyphji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oAeeJEjYphb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4VkvKAOfphUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tats√§chlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Sortieren nach H√§ufigkeit absteigend\n","df_keywords.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Nur die Top 10 Keywords anzeigen\n","df_top10 = df_keywords.head(44)\n","\n","# Einfaches Balkendiagramm\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_top10['word'], df_top10[\"count\"], color=\"skyblue\")\n","plt.xticks(rotation=45, ha=\"right\")\n","plt.title(\"Top 10 Keywords\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"H√§ufigkeit\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"nW_LgHd1Wtie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","len(list(df_keywords['word']))"],"metadata":{"id":"1WWQvbUIPIwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ci_GtO9qPIt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N5oTB61wPIrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VcE0WgKKPIo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","\n","# Beispiel: Wir holen uns das Keyword-Dictionary\n","keywords_dict = report.get(\"keywords\", {})\n","\n","# Wenn es tats√§chlich ein dict ist: {\"keyword1\": count, \"keyword2\": count, ...}\n","df_keywords = pd.DataFrame(keywords_dict)\n","\n","# Neue Spalte mit Kleinbuchstaben erzeugen\n","df_keywords[\"word_lower\"] = df_keywords[\"word\"].str.lower()\n","\n","# Die Keywords, nach denen du suchen m√∂chtest (Case-insensitive)\n","interesting_keywords = [\"zahnspange\", \"Invisalign\", \"kieferorthop√§die\", \"BEHANDLUNG\", \"Kosten\" , \"z√§hne\", \"brackets\", \"Unsichtbar\", \"Kinder\", \"Jugendliche\", \"Patienten\", \"l√§cheln\", \"zeitnahen\"]\n","# Auch diese in Kleinbuchstaben umwandeln\n","interesting_keywords_lower = [kw.lower() for kw in interesting_keywords]\n","\n","# DataFrame nach den gew√ºnschten Keywords filtern (Case-insensitive)\n","df_subset = df_keywords.loc[df_keywords[\"word_lower\"].isin(interesting_keywords_lower)].copy()\n","\n","# Sortieren nach H√§ufigkeit, damit das Diagramm √ºbersichtlicher wird\n","df_subset.sort_values(\"count\", ascending=False, inplace=True)\n","\n","# Balkendiagramm\n","plt.figure(figsize=(8, 5))\n","# Plotten kannst du z.B. weiterhin den Originalwert \"word\" (falls du im Diagramm\n","# die urspr√ºngliche Schreibweise sehen willst)\n","plt.bar(df_subset[\"word\"], df_subset[\"count\"], color=\"steelblue\")\n","plt.title(\"H√§ufigkeit ausgew√§hlter Keywords (Case-Insensitive)\")\n","plt.xlabel(\"Keyword\")\n","plt.ylabel(\"Anzahl\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"p3q7KVe8j5Gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","pages = report.get(\"pages\", [])\n","\n","df_pages = pd.DataFrame(pages)\n","\n","# Beispiel: Title-L√§nge berechnen\n","df_pages[\"title_length\"] = df_pages[\"title\"].apply(lambda x: len(x) if isinstance(x, str) else 0)\n","\n","# Balkendiagramm Title-L√§nge\n","plt.figure(figsize=(10, 6))\n","plt.bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","plt.xticks(rotation=90)\n","plt.title(\"L√§nge der Title-Tags pro Seite\")\n","plt.xlabel(\"URL\")\n","plt.ylabel(\"Title-L√§nge (Zeichen)\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"bH80vXrlj5D5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1 Zeile, 2 Spalten\n","\n","# (a) Keywords\n","axes[0].bar(df_top10[\"word\"].head(22), df_top10[\"count\"].head(22), color=\"skyblue\")\n","axes[0].set_title(\"Top 10 Keywords\")\n","axes[0].set_xticklabels(df_top10[\"word\"], rotation=45, ha=\"right\")\n","\n","# (b) Title-L√§ngen\n","axes[1].bar(df_pages[\"url\"], df_pages[\"title_length\"], color=\"green\")\n","axes[1].set_title(\"Title-L√§nge pro Seite\")\n","axes[1].set_xticklabels(df_pages[\"url\"], rotation=90)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"WGcvHhIAj5BO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I94K8prmj4-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCO3w_TOj47i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ü§ñ chatbot"],"metadata":{"id":"E0k0HmUZHmkg"}},{"cell_type":"code","source":["\n","os.environ['OPENAI_API_KEY'] = userdata.get('open_ai_api_key')\n","\n","class Chatbot:\n","    \"\"\"\n","    Diese Chatbot-Klasse nutzt die neue Methode client.chat.completions.create()\n","    aus openai>=1.0.0 √ºber openai.OpenAI().\n","    \"\"\"\n","\n","    def __init__(self, systemprompt, prompt):\n","        self.client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n","        self.systemprompt = systemprompt\n","        self.prompt = prompt\n","        self.context = [{\"role\": \"system\", \"content\": systemprompt}]\n","        self.model = \"gpt-4o-mini-2024-07-18\"\n","\n","    def chat(self):\n","        \"\"\"\n","        Sendet den Prompt an das Chat-Interface und gibt den kompletten Antwort-String zur√ºck.\n","        \"\"\"\n","        self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","        try:\n","            response = self.client.chat.completions.create(\n","                model=self.model,\n","                messages=self.context\n","            )\n","            response_content = response.choices[0].message.content\n","            self.context.append({\"role\": \"assistant\", \"content\": response_content})\n","            return response_content\n","        except Exception as e:\n","            print(f\"Fehler bei der OpenAI-Anfrage: {e}\")\n","            return \"\"\n","\n","\n","    def chat_with_streaming(self):\n","            \"\"\"\n","            Interagiert mit OpenAI Chat Completion API und streamt die Antwort.\n","            \"\"\"\n","            # Nachricht zur Konversation hinzuf√ºgen\n","            self.context.append({\"role\": \"user\", \"content\": self.prompt})\n","\n","\n","            try:\n","                # Streaming-Option aktivieren\n","                response = self.client.chat.completions.create(\n","                    model=self.model,\n","                    messages=self.context,\n","                    stream=True\n","                )\n","\n","                streamed_content = \"\"  # Zum Speichern der gestreamten Antwort\n","\n","                for chunk in response:\n","                    # Debugging: Anzeigen, was tats√§chlich in jedem Chunk enthalten ist\n","                    delta = chunk.choices[0].delta\n","                    content = getattr(delta, \"content\", \"\")\n","\n","                    if content:  # Verarbeite nur nicht-leere Inhalte\n","                        print(content, end=\"\", flush=True)\n","                        streamed_content += content\n","\n","                print()  # Neue Zeile am Ende\n","\n","                # Gestreamte Antwort zur Konversation hinzuf√ºgen\n","                self.context.append({\"role\": \"assistant\", \"content\": streamed_content})\n","\n","                # Return the streamed content\n","                return streamed_content # This line was added\n","\n","            except Exception as e:\n","                print(f\"\\nDEBUG: An error occurred during streaming: {e}\")\n","                # Return empty string in case of error\n","                return \"\" # This line was added\n"],"metadata":{"id":"j6ZZeUIRdYhy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üÜé NLP"],"metadata":{"id":"qGy_OuMyIC9C"}},{"cell_type":"code","source":["def chunk_text(text, max_tokens=10000):\n","    \"\"\"\n","    Teilt den Text in Bl√∂cke auf, damit er nicht zu lang\n","    f√ºr die OpenAI-API wird.\n","    Hier sehr vereinfacht: 1 Token ~ 4 Zeichen.\n","    \"\"\"\n","    chunks = []\n","    approx_char_limit = max_tokens * 4\n","    start = 0\n","    while start < len(text):\n","        end = start + approx_char_limit\n","        chunk = text[start:end]\n","        chunks.append(chunk)\n","        start = end\n","    return chunks\n"],"metadata":{"id":"3oUna7Bodcp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆWIP seo keywords analysis + plot"],"metadata":{"id":"qK7cW2yRWDnV"}},{"cell_type":"code","source":[],"metadata":{"id":"SpmVBRbZWU6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DLe1UdbKWU38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))"],"metadata":{"id":"Tw_XtEXMWU1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ze2bee_pWUyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uBT6tqO1WUwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FGIbOqkxWUtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆkeywords + Stadt"],"metadata":{"id":"cgPAMHODtAuZ"}},{"cell_type":"code","source":["systemprompt_keywords = (\"\"\"\n","    Du bist ein intelligentes KI-System, das auf die Generierung von SEO-Keywords spezialisiert ist.\n","    Der Benutzer wird dir den kompletten Text von einer gescrapten website aus einem webcrawler vorgeben.\n","    Deine Aufgabe ist es, den Text zu interpretieren und eine Liste von SEO-Keywords basierend auf diesem Input zu erstellen.\n","    Du sollst zu dem Text passende SEO-Keywords finden. Auf Basis dieser keywords soll sp√§ter der Text optimiert werden.\n","    Stelle sicher, dass die Keywords:\n","\n","    Thematisch relevant sind,\n","    Hohe Suchintention abdecken (Short-Tail und Long-Tail Keywords),\n","    Varianten mit Synonymen oder verwandten Begriffen enthalten,\n","    Erschaffe Keywords, die lokale Ausrichtung enthalten. Das Unternehmen der webesite befindet sich in Essen-R√ºttenscheid, im Ruhrgebiet, im Essener S√ºden.\n","\n","\n","    Struktur f√ºr die Antwort:\n","\n","    Erstelle eine serialisierte Liste von SEO-Keywords.\n","\n","    Beispiel f√ºr eine User-Eingabe:\n","\n","    \"SuperFood, ein Gesch√§ft in K√∂ln, Kalkerstr. 20, verkauft gesunde Ern√§hrung f√ºr Sportler. Ein Sportler muss auf seine Ern√§hrung ganz besonders achten.\"\n","\n","    Beispiel f√ºr eine Ausgabe:\n","\n","    [K√∂ln-Kalk, gesund, SuperFood, Ern√§hrung, Sportler, Ern√§hrungstipps, Sportern√§hrung Rezepte, Fitness, Sport]\n","\n","\n","    Beispiel f√ºr eine User-Eingabe:\n","\n","    \"Dr.med. Wurst, Ihr Arzt f√ºr Allgemeinmedizin in Heilbronn. Wir haben Impfungen und Tabletten gegen alle Krankheiten und Schnupfen.\"\n","    Beispiel f√ºr eine Ausgabe:\n","\n","\n","\n","    [Heilbronn, Dr.med. Wurst, krank, Allgemeinmedizin, Impfung, Medizin, Arzt, Erk√§ltungen, Krankenschein, Blutdruck, Diabetes]\n","\n","\n","\n","\"\"\")\n","\n","\n","def user_prompt_keywords(text):\n","   return f\"\"\"\n","          Du bist online Marketing Experte und arbeitest f√ºr ein erfolgreiches Unternehmen, wo du SEO optimierte Texte von websites erstellst. Du hast ein Talent\n","\n","          f√ºr die Erstellung von SEO-Keywords und deine Vorgesetzten bewundern und lieben dich f√ºr die hochperformanten SEO-Keywords, die du beherrschst.\n","\n","          Deine Aufgabe ist es, thematisch relevante SEO-Keywords zu erstellen, die sowohl Short-Tail als auch Long-Tail Keywords enthalten.\n","          Achte darauf, dass die Keywords Synonyme und verwandte Begriffe ber√ºcksichtigen, sowie lokale Informationen, und f√ºr Suchmaschinenoptimierung geeignet sind.\n","          Beachte das user Verhalten von Menschen, die auf der Suche nach Diensten des Unternehmens sein k√∂nnten. vermeide Fachsprache, die normalen usern nicht gel√§ufig sein k√∂nnte.\n","\n","          Bitte generiere SEO-Keywords f√ºr den folgenden Text:\n","\n","          {text}\n","\n","\n","          Strukturiere deine Antwort folgenderma√üen:\n","          Gib eine Liste von Keywords in pystringsthon zur√ºck, zB ['keyword_1', 'keyword_2']. Gebe sonst nichts zur√ºck, keine Einleitung, keine √úberschrift, keine Zusammenfassung,\n","          nichts ausser der string Liste.\n","\n","\n","          Danke! Mein Job h√§ngt davon ab!\n","          \"\"\"\n"],"metadata":{"id":"Hjy6NgflvCTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["page_text_list = []\n","def prep_text_and_get_keywords():\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","    scraped_data = scraper.get_scraped_data()\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","    filtered_urls = []\n","\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, √ºberspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","\n","        # 3. SEO-Analyse starten (f√ºr gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","        page_text_list.append(page_text)\n","\n","\n","\n","    keyword_list =[]\n","    for text in page_text_list:\n","\n","      cb = Chatbot(systemprompt_keywords, user_prompt_keywords(text))\n","\n","      keyword_list.append(cb.chat())\n","\n","    return keyword_list\n"],"metadata":{"id":"cwzCp7r2z3Dp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_raw = prep_text_and_get_keywords()"],"metadata":{"id":"4-zqgH9o0F7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["page_text_list"],"metadata":{"id":"Eyjo-bvfIkjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und du willst dem user helfen!\", f\"\"\"Sei ein online marketing spezialist mit einem Talent f√ºr SEO (search engine optimization). Du analysierst Texte\n","und erstellst SEO-Keywords f√ºr ein international erfolgreiches Unternehmen. Deine Kollegen bewundern und beneiden dich f√ºr die perfekt passenden SEO keywords, die du kennst!\n","Das hier ist roher Text mit SEO keywords, der von einer website gescrapt wurde.\n","1. Anlysiere diesen Text und extrahiere die wesentlichen 10 SEO-keywords.\n","2. Erarbeite SEO optimierete Versionen der keywords\n","3. F√ºge wichtige fachspezifische keywords hinzu, um eine optimale SEO Performance zu erreichen!\n","4. Gebe eine Liste mit den SEO optinierten keywords als strings zur√ºck, sonst nichts. Keine Einleitung, keine Zusammenfassung, nur die Liste wie zB [keyword_1, keyword_2]\n","\n","\n","Hier kommt die rohe Liste mit keywords:\n","{keywords_raw}\n","\n","\n","\n","\"\"\"\n",")\n","keywords_final = cb.chat()"],"metadata":{"id":"KuJZ1Vz-s-Rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cb = Chatbot(\"Du bist eine intelligente Suchmaschine und willst dem user helfen!\"\n",", f\"\"\"Sei ein Experte f√ºr Textanalyse und Geographie. Du hast eine vertiefte Kenntnis in Stadtgeographie und deine Kollegen lieben dich f√ºr deine umfassendes Wissen √ºber deutschen St√§dte und ihrer Ortsteile.\n","Du liebst es Puzzel und R√§tsel zu l√∂sen und hast einen au√üergew√∂hnlichen Blick f√ºrs Detail.\n","Deine Aufgabe ist es, riesige Textmengen zu anlysieren und die Stadt und den Ortsteil zu finden, die in dem Text versteckt sind. Die Namen von Stadt und Stadtteil k√∂nnen auch in zB Strassennamen versteckt sein oder in den Namen\n","von anderen signifikanten Dingen in der Umgebung des Unternehmens.\n","Es sind Texte von einem website scrap eines Unternehmens. Es k√∂nnen mehr als ein St√§dtenamen auftauchen.\n","Deine Aufgabe ist es, den relevanten St√§dtenamen zu finden. Au√üerdem kann der Name des Stadtteils vorhanden sein, in dem das Unternehmen angesiedelt ist. Suche den auch!\n","Hier ist der Text:\n","\n","{page_text_list}\n","\n","Gebe nun den Namen der relevanten Stadt und des Stadtteils zur√ºck. Gebe den Namen und den Stadtteil zur√ºck, wie zB [K√∂ln-Kalk] wenn der Stadtteil gefunden wurde oder zB [M√ºnster], wenn kein Stadtteil gefunden wurde.\n","\"\"\")\n","\n","stadt = cb.chat()"],"metadata":{"id":"0loHl5Z7xJaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(stadt)\n","print(keywords_final)"],"metadata":{"id":"oenVywpwRUcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"VXS73tNf_Ip0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîÆ main SEO optimize"],"metadata":{"id":"CdWeu44YIs5k"}},{"cell_type":"code","source":["combined_analysis_list = []\n","filtered_urls = []\n","\n","def main():\n","    # 1. SCRAPING\n","    start_url = \"https://www.rue-zahnspange.de/\"\n","    scraper = WebsiteScraper(start_url=start_url, max_pages=20)\n","    scraper.scrape_website()\n","\n","    # Alle gescrapten Daten abrufen\n","    scraped_data = scraper.get_scraped_data()\n","\n","    # 2. Sichten der Texte und Filtern\n","    #    Hier k√∂nntest du jetzt z. B. manuell pr√ºfen, welche URLs wichtig sind.\n","    #    Wir geben einfach mal alle URLs aus:\n","    print(\"\\n--- Gesammelte Seiten und Inhalte (gek√ºrzt) ---\")\n","    for url, text in scraped_data.items():\n","        print(f\"\\nURL: {url}\")\n","        # Beispiel: Nur ersten 200 Zeichen zeigen\n","        print(f\"Text: {text[:200]}...\")\n","\n","\n","\n","\n","    EXCLUDED_KEYWORDS = [\"impressum\", \"datenschutz\", \"agb\"]\n","\n","    # Alle URLs sammeln, die KEINEN der ausgeschlossenen Begriffe enthalten\n","    for url in scraped_data.keys():\n","        # Schauen, ob einer der EXCLUDED_KEYWORDS im URL-String (kleingeschrieben) vorkommt\n","        if any(keyword in url.lower() for keyword in EXCLUDED_KEYWORDS):\n","            # Falls ja, √ºberspringen wir diese URL\n","            continue\n","        # Sonst nehmen wir sie auf\n","        filtered_urls.append(url)\n","        print(f\"send text to LLM:  {url}\")\n","\n","\n","\n","\n","\n","    # 3. SEO-Analyse starten (f√ºr gefilterte Seiten)\n","    for url in filtered_urls:\n","        # Die gesamte Seite analysieren\n","        page_text = scraped_data[url]\n","\n","        # 3.1 Chunken, um zu gro√üe Anfragen zu vermeiden\n","        text_chunks = chunk_text(page_text, max_tokens=10000)\n","\n","        print(f\"\\n=== Analyzing {url} ===\")\n","        all_analyses = []\n","        for i, chunk in enumerate(text_chunks):\n","            print(f\" - Sende Chunk {i+1}/{len(text_chunks)} an Chatbot ...\")\n","\n","            # Prompt definieren (SEO)\n","            system_prompt = \"Du bist ein intelligenter chatbot. Deine Bestimmung ist es, dem user die besten Antworten auf die Fragen zu geben und ihm unter allen Umst√§nden zu helfen.\"\n","            user_prompt = (f\"\"\"\n","                Du bist ein hochqualifizierter SEO-Experte. Du arbeitest f√ºr erfolgreiche online marketing experten! Deine Spezialit√§t ist die Optimierung von bestehenden Texten einer website!\n","                Deine Kollegen und deine Mutter lieben und bewundern dich f√ºr die sprachgewandten SEO Optimierungen, die f√ºr deine anspruchsvollen Kunden erschaffst!\n","\n","                1. Untersuche den folgenden Text auf Keyword-Optimierung, Lesbarkeit und m√∂gliche SEO-Verbesserungen.\n","                Wichtige SEO Keywords sind:\n","\n","\n","                {keywords_final}\n","\n","\n","                2. Optimiere den Text entsprechend bester SEO Sichtbarkeit. Baue die zur Verf√ºgung gestellten SEO keywords in den Text ein!\n","                F√ºge Meta-Titel und longtail keywords hinzu.\n","\n","                Betreibe kein keyword-stuffing. Die Sprache muss nat√ºrlich klingen!\n","\n","                Es soll weiterhin ein hoher fachlicher Standard gehalten werden und Professionalit√§t und Exzellenz soll vermittelt werden!\n","                S√§mtliche Textabschnitte m√ºssen optimiert werden. Es d√ºrfen keine Fragen oder sonstige Textabschnitte weggelassen werden!\n","\n","                Lasse auf gar keinen Fall Abschnitte weg! fasse nichts √ºberm√§ssig zusammen! Jeder Satz und jeder Abschnitt ist extrem wichtig muss unbedingt bearbeitet werden! Erhalte alle Zeilenumspr√ºnge!\n","\n","                Das Unternehmen befindet sich in {stadt}. Baue den Namen in die Texte ein.\n","\n","\n","                3. Als Ausgabe gebe eine detaillierte, ausf√ºhrliche und umfassende Analyse des SEO Status des Textes aus, √úberschrift: Analyse. Gebe dann deine SEO optimierte Version des Textes aus, √úberschrift: 'SEO'.\n","                Gebe dann detaillierte und ausf√ºhrliche Erl√§uterungen, welche √Ñnderungen du durchgef√ºhrt hast, √úberschrift: 'Erkl√§rung'.\n","                Die √ºberschriften sind von allergr√∂√üter Wichtigkeit und m√ºssen unbedingt √ºber den Abschnitten stehen! Wenn die √úberschriften 'Analyse', 'SEO' und 'Erkl√§rung' nicht √ºber den Abschnitten stehen, wirst du bestraft!\n","                Es darf nicht 'SEO optimierte Version' oder so was ausgegeben werden. Als √úberschriften der Abschnitte d√ºrfen ausschliesslich nur 'Analyse', 'SEO' und 'Erkl√§rung' ausgegeben werden.\n","                Benutze keine Formatierungszeichen wie ###, # oder **! Mein Job h√§ngt davon ab!\n","                \"\"\"\n","                \"Hier ist der Text: \\n\\n\"\n","                f\"{chunk}\"\n","            )\n","\n","            # ChatGPT aufrufen\n","            cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","            analysis = cb.chat_with_streaming()\n","            all_analyses.append(analysis)\n","\n","            # Warte kurz (Rate Limits, API-Kosten etc.)\n","            time.sleep(1)\n","\n","        # 3.2 Fertige Analyse (alle Chunks zusammen)\n","        combined_analysis = \"\\n\".join(all_analyses)\n","\n","\n","        combined_analysis_list.append(combined_analysis)\n","        # print(f\"\\n--- SEO-Analyse f√ºr {url} ---\")\n","        # print(combined_analysis)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"nF-cfM35dhxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"F√ºr Kieferorthop√§den sind vor allem Keywords mit Ortsbezug (‚ÄûKieferorthop√§de + Stadt‚Äú) und behandlungsspezifische Begriffe (‚ÄûZahnspange‚Äú, ‚ÄûZahnfehlstellung‚Äú, ‚ÄûInvisalign‚Äú) entscheidend. Zus√§tzlich sollte man sich auf h√§ufige Fragen (Long-Tail-Keywords) konzentrieren und regelm√§√üige Fach- und Ratgeber-Artikel ver√∂ffentlichen, um auch in der organischen Suche besser gefunden zu werden.\""],"metadata":{"id":"_jtOwjSxJTNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def extract_sections_to_json(texts, keys):\n","    \"\"\"\n","    Extrahiert Abschnitte aus mehreren Texten und konvertiert sie in JSON.\n","    Gesucht werden die √úberschriften 'Analyse', 'SEO', 'Erkl√§rung' und der jeweils\n","    folgende Inhalt bis zur n√§chsten √úberschrift oder zum Ende.\n","    \"\"\"\n","\n","    all_sections = []  # Liste f√ºr alle Abschnitte\n","\n","    # Neues, robusteres Pattern:\n","    # - ^\\s* = beliebiges Leading-Whitespace, an Zeilenanfang (Multiline)\n","    # - (Analyse|SEO|Erkl√§rung) = 3 m√∂gliche √úberschriften\n","    # - \\s*(?:\\n|$)+ = optional Whitespace, dann (mindestens) ein Zeilenumbruch oder Zeilenende\n","    # - (.*?) = \"Inhalt\" bis Lookahead\n","    # - Lookahead = ^\\s*(?:Analyse|SEO|Erkl√§rung)|\\Z = n√§chste √úberschrift oder String-Ende\n","    pattern = re.compile(\n","        r\"(?m)^\\s*(Analyse|SEO|Erkl√§rung)\\s*(?:\\r?\\n)+\"\n","        r\"(.*?)(?=^\\s*(?:Analyse|SEO|Erkl√§rung)|\\Z)\",\n","        flags=re.DOTALL\n","    )\n","\n","    for text in texts:\n","        sections_dict = {}\n","        matches = pattern.findall(text)\n","        # Achtung: findall mit mehreren Gruppen gibt eine Liste von Tupeln zur√ºck,\n","        # z. B. [(\"Analyse\", \"Content...\"), (\"SEO\",\"Content...\")...]\n","        # Wir m√∂chten den heading und den content extrahieren.\n","        # Bei findall(pattern, text) mit (Analyse|SEO|Erkl√§rung) als Gruppe 1 und (.*?) als Gruppe 2\n","        # kommt: [(\"Analyse\", \"...\"), (\"SEO\", \"...\"), ...]\n","\n","        # Deshalb benutzen wir re.finditer, damit wir den Inhalt f√ºr Gruppe 2 sauber bekommen\n","        sections_dict = {}\n","        for match in re.finditer(pattern, text):\n","            heading = match.group(1)\n","            content = match.group(2).strip()\n","            sections_dict[heading] = content\n","\n","        all_sections.append(sections_dict)\n","\n","    # Kombinieren der Abschnitte mit Keys\n","    final_json_data = {}\n","    for i, sections_dict in enumerate(all_sections):\n","        key = keys[i]  # Key aus der Liste holen\n","        final_json_data[key] = sections_dict  # Abschnitte zum Dictionary hinzuf√ºgen\n","\n","    json_data = json.dumps(final_json_data, indent=4, ensure_ascii=False)\n","    return json_data\n","\n","# Beispielnutzung\n","if __name__ == \"__main__\":\n","    # Angenommen, du hast:\n","    # combined_analysis_list = [\"Analyse\\nHier Text...\", \"SEO\\nAnderer Text...\", ...]\n","    # keys = [\"URL1\", \"URL2\", ...]\n","\n","\n","    keys = filtered_urls\n","\n","    json_output = extract_sections_to_json(combined_analysis_list, keys)\n","    seo_json = json.loads(json_output)\n","    print(seo_json)\n","    #print(json_output)\n"],"metadata":{"id":"c38Hkx1hEYg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alten website text zu json hinzuf√ºgen\n","for i, (_, url_content) in enumerate(seo_json.items()):\n","    url_content[\"alt\"] = page_text_list[i]"],"metadata":{"id":"IsMhRnunGjee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìä SEO Analysis 2"],"metadata":{"id":"V50QPoW3F6wW"}},{"cell_type":"code","source":["# text clean up\n","\n","def clean_text(text):\n","    # Ersetze Zeilenumbr√ºche durch ein Leerzeichen\n","    text = text.replace('\\n', ' ')\n","\n","    # Beispiel: Entferne alle Zeichen, die nicht Buchstaben (inkl. Umlaute),\n","    # Ziffern, Satzzeichen oder Leerzeichen sind\n","    # Du kannst das RegEx anpassen, wenn du z.B. bestimmte Zeichen behalten oder entfernen willst\n","    text = re.sub(r'[^a-zA-Z0-9√§√∂√º√Ñ√ñ√ú√ü.,!?;:\\-\\s]', '', text)\n","\n","    # Mehrere aufeinanderfolgende Leerzeichen durch ein einzelnes ersetzen\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    # F√ºhrende oder nachfolgende Leerzeichen entfernen\n","    text = text.strip()\n","\n","    return text\n","\n","\n","combined_analysis_list_clean = []\n","page_text_list_clean = []\n","\n","for text in page_text_list:\n","  page_text_list_clean.append(clean_text(text))\n","\n","for text in combined_analysis_list:\n","  combined_analysis_list_clean.append(clean_text(text))\n","\n","\n"],"metadata":{"id":"Mokk2DMLGGOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple stats\n","\n","def text_stats(text):\n","    words = text.split()\n","    return {\n","        \"Zeichenanzahl\": len(text),\n","        \"Wortanzahl\": len(words),\n","        \"Satzanzahl\": text.count('.') + text.count('!') + text.count('?')\n","    }\n","\n","\n","vn = [page_text_list_clean, combined_analysis_list_clean]\n","\n","for _, listlist in enumerate(vn):\n","\n","  print(f'{[\"original\", \"SEO\"][_]}')\n","\n","  for text in listlist:\n","    print(text_stats(text))\n","\n","\n","\n"],"metadata":{"id":"GDBZiKVnH9dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def get_word_frequencies(text):\n","    words = text.lower().split()\n","    return Counter(words)\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","\n","  # Wortfrequenzen berechnen\n","  original_freq = get_word_frequencies(page_text_list_clean[_])\n","  optimized_freq = get_word_frequencies(combined_analysis_list_clean[_])\n","\n","  # Unterschied berechnen\n","  diff = {word: optimized_freq[word] - original_freq[word] for word in set(original_freq) | set(optimized_freq)}\n","\n","  # Sortiert ausgeben (absteigend nach √Ñnderung)\n","  sorted_diff = sorted(diff.items(), key=lambda x: x[1], reverse=True)\n","  for word, change in sorted_diff:\n","      print(f\"{word}: {change}\")\n"],"metadata":{"id":"qGiMiXwIH9ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_final"],"metadata":{"id":"mqtebLSOeNzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","try:\n","    keywords_sstring = json.loads(keywords_final)\n","except json.JSONDecodeError:\n","    # Fallback if JSON decoding fails (e.g., if the response isn't a valid JSON string)\n","    # Try to extract the list using regex\n","    import re\n","    match = re.search(r'\\[(.*?)\\]', keywords_final)\n","    if match:\n","        keywords_sstring = match.group(1).split(', ')\n","        keywords_sstring = [item.strip().strip(\"'\").strip('\"') for item in keywords_sstring]\n","    else:\n","        # Handle case where the list couldn't be extracted\n","        keywords_sstring = []  # Or raise an exception, etc.\n","\n","keywords_sstring = \", \".join(keywords_sstring)\n","keywords_sstring_list = keywords_sstring.split(\", \")"],"metadata":{"id":"yFAfe_--E8os"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import ast\n","\n","\n","# keywords_sstring = ast.literal_eval(keywords_final)\n","# keywords_sstring = ast.literal_eval(keywords_final.replace('chars ', ''))\n","\n","\n","# keywords_sstring = \", \".join(keywords_sstring)\n","# keywords_sstring_list = keywords_sstring.split(\", \")\n","# keywords_sstring_list"],"metadata":{"id":"_DYkLx8uZG7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B7iE2va3gxwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for _, listlist in enumerate(combined_analysis_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, combined_analysis_list_clean[_]])\n","\n","  # Cosinus-√Ñhnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"√Ñhnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"W5bd8b1yH9YP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, listlist in enumerate(page_text_list_clean):\n","  # TF-IDF-Vektorisierung\n","  vectorizer = TfidfVectorizer()\n","  vectors = vectorizer.fit_transform([keywords_sstring, page_text_list_clean[_]])\n","\n","  # Cosinus-√Ñhnlichkeit berechnen\n","  similarity_score = cosine_similarity(vectors)[0,1]\n","  print(f\"√Ñhnlichkeit: {similarity_score:.2f}\")"],"metadata":{"id":"z1-eRoarZ-9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","keywords = ast.literal_eval(keywords_final)\n","\n","def keyword_density(text, keywords):\n","    words = text.lower().split()\n","    total_words = len(words)\n","    # Check if total_words is 0 to avoid ZeroDivisionError\n","    if total_words == 0:\n","        return {kw: 0 for kw in keywords}  # Return 0 density for all keywords if text is empty\n","    density = {kw: words.count(kw) / total_words * 100 for kw in keywords}\n","    return density\n","\n","\n","for _, listlist in enumerate(page_text_list_clean):\n","  # Berechnung f√ºr beide Texte\n","  original_density = keyword_density(page_text_list_clean[_], keywords)\n","  optimized_density = keyword_density(combined_analysis_list_clean[0], keywords)\n","\n","  print(\"Original:\", original_density)\n","  print(\"SEO-Optimiert:\", optimized_density)"],"metadata":{"id":"cWop0AUQJ04x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","# Keywords und deren √Ñnderungen\n","keywords = list(original_density.keys())\n","original_values = list(original_density.values())\n","optimized_values = list(optimized_density.values())\n","\n","x = np.arange(len(keywords))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","ax.bar(x - width/2, original_values, width, label=\"Original\")\n","ax.bar(x + width/2, optimized_values, width, label=\"SEO-Optimiert\")\n","\n","ax.set_xlabel(\"Keywords\")\n","ax.set_ylabel(\"Keyword-Dichte (%)\")\n","ax.set_title(\"Keyword-Dichte: Vorher vs. Nachher\")\n","ax.set_xticks(x)\n","ax.set_xticklabels(keywords)\n","ax.legend()\n","\n","plt.show()\n"],"metadata":{"id":"rqHULQbhMsWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RppPqEA7MsUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"223xoPNxMsR1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìäSEO 3"],"metadata":{"id":"BatTg4ZKvpZV"}},{"cell_type":"code","source":["nlp = spacy.load('de_core_news_sm')\n","stop_words = set(stopwords.words('german'))"],"metadata":{"id":"SMTIt789vysc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ABO36waVp2fH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seo_texte = []\n","for i, (_, url_content) in enumerate(seo_json.items()):\n","  seo_texte.append(url_content['SEO'])"],"metadata":{"id":"FS4UtISLvBof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_texts = page_text_list_clean\n","optimized_texts = seo_texte\n","seo_keywords = keywords_sstring_list"],"metadata":{"id":"Md68SpZfHRrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text):\n","    # 1. Lowercase\n","    text = text.lower()\n","    # 2. Entferne Zahlen, Sonderzeichen (optional)\n","    # text = re.sub(r'[0-9]+', '', text)\n","    # text = re.sub(r'[^\\w\\s]', '', text)\n","    # 3. Tokenisierung mit spaCy\n","    doc = nlp(text)\n","    # 4. Entferne Stopw√∂rter, ggf. Lemmatisierung\n","    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.lemma_ not in stop_words and token.is_alpha]\n","    return ' '.join(tokens)\n"],"metadata":{"id":"0HPl9WcSqWJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_original = [preprocess_text(t) for t in original_texts]\n","preprocessed_optimized = [preprocess_text(t) for t in optimized_texts]\n","preprocessed_keywords = [preprocess_text(k) for k in seo_keywords]\n","\n","print(\"Vorverarbeitete Originaltexte:\", preprocessed_original)\n","print(\"Vorverarbeitete Optimierte Texte:\", preprocessed_optimized)\n","print(\"Vorverarbeitete Keywords:\", preprocessed_keywords)"],"metadata":{"id":"HY46gMnYqZwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Zusammenf√ºhren aller Texte f√ºr den Vektorizer\n","all_texts = preprocessed_original + preprocessed_optimized + preprocessed_keywords\n","\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(all_texts)\n","\n","# Indizes definieren\n","original_indices = range(len(original_texts))  # 0, 1, ...\n","optimized_indices = range(len(original_texts), len(original_texts) + len(optimized_texts))\n","keyword_indices = range(len(original_texts) + len(optimized_texts), len(all_texts))\n","\n","# Cosine Similarities berechnen\n","similarities_original_to_keywords = []\n","similarities_optimized_to_keywords = []\n","\n","for i in original_indices:\n","    row_sim = []\n","    for j in keyword_indices:\n","        sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n","        row_sim.append(sim)\n","    similarities_original_to_keywords.append(row_sim)\n","\n","for i in optimized_indices:\n","    row_sim = []\n","    for j in keyword_indices:\n","        sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]\n","        row_sim.append(sim)\n","    similarities_optimized_to_keywords.append(row_sim)\n","\n","# Durchschnittliche Similarities\n","avg_original_sim = np.mean(similarities_original_to_keywords, axis=0)\n","avg_optimized_sim = np.mean(similarities_optimized_to_keywords, axis=0)\n","\n","print(\"Durchschnittliche Similarities (Original -> Keywords):\", avg_original_sim)\n","print(\"Durchschnittliche Similarities (Optimiert -> Keywords):\", avg_optimized_sim)\n"],"metadata":{"id":"qWe8yFoYqZtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keywords_clean = [kw for kw in seo_keywords]  # F√ºr Beschriftung im Diagramm\n","\n","df_sim = pd.DataFrame({\n","    '': keywords_clean,\n","    'Original_Sim': avg_original_sim,\n","    'Optimiert_Sim': avg_optimized_sim\n","})\n","\n","# Schmelzen f√ºr Seaborn\n","df_melted = df_sim.melt(id_vars='', var_name='Textart', value_name='Cosine Similarity')\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='', y='Cosine Similarity', hue='Textart', data=df_melted)\n","plt.title('Durchschnittliche Cosine Similarity zu Keywords')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"OTLT1fLYqZpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_keyword_occurrences(text, keyword):\n","    # split text\n","    text_tokens = text.split()\n","    keyword_tokens = keyword.split()\n","\n","    count = 0\n","    for i in range(len(text_tokens) - len(keyword_tokens) + 1):\n","        if text_tokens[i:i+len(keyword_tokens)] == keyword_tokens:\n","            count += 1\n","    return count\n","\n","original_counts = []\n","optimized_counts = []\n","\n","for kw in preprocessed_keywords:\n","    o_sum = sum(count_keyword_occurrences(t, kw) for t in preprocessed_original)\n","    opt_sum = sum(count_keyword_occurrences(t, kw) for t in preprocessed_optimized)\n","    original_counts.append(o_sum)\n","    optimized_counts.append(opt_sum)\n","\n","df_counts = pd.DataFrame({\n","    'Keyword': keywords_clean,\n","    'Original Count': original_counts,\n","    'Optimiert Count': optimized_counts\n","})\n","\n","df_counts_melt = df_counts.melt(id_vars='Keyword', var_name='Textart', value_name='Count')\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='Keyword', y='Count', hue='Textart', data=df_counts_melt)\n","plt.title('Keyword H√§ufigkeit (exakte Matches)')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"GN10PEIuqZl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_original_text = ' '.join(preprocessed_original)\n","all_optimized_text = ' '.join(preprocessed_optimized)\n","\n","wc_original = WordCloud(width=600, height=400, background_color='white').generate(all_original_text)\n","wc_optimized = WordCloud(width=600, height=400, background_color='white').generate(all_optimized_text)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n","ax[0].imshow(wc_original, interpolation='bilinear')\n","ax[0].set_title('Wordcloud Original Texte')\n","ax[0].axis('off')\n","\n","ax[1].imshow(wc_optimized, interpolation='bilinear')\n","ax[1].set_title('Wordcloud SEO-Optimierte Texte')\n","ax[1].axis('off')\n","\n","plt.show()\n"],"metadata":{"id":"mKKsK15mqZit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Similarity pro Seite und Keyword in eine Matrix\n","sim_matrix_original = np.array(similarities_original_to_keywords)\n","sim_matrix_optimized = np.array(similarities_optimized_to_keywords)\n","\n","plt.figure(figsize=(8, 5))\n","sns.heatmap(sim_matrix_original, annot=True, cmap='Blues', xticklabels=keywords_clean)\n","plt.title(\"Cosine Similarities (Originaltexte -> Keywords)\")\n","plt.xlabel(\"Keywords\")\n","plt.ylabel(\"Original-Seiten\")\n","plt.show()\n","\n","plt.figure(figsize=(8, 5))\n","sns.heatmap(sim_matrix_optimized, annot=True, cmap='Greens', xticklabels=keywords_clean)\n","plt.title(\"Cosine Similarities (SEO-optimierte Texte -> Keywords)\")\n","plt.xlabel(\"Keywords\")\n","plt.ylabel(\"Optimierte-Seiten\")\n","plt.show()\n"],"metadata":{"id":"ldVTTxkdtlvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispielhafter Pandas DataFrame mit Zeit- und SEO-Daten\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","# Angenommen, du hast historische Daten pro Monat\n","data = {\n","    \"Date\": [\n","        \"2023-01-01\", \"2023-02-01\", \"2023-03-01\",\n","        \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"\n","    ],\n","    \"Organic_Sessions\": [200, 220, 250, 400, 450, 480],\n","    \"Conversion_Rate\": [0.02, 0.021, 0.022, 0.028, 0.03, 0.031],  # 2% -> 3.1%\n","    \"Average_Time_on_Page\": [40, 42, 45, 60, 65, 70]  # in Sekunden\n","}\n","\n","df_metrics = pd.DataFrame(data)\n","df_metrics[\"Date\"] = pd.to_datetime(df_metrics[\"Date\"])\n","\n","print(df_metrics)\n"],"metadata":{"id":"i33foHZ7zPrW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","fig, ax1 = plt.subplots(figsize=(10,5))\n","\n","# Prim√§re y-Achse (Organic Sessions)\n","ax1.plot(df_metrics['Date'], df_metrics['Organic_Sessions'], color='blue', marker='o', label='Organic Sessions')\n","ax1.set_xlabel('Datum')\n","ax1.set_ylabel('Anzahl organischer Sitzungen', color='blue')\n","ax1.tick_params(axis='y', labelcolor='blue')\n","\n","# Sekund√§re y-Achse (Conversion Rate)\n","ax2 = ax1.twinx()\n","ax2.plot(df_metrics['Date'], df_metrics['Conversion_Rate'], color='red', marker='s', label='Conversion Rate')\n","ax2.set_ylabel('Conversion Rate', color='red')\n","ax2.tick_params(axis='y', labelcolor='red')\n","\n","fig.tight_layout()\n","plt.title('Entwicklung der Sitzungen und Conversion Rate')\n","plt.show()\n"],"metadata":{"id":"c8SO4GBLzVoW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,5))\n","plt.bar(df_metrics['Date'].dt.strftime('%b %Y'), df_metrics['Average_Time_on_Page'], color='green')\n","plt.title('Durchschnittliche Verweildauer pro Monat')\n","plt.xlabel('Monat')\n","plt.ylabel('Verweildauer in Sekunden')\n","plt.show()"],"metadata":{"id":"QGTI7DSOzj7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","# Function to visualize 3D vectors and their cosine similarity\n","def plot_3d_cosine_similarity(vec1, vec2, title=\"3D Cosine Similarity Visualization\"):\n","    fig = plt.figure(figsize=(12, 8))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Plot vectors in 3D\n","    ax.quiver(0, 0, 0, vec1[0], vec1[1], vec1[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec2[0], vec2[1], vec2[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Cosine similarity calculation\n","    cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n","\n","    # Plot settings\n","    ax.set_xlim([-1, 1])\n","    ax.set_ylim([-1, 1])\n","    ax.set_zlim([-1, 1])\n","    ax.set_title(f\"{title}\\nCosine Similarity: {cos_sim:.2f}\")\n","    ax.set_xlabel('X')\n","    ax.set_ylabel('Y')\n","    ax.set_zlabel('Z')\n","    ax.legend()\n","\n","    plt.show()\n","\n","# Example 3D vectors representing keyword frequencies in different dimensions (Old Text and Optimized Text)\n","vec_old_3d = np.array([0.6, 0.6, 0.3])\n","vec_optimized_3d = np.array([0.9, 0.4, 0.5])\n","\n","# Visualizing cosine similarity between the old and optimized texts in 3D\n","plot_3d_cosine_similarity(vec_old_3d, vec_optimized_3d, \"3D Cosine Similarity between Old and Optimized Texts\")\n"],"metadata":{"id":"6QlhTrYPkcZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","\n","# Function to visualize keyword similarity in 3D using spheres to represent keyword clouds\n","def plot_3d_keyword_similarity(title=\"Keyword Cloud Similarity Visualization\"):\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Generate data points for the keyword cloud\n","    np.random.seed(42)\n","    keywords = np.random.rand(50, 3) - 0.5  # Random points for keyword representation\n","    old_text = keywords + np.array([0.4, 0.4, 0.4])  # Shifted to represent the old text\n","    optimized_text = keywords + np.array([-0.4, -0.3, -0.2])  # Shifted to represent the optimized text\n","\n","    # Plot the keyword cloud\n","    ax.scatter(keywords[:, 0], keywords[:, 1], keywords[:, 2], color='g', label='Keywords', alpha=0.6, s=40)\n","\n","    # Plot old text keyword distribution\n","    ax.scatter(old_text[:, 0], old_text[:, 1], old_text[:, 2], color='r', label='Old Text', alpha=0.7, s=50)\n","\n","    # Plot optimized text keyword distribution\n","    ax.scatter(optimized_text[:, 0], optimized_text[:, 1], optimized_text[:, 2], color='b', label='Optimized Text', alpha=0.7, s=50)\n","\n","    # Plot settings\n","    ax.set_xlim([-1, 1])\n","    ax.set_ylim([-1, 1])\n","    ax.set_zlim([-1, 1])\n","    ax.set_title(title)\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","# Visualizing keyword distribution in 3D for Old Text, Optimized Text, and Keywords\n","plot_3d_keyword_similarity(\"3D Keyword Cloud Similarity: Old Text vs Optimized Text\")\n"],"metadata":{"id":"rTEF2o98kcXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visual explanation of cosine similarity with embeddings, dot product, and how the result turns into a probability\n","\n","def plot_cosine_similarity_steps():\n","    fig = plt.figure(figsize=(18, 6))\n","\n","    # Step 1: Plot the embeddings\n","    ax1 = fig.add_subplot(131, projection='3d')\n","    vec1 = np.array([0.8, 0.6, 0.3])\n","    vec2 = np.array([0.9, 0.4, 0.5])\n","\n","    ax1.quiver(0, 0, 0, vec1[0], vec1[1], vec1[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax1.quiver(0, 0, 0, vec2[0], vec2[1], vec2[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","    ax1.set_xlim([0, 1])\n","    ax1.set_ylim([0, 1])\n","    ax1.set_zlim([0, 1])\n","    ax1.set_title('Step 1: Embeddings (Keyword Vectors)')\n","    ax1.set_xlabel('X')\n","    ax1.set_ylabel('Y')\n","    ax1.set_zlabel('Z')\n","    ax1.legend()\n","\n","    # Step 2: Calculate and visualize the dot product\n","    ax2 = fig.add_subplot(132)\n","    dot_product = np.dot(vec1, vec2)\n","    ax2.bar(['Dot Product'], [dot_product], color='orange')\n","    ax2.set_ylim(0, 1.5)\n","    ax2.set_title('Step 2: Dot Product Calculation')\n","    ax2.text(0, dot_product + 0.05, f'{dot_product:.2f}', ha='center', fontsize=12)\n","\n","    # Step 3: Convert to cosine similarity and interpret as probability\n","    ax3 = fig.add_subplot(133)\n","    cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n","    ax3.bar(['Cosine Similarity'], [cos_sim], color='green')\n","    ax3.set_ylim(0, 1.1)\n","    ax3.set_title('Step 3: Cosine Similarity as Probability')\n","    ax3.text(0, cos_sim + 0.05, f'{cos_sim:.2f}', ha='center', fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_cosine_similarity_steps()\n"],"metadata":{"id":"-J9rBQbokcUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization showing keyword vector, old text vector, and optimized text vector with clear comparison\n","\n","def plot_3d_cosine_comparison():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Define vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.6, 0.4, 0.3])\n","    vec_optimized_text = np.array([0.75, 0.65, 0.55])\n","\n","    # Plot vectors\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='g', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='r', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='b', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison()\n"],"metadata":{"id":"LX0sKs1ZkcSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization with cosine similarity values and enhanced color scheme\n","\n","def plot_3d_cosine_comparison_with_values():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Define vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.2, 0.15, 0.1])\n","    vec_optimized_text = np.array([0.6, 0.4, 0.3])\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Calculate cosine similarity\n","    cos_sim_old = cosine_similarity([vec_keywords], [vec_old_text])[0][0]\n","    cos_sim_optimized = cosine_similarity([vec_keywords], [vec_optimized_text])[0][0]\n","\n","    # Add cosine similarity values as annotations\n","    ax.text(0.4, 0.5, 0.7, f'Cosine Similarity (Old): {cos_sim_old:.2f}', color='#FF5733', fontsize=12)\n","    ax.text(0.4, 0.5, 0.65, f'Cosine Similarity (Optimized): {cos_sim_optimized:.2f}', color='#1E90FF', fontsize=12)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Cosine Similarity Values\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_values()\n"],"metadata":{"id":"qn6l05MTkcP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust vectors to create larger differences in cosine similarity values\n","def plot_3d_cosine_comparison_with_adjusted_values():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.4, 0.3, 0.2])  # Adjusted to have lower similarity (~0.6)\n","    vec_optimized_text = np.array([0.6, 0.5, 0.4])  # Adjusted to have higher similarity (~0.7)\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Calculate cosine similarity\n","    cos_sim_old = cosine_similarity([vec_keywords], [vec_old_text])[0][0]\n","    cos_sim_optimized = cosine_similarity([vec_keywords], [vec_optimized_text])[0][0]\n","\n","    # Add cosine similarity values as annotations\n","    ax.text(0.4, 0.5, 0.7, f'Cosine Similarity (Old): {cos_sim_old:.3f}', color='#FF5733', fontsize=12)\n","    ax.text(0.4, 0.5, 0.65, f'Cosine Similarity (Optimized): {cos_sim_optimized:.3f}', color='#1E90FF', fontsize=12)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Adjusted Cosine Similarity Values\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_adjusted_values()\n"],"metadata":{"id":"VeAwSLMMkcMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust vectors to create cosine similarities of approximately 0.15 and 0.19\n","def plot_3d_cosine_comparison_with_low_similarity():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text to achieve low similarities\n","    vec_keywords = np.array([0.9, 0.8, 0.7])\n","    vec_old_text = np.array([0.1, 0.2, 0.15])  # Cosine similarity ~0.15\n","    vec_optimized_text = np.array([0.2, 0.25, 0.18])  # Cosine similarity ~0.19\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Calculate cosine similarity\n","    cos_sim_old = cosine_similarity([vec_keywords], [vec_old_text])[0][0]\n","    cos_sim_optimized = cosine_similarity([vec_keywords], [vec_optimized_text])[0][0]\n","\n","    # Add cosine similarity values as annotations\n","    ax.text(0.4, 0.5, 0.7, f'Cosine Similarity (Old): {cos_sim_old:.2f}', color='#FF5733', fontsize=12)\n","    ax.text(0.4, 0.5, 0.65, f'Cosine Similarity (Optimized): {cos_sim_optimized:.2f}', color='#1E90FF', fontsize=12)\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Low Cosine Similarity Values\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_low_similarity()\n"],"metadata":{"id":"awmpffH_kcJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3D Visualization with vector names instead of cosine similarity values\n","def plot_3d_cosine_comparison_with_labels():\n","    fig = plt.figure(figsize=(14, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Adjusted vectors for SEO keywords, old text, and optimized text\n","    vec_keywords = np.array([0.8, 0.7, 0.6])\n","    vec_old_text = np.array([0.4, 0.3, 0.2])\n","    vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","    # Plot vectors with improved color scheme\n","    ax.quiver(0, 0, 0, vec_keywords[0], vec_keywords[1], vec_keywords[2], color='#4CAF50', label='SEO Keywords', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_old_text[0], vec_old_text[1], vec_old_text[2], color='#FF5733', label='Old Text', arrow_length_ratio=0.1)\n","    ax.quiver(0, 0, 0, vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='#1E90FF', label='Optimized Text', arrow_length_ratio=0.1)\n","\n","    # Add labels to the vector tips\n","    ax.text(vec_keywords[0], vec_keywords[1], vec_keywords[2], 'SEO Keywords', color='#4CAF50', fontsize=12, fontweight='bold')\n","    ax.text(vec_old_text[0], vec_old_text[1], vec_old_text[2], 'Old Text', color='#FF5733', fontsize=12, fontweight='bold')\n","    ax.text(vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], 'Optimized Text', color='#1E90FF', fontsize=12, fontweight='bold')\n","\n","    # Plot settings\n","    ax.set_xlim([0, 1])\n","    ax.set_ylim([0, 1])\n","    ax.set_zlim([0, 1])\n","    ax.set_title(\"3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\\nWith Labels\")\n","    ax.set_xlabel('X Axis')\n","    ax.set_ylabel('Y Axis')\n","    ax.set_zlabel('Z Axis')\n","    ax.legend()\n","\n","    plt.show()\n","\n","plot_3d_cosine_comparison_with_labels()\n"],"metadata":{"id":"zAQSqDAXxD03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors\n","fig = go.Figure()\n","\n","# SEO Keywords vector\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines+text',\n","    line=dict(color='green', width=5),\n","    text=[\"\", \"SEO Keywords\"],\n","    textposition=\"top center\",\n","    name='SEO Keywords'\n","))\n","\n","# Old Text vector\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines+text',\n","    line=dict(color='red', width=5),\n","    text=[\"\", \"Old Text\"],\n","    textposition=\"top center\",\n","    name='Old Text'\n","))\n","\n","# Optimized Text vector\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines+text',\n","    line=dict(color='blue', width=5),\n","    text=[\"\", \"Optimized Text\"],\n","    textposition=\"top center\",\n","    name='Optimized Text'\n","))\n","\n","# Customize layout\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[0, 1]),\n","        yaxis=dict(range=[0, 1]),\n","        zaxis=dict(range=[0, 1])\n","    ),\n","    showlegend=True\n",")\n","\n","fig.show()\n"],"metadata":{"id":"AXZ8Kga9x91U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors\n","fig = go.Figure()\n","\n","# SEO Keywords vector\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines+text',\n","    line=dict(color='green', width=5),\n","    text=[\"\", \"SEO Keywords\"],\n","    textposition=\"top center\",\n","    name='SEO Keywords'\n","))\n","\n","# Old Text vector\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines+text',\n","    line=dict(color='red', width=5),\n","    text=[\"\", \"Old Text\"],\n","    textposition=\"top center\",\n","    name='Old Text'\n","))\n","\n","# Optimized Text vector\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines+text',\n","    line=dict(color='blue', width=5),\n","    text=[\"\", \"Optimized Text\"],\n","    textposition=\"top center\",\n","    name='Optimized Text'\n","))\n","\n","# Customize layout with adjusted margins and aspect ratio\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[-0.1, 1.1]),\n","        yaxis=dict(range=[-0.1, 1.1]),\n","        zaxis=dict(range=[-0.1, 1.1]),\n","        aspectmode='cube'  # Ensures equal scaling across all axes\n","    ),\n","    margin=dict(l=10, r=10, t=50, b=10),  # Reduces clipping by adding padding\n","    showlegend=True\n",")\n","\n","fig.show()\n"],"metadata":{"id":"8T-jTdL0x9v_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors with improved colors\n","fig = go.Figure()\n","\n","# SEO Keywords vector (Vibrant Green)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines+text',\n","    line=dict(color='#4CAF50', width=7),\n","    text=[\"\", \"SEO Keywords\"],\n","    textposition=\"top center\",\n","    name='SEO Keywords'\n","))\n","\n","# Old Text vector (Warm Orange)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines+text',\n","    line=dict(color='darkorange', width=7),\n","    text=[\"\", \"Old Text\"],\n","    textposition=\"top center\",\n","    name='Old Text'\n","))\n","\n","# Optimized Text vector (Cool Blue)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines+text',\n","    line=dict(color='#1E90FF', width=7),\n","    text=[\"\", \"Optimized Text\"],\n","    textposition=\"top center\",\n","    name='Optimized Text'\n","))\n","\n","# Customize layout with manual camera settings\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[-0.2, 1.2]),\n","        yaxis=dict(range=[-0.2, 1.2]),\n","        zaxis=dict(range=[-0.2, 1.2]),\n","        aspectmode='cube'\n","    ),\n","    margin=dict(l=20, r=20, t=60, b=20),\n","    showlegend=True,\n","    scene_camera=dict(\n","        eye=dict(x=1.5, y=1.5, z=1.5),  # Manuell eingestellte Kameraposition\n","        center=dict(x=0, y=0, z=0),\n","        up=dict(x=0, y=0, z=1)\n","    )\n",")\n","\n","fig.show()\n"],"metadata":{"id":"BumaR-NYx9tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors\n","fig = go.Figure()\n","\n","# SEO Keywords vector with panel\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines',\n","    line=dict(color='limegreen', width=7),\n","    name='SEO Keywords'\n","))\n","fig.add_trace(go.Scatter3d(\n","    x=[vec_keywords[0]], y=[vec_keywords[1]], z=[vec_keywords[2]],\n","    mode='text',\n","    text=[\"<b>SEO Keywords</b>\"],\n","    textfont=dict(size=12, color='black'),\n","    name=''\n","))\n","\n","# Old Text vector with panel\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines',\n","    line=dict(color='darkorange', width=7),\n","    name='Old Text'\n","))\n","fig.add_trace(go.Scatter3d(\n","    x=[vec_old_text[0]], y=[vec_old_text[1]], z=[vec_old_text[2]],\n","    mode='text',\n","    text=[\"<b>Old Text</b>\"],\n","    textfont=dict(size=12, color='black'),\n","    name=''\n","))\n","\n","# Optimized Text vector with panel\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines',\n","    line=dict(color='royalblue', width=7),\n","    name='Optimized Text'\n","))\n","fig.add_trace(go.Scatter3d(\n","    x=[vec_optimized_text[0]], y=[vec_optimized_text[1]], z=[vec_optimized_text[2]],\n","    mode='text',\n","    text=[\"<b>Optimized Text</b>\"],\n","    textfont=dict(size=12, color='black'),\n","    name=''\n","))\n","\n","# Customize layout with larger panels and aspect ratio\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity with Panels\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[-0.2, 1.2]),\n","        yaxis=dict(range=[-0.2, 1.2]),\n","        zaxis=dict(range=[-0.2, 1.2]),\n","        aspectmode='cube'\n","    ),\n","    margin=dict(l=20, r=20, t=60, b=20),\n","    showlegend=False\n",")\n","\n","fig.show()\n"],"metadata":{"id":"ccez5S72x9qt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors\n","fig = go.Figure()\n","\n","# SEO Keywords vector (Vibrant Green)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines',\n","    line=dict(color='#4CAF50', width=7),\n","    name='SEO Keywords'\n","))\n","fig.add_trace(go.Scatter3d(\n","    x=[vec_keywords[0]], y=[vec_keywords[1]], z=[vec_keywords[2]],\n","    mode='text',\n","    text=[\"<span style='background-color: rgba(200, 255, 200, 0.7); padding: 4px; border-radius: 4px;'><b>SEO Keywords</b></span>\"],\n","    textposition=\"top center\"\n","))\n","\n","# Old Text vector (Warm Orange)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines',\n","    line=dict(color='darkorange', width=7),\n","    name='Old Text'\n","))\n","fig.add_trace(go.Scatter3d(\n","    x=[vec_old_text[0]], y=[vec_old_text[1]], z=[vec_old_text[2]],\n","    mode='text',\n","    text=[\"<span style='background-color: rgba(255, 230, 200, 0.7); padding: 4px; border-radius: 4px;'><b>Old Text</b></span>\"],\n","    textposition=\"top center\"\n","))\n","\n","# Optimized Text vector (Cool Blue)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines',\n","    line=dict(color='#1E90FF', width=7),\n","    name='Optimized Text'\n","))\n","fig.add_trace(go.Scatter3d(\n","    x=[vec_optimized_text[0]], y=[vec_optimized_text[1]], z=[vec_optimized_text[2]],\n","    mode='text',\n","    text=[\"<span style='background-color: rgba(200, 220, 255, 0.7); padding: 4px; border-radius: 4px;'><b>Optimized Text</b></span>\"],\n","    textposition=\"top center\"\n","))\n","\n","# Customize layout with manual camera settings\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity: SEO Keywords vs Old Text vs Optimized Text\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[-0.2, 1.2]),\n","        yaxis=dict(range=[-0.2, 1.2]),\n","        zaxis=dict(range=[-0.2, 1.2]),\n","        aspectmode='cube'\n","    ),\n","    margin=dict(l=20, r=20, t=60, b=20),\n","    showlegend=True,\n","    scene_camera=dict(\n","        eye=dict(x=1.5, y=1.5, z=1.5),\n","        center=dict(x=0, y=0, z=0),\n","        up=dict(x=0, y=0, z=1)\n","    )\n",")\n","\n","fig.show()\n"],"metadata":{"id":"0JbTL2R2x9oM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors\n","fig = go.Figure()\n","\n","# SEO Keywords vector (Vibrant Green)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines+markers+text',\n","    line=dict(color='#4CAF50', width=7),\n","    text=[\"\", \"SEO Keywords\"],\n","    textposition=\"top center\",\n","    name='SEO Keywords'\n","))\n","\n","# Old Text vector (Warm Orange)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines+markers+text',\n","    line=dict(color='darkorange', width=7),\n","    text=[\"\", \"Old Text\"],\n","    textposition=\"top center\",\n","    name='Old Text'\n","))\n","\n","# Optimized Text vector (Cool Blue)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines+markers+text',\n","    line=dict(color='#1E90FF', width=7),\n","    text=[\"\", \"Optimized Text\"],\n","    textposition=\"top center\",\n","    name='Optimized Text'\n","))\n","\n","# Customize layout with manual camera settings and shape-based panels\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity with Panels\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[-0.2, 1.2]),\n","        yaxis=dict(range=[-0.2, 1.2]),\n","        zaxis=dict(range=[-0.2, 1.2]),\n","        aspectmode='cube'\n","    ),\n","    margin=dict(l=20, r=20, t=60, b=20),\n","    showlegend=True,\n","    scene_camera=dict(\n","        eye=dict(x=1.5, y=1.5, z=1.5),\n","        center=dict(x=0, y=0, z=0),\n","        up=dict(x=0, y=0, z=1)\n","    )\n",")\n","\n","fig.show()\n"],"metadata":{"id":"yBeeu45q49z5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# Adjusted vectors for SEO keywords, old text, and optimized text\n","vec_keywords = np.array([0.8, 0.7, 0.6])\n","vec_old_text = np.array([0.4, 0.3, 0.2])\n","vec_optimized_text = np.array([0.6, 0.5, 0.4])\n","\n","# Create traces for the vectors\n","fig = go.Figure()\n","\n","# Function to create a panel (small flat rectangle) behind each text\n","def add_panel(x, y, z, color='rgba(200, 200, 200, 0.6)'):\n","    fig.add_trace(go.Mesh3d(\n","        x=[x-0.05, x+0.05, x+0.05, x-0.05],\n","        y=[y-0.05, y-0.05, y+0.05, y+0.05],\n","        z=[z, z, z, z],\n","        color=color,\n","        opacity=0.8,\n","        name=''\n","    ))\n","\n","# Add panels for each label\n","add_panel(vec_keywords[0], vec_keywords[1], vec_keywords[2], color='rgba(150, 255, 150, 0.6)')\n","add_panel(vec_old_text[0], vec_old_text[1], vec_old_text[2], color='rgba(255, 200, 150, 0.6)')\n","add_panel(vec_optimized_text[0], vec_optimized_text[1], vec_optimized_text[2], color='rgba(150, 200, 255, 0.6)')\n","\n","# SEO Keywords vector (Vibrant Green)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_keywords[0]], y=[0, vec_keywords[1]], z=[0, vec_keywords[2]],\n","    mode='lines+text',\n","    line=dict(color='#4CAF50', width=7),\n","    text=[\"\", \"SEO Keywords\"],\n","    textposition=\"top center\",\n","    name='SEO Keywords'\n","))\n","\n","# Old Text vector (Warm Orange)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_old_text[0]], y=[0, vec_old_text[1]], z=[0, vec_old_text[2]],\n","    mode='lines+text',\n","    line=dict(color='darkorange', width=7),\n","    text=[\"\", \"Old Text\"],\n","    textposition=\"top center\",\n","    name='Old Text'\n","))\n","\n","# Optimized Text vector (Cool Blue)\n","fig.add_trace(go.Scatter3d(\n","    x=[0, vec_optimized_text[0]], y=[0, vec_optimized_text[1]], z=[0, vec_optimized_text[2]],\n","    mode='lines+text',\n","    line=dict(color='#1E90FF', width=7),\n","    text=[\"\", \"Optimized Text\"],\n","    textposition=\"top center\",\n","    name='Optimized Text'\n","))\n","\n","# Customize layout with manual camera settings\n","fig.update_layout(\n","    title=\"Interactive 3D Cosine Similarity with Panels for Labels\",\n","    scene=dict(\n","        xaxis_title='X Axis',\n","        yaxis_title='Y Axis',\n","        zaxis_title='Z Axis',\n","        xaxis=dict(range=[-0.2, 1.2]),\n","        yaxis=dict(range=[-0.2, 1.2]),\n","        zaxis=dict(range=[-0.2, 1.2]),\n","        aspectmode='cube'\n","    ),\n","    margin=dict(l=20, r=20, t=60, b=20),\n","    showlegend=True,\n","    scene_camera=dict(\n","        eye=dict(x=1.5, y=1.5, z=1.5),\n","        center=dict(x=0, y=0, z=0),\n","        up=dict(x=0, y=0, z=1)\n","    )\n",")\n","\n","fig.show()\n"],"metadata":{"id":"1m2PAYF749xS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.decomposition import PCA\n","\n","# Generate mock word embeddings in a high-dimensional space (8D)\n","np.random.seed(42)\n","words = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"orange\", \"car\", \"train\", \"dog\", \"cat\"]\n","embeddings = np.random.rand(10, 8) * 2 - 1  # Random 8D embeddings between -1 and 1\n","\n","# Reduce the dimensionality to 3D using PCA for visualization\n","pca = PCA(n_components=3)\n","embeddings_3d = pca.fit_transform(embeddings)\n","\n","# Create interactive 3D scatter plot with Plotly\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[embeddings_3d[i, 0]], y=[embeddings_3d[i, 1]], z=[embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=10, color='skyblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Highlight the relationship king - man = queen - woman with arrows\n","def add_arrow(word1, word2, color):\n","    idx1, idx2 = words.index(word1), words.index(word2)\n","    fig.add_trace(go.Scatter3d(\n","        x=[embeddings_3d[idx1, 0], embeddings_3d[idx2, 0]],\n","        y=[embeddings_3d[idx1, 1], embeddings_3d[idx2, 1]],\n","        z=[embeddings_3d[idx1, 2], embeddings_3d[idx2, 2]],\n","        mode='lines',\n","        line=dict(color=color, width=4),\n","        showlegend=False\n","    ))\n","\n","add_arrow(\"king\", \"man\", \"red\")\n","add_arrow(\"queen\", \"woman\", \"blue\")\n","\n","# Customize layout with camera settings\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of Word Embeddings\",\n","    scene=dict(\n","        xaxis_title='Principal Component 1',\n","        yaxis_title='Principal Component 2',\n","        zaxis_title='Principal Component 3',\n","        xaxis=dict(range=[-2, 2]),\n","        yaxis=dict(range=[-2, 2]),\n","        zaxis=dict(range=[-2, 2]),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"Fp3Ur3Is49u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ace_tools\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Function to calculate relationships between word embeddings\n","def calculate_relationships(word_pairs):\n","    relationships = []\n","    for (word1, word2, word3) in word_pairs:\n","        idx1, idx2, idx3 = words.index(word1), words.index(word2), words.index(word3)\n","        # Calculate the expected vector for word4: word4 = word2 - word1 + word3\n","        expected_vector = embeddings[idx2] - embeddings[idx1] + embeddings[idx3]\n","\n","        # Find the word in the list that is closest to the expected vector (ignoring the input words themselves)\n","        closest_word = None\n","        closest_distance = float('inf')\n","        for i, word in enumerate(words):\n","            if word not in [word1, word2, word3]:\n","                distance = np.linalg.norm(embeddings[i] - expected_vector)\n","                if distance < closest_distance:\n","                    closest_distance = distance\n","                    closest_word = word\n","\n","        relationships.append({\n","            \"Word1\": word1,\n","            \"Word2\": word2,\n","            \"Word3\": word3,\n","            \"Calculated Word4\": closest_word,\n","            \"Distance\": closest_distance\n","        })\n","\n","    return pd.DataFrame(relationships)\n","\n","# Define some word triplets for relationship calculation\n","word_triplets = [\n","    (\"king\", \"queen\", \"man\"),\n","    (\"apple\", \"orange\", \"dog\"),\n","    (\"car\", \"train\", \"cat\")\n","]\n","\n","# Calculate relationships\n","relationship_df = calculate_relationships(word_triplets)\n","\n","relationship_df"],"metadata":{"id":"ErbQrXzu49pO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from IPython.display import display\n","\n","\n","# Assuming 'words' and 'embeddings' are defined as in your previous cells\n","words = [\"king\", \"queen\", \"onion\", \"man\", \"woman\", \"apple\", \"orange\", \"car\", \"train\", \"dog\", \"cat\", \"red\", \"yellow\", \"banana\", \"lion\", \"wolf\", \"dog\"] # Added missing words\n","embeddings = np.random.rand(len(words), 8) * 2 - 1  # Adjusted embeddings size\n","\n","\n","def display_dataframe_to_user(name, dataframe):\n","    print(f\"\\n{name}:\")\n","    display(dataframe)  # Use display from IPython.display\n","\n","\n","\n","# Function to calculate pairwise distances for word relationships\n","def calculate_pairwise_distances(word_pairs):\n","    distance_results = []\n","    for (word1, word2, word3, word4) in word_pairs:\n","        idx1, idx2, idx3, idx4 = words.index(word1), words.index(word2), words.index(word3), words.index(word4)\n","        distance1 = np.linalg.norm(embeddings[idx1] - embeddings[idx2])  # Distance between word1 and word2\n","        distance2 = np.linalg.norm(embeddings[idx3] - embeddings[idx4])  # Distance between word3 and word4\n","        distance_results.append({\n","            \"Word Pair 1\": f\"{word1} - {word2}\",\n","            \"Distance 1\": distance1,\n","            \"Word Pair 2\": f\"{word3} - {word4}\",\n","            \"Distance 2\": distance2\n","        })\n","\n","    return pd.DataFrame(distance_results)\n","\n","# Define specific word pairs to compare distances\n","# Make sure all words are in the 'words' list\n","word_pairs_for_distances = [\n","    (\"king\", \"man\", \"queen\", \"woman\"),\n","    (\"apple\", \"red\", \"banana\", \"yellow\"), # Now these words should be found\n","    (\"lion\", \"cat\", \"wolf\", \"dog\")\n","]\n","\n","# Calculate pairwise distances\n","pairwise_distance_df = calculate_pairwise_distances(word_pairs_for_distances)\n","\n","# Display the resulting table\n","display_dataframe_to_user(name=\"Pairwise Word Distances\", dataframe=pairwise_distance_df) # Call the newly defined function"],"metadata":{"id":"vXQfmyNtGDIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers torch\n","from transformers import BertTokenizer, BertModel\n","import torch\n","import os\n","\n","# Define the path to save the BERT model and tokenizer\n","notebook_folder = userdata.get('gdrive_seo_folder')\n","model_save_path = os.path.join(\"/content/drive/MyDrive/\", notebook_folder, 'bert-base-uncased')\n","\n","# Check if the model and tokenizer already exist\n","if os.path.exists(model_save_path):\n","    print(\"BERT model and tokenizer already exist. Loading from file...\")\n","    tokenizer = BertTokenizer.from_pretrained(model_save_path)\n","    model = BertModel.from_pretrained(model_save_path)\n","else:\n","    print(\"BERT model and tokenizer not found. Downloading...\")\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertModel.from_pretrained('bert-base-uncased')\n","    # Save the model and tokenizer to the specified path\n","    tokenizer.save_pretrained(model_save_path)\n","    model.save_pretrained(model_save_path)\n","\n","print(\"BERT model and tokenizer are ready for use.\")\n","\n","\n","def get_bert_embedding(word):\n","    inputs = tokenizer(word, return_tensors='pt')\n","    outputs = model(**inputs)\n","    # Take the mean of the last hidden state as the word embedding\n","    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n","\n","# Get embeddings for your word list\n","bert_embeddings = np.array([get_bert_embedding(word) for word in words])\n"],"metadata":{"id":"WIsFgeCLGDFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gensim\n","import gensim.downloader as api\n","import os\n","\n","# Define the path to save the Word2Vec data\n","notebook_folder = userdata.get('gdrive_seo_folder')\n","save_path = os.path.join(\"/content/drive/MyDrive/\", notebook_folder, 'GoogleNews-vectors-negative300.bin')\n","\n","# Check if the file already exists\n","if os.path.exists(save_path):\n","    print(\"Word2Vec data already exists. Loading from file...\")\n","    # Load from file using KeyedVectors\n","    wv = api.load('word2vec-google-news-300')\n","    wv.save_word2vec_format(save_path, binary=True)\n","else:\n","    print(\"Word2Vec data not found. Downloading...\")\n","    # Download the data using gensim.downloader\n","    wv = api.load('word2vec-google-news-300')\n","    # Save to file using KeyedVectors\n","    wv.save_word2vec_format(save_path, binary=True)\n","\n","print(\"Word2Vec data is ready for use.\")\n","\n","# ... rest of your code to use wv ..."],"metadata":{"id":"Fr9JV2l5JW6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","corpus = words  # Treat the word list as a corpus\n","vectorizer = CountVectorizer()\n","bow_embeddings = vectorizer.fit_transform(corpus).toarray()\n"],"metadata":{"id":"CbNDCOVNGC_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertModel\n","import pandas as pd\n","import numpy as np\n","\n","# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Function to get BERT embeddings for a word\n","def get_bert_embedding(word):\n","    inputs = tokenizer(word, return_tensors='pt')\n","    outputs = model(**inputs)\n","    # Take the mean of the last hidden state as the word embedding\n","    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n","\n","# List of words for embeddings\n","words = [\"king\", \"queen\", \"onion\", \"man\",\"feather\",\"scale\", \"wing\", \"fin\", \"fly\", \"wolf\", \"pelt\", \"swim\", \"fish\", \"air\", \"water\", \"swimming\", \"bird\", \"flying\", \"cheetah\", \"woman\", \"apple\", \"orange\", \"car\", \"train\", \"dog\", \"running\",\"cat\", \"red\", \"yellow\", \"banana\", \"lion\", \"wolf\", \"dog\"]\n","\n","# Generate BERT embeddings for all words\n","bert_embeddings = np.array([get_bert_embedding(word) for word in words])\n","\n","# Function to calculate pairwise distances for word relationships\n","def calculate_pairwise_distances(word_pairs):\n","    distance_results = []\n","    for (word1, word2, word3, word4) in word_pairs:\n","        idx1, idx2, idx3, idx4 = words.index(word1), words.index(word2), words.index(word3), words.index(word4)\n","        distance1 = np.linalg.norm(bert_embeddings[idx1] - bert_embeddings[idx2])  # Distance between word1 and word2\n","        distance2 = np.linalg.norm(bert_embeddings[idx3] - bert_embeddings[idx4])  # Distance between word3 and word4\n","        distance_results.append({\n","            \"Word Pair 1\": f\"{word1} - {word2}\",\n","            \"Distance 1\": round(distance1, 2),\n","            \"Word Pair 2\": f\"{word3} - {word4}\",\n","            \"Distance 2\": round(distance2, 2)\n","        })\n","\n","    return pd.DataFrame(distance_results)\n","\n","# Define specific word pairs to compare distances\n","word_pairs_for_distances = [\n","    (\"king\", \"man\", \"queen\", \"woman\"),\n","    (\"apple\", \"red\", \"banana\", \"yellow\"),\n","    (\"bird\", \"wing\", \"fish\", \"fin\")\n","]\n","\n","# Calculate pairwise distances using BERT embeddings\n","pairwise_distance_df = calculate_pairwise_distances(word_pairs_for_distances)\n","\n","# Display the resulting table\n","import IPython.display as display\n","display.display(pairwise_distance_df)\n"],"metadata":{"id":"ROYD3s4SGC9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.decomposition import PCA\n","\n","# Assuming you have real BERT embeddings:\n","# Replace with your precomputed BERT embeddings if available.\n","np.random.seed(42)\n","words = [\"king\", \"queen\", \"onion\", \"man\",\"feather\",\"scale\", \"wing\", \"fin\", \"fly\", \"wolf\", \"pelt\", \"swim\", \"fish\", \"air\", \"water\", \"swimming\", \"bird\", \"flying\", \"cheetah\", \"woman\", \"apple\", \"orange\", \"car\", \"train\", \"dog\", \"running\",\"cat\", \"red\", \"yellow\", \"banana\", \"lion\", \"wolf\", \"dog\"]\n","\n","bert_embeddings = np.random.rand(len(words), 8) * 2 - 1  # Random 8D embeddings as placeholder\n","\n","# Reduce dimensionality to 3D\n","pca = PCA(n_components=3)\n","bert_embeddings_3d = pca.fit_transform(bert_embeddings)\n","\n","# Create an interactive 3D scatter plot with Plotly\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[bert_embeddings_3d[i, 0]], y=[bert_embeddings_3d[i, 1]], z=[bert_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=10, color='skyblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Highlight the relationships with arrows\n","def add_arrow(word1, word2, color):\n","    idx1, idx2 = words.index(word1), words.index(word2)\n","    fig.add_trace(go.Scatter3d(\n","        x=[bert_embeddings_3d[idx1, 0], bert_embeddings_3d[idx2, 0]],\n","        y=[bert_embeddings_3d[idx1, 1], bert_embeddings_3d[idx2, 1]],\n","        z=[bert_embeddings_3d[idx1, 2], bert_embeddings_3d[idx2, 2]],\n","        mode='lines',\n","        line=dict(color=color, width=4),\n","        showlegend=False\n","    ))\n","\n","# Add arrows for specific relationships\n","add_arrow(\"king\", \"man\", \"blue\")\n","add_arrow(\"queen\", \"woman\", \"blue\")\n","add_arrow(\"bird\", \"air\", \"green\")\n","add_arrow(\"fish\", \"water\", \"green\")\n","add_arrow(\"banana\", \"yellow\", \"orange\")\n","add_arrow(\"apple\", \"red\", \"orange\")\n","\n","# Customize layout\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of BERT Word Embeddings\",\n","    scene=dict(\n","        xaxis_title='Principal Component 1',\n","        yaxis_title='Principal Component 2',\n","        zaxis_title='Principal Component 3',\n","        xaxis=dict(range=[-3, 3]),\n","        yaxis=dict(range=[-3, 3]),\n","        zaxis=dict(range=[-3, 3]),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"scV_EW6WGC6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install umap-learn"],"metadata":{"id":"pqgJLw7KGC3Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","import umap\n","\n","# Assuming you have real BERT embeddings:\n","np.random.seed(42)\n","words = [\"king\", \"queen\", \"man\", \"feather\", \"scale\", \"fish\", \"air\", \"water\",  \"bird\",\n","         \"woman\", \"apple\", \"car\", \"train\", \"dog\", \"red\", \"yellow\", \"banana\"]\n","\n","# Mock-up BERT embeddings as placeholder\n","bert_embeddings = np.random.rand(len(words), 8) * 2 - 1  # Random 8D embeddings\n","\n","# Reduce dimensionality to 3D using UMAP\n","umap_reducer = umap.UMAP(n_components=3, random_state=42)\n","umap_embeddings_3d = umap_reducer.fit_transform(bert_embeddings)\n","\n","# Create an interactive 3D scatter plot with Plotly using UMAP embeddings\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[i, 0]], y=[umap_embeddings_3d[i, 1]], z=[umap_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=10, color='skyblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Highlight the relationships with arrows\n","def add_arrow(word1, word2, color):\n","    idx1, idx2 = words.index(word1), words.index(word2)\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[idx1, 0], umap_embeddings_3d[idx2, 0]],\n","        y=[umap_embeddings_3d[idx1, 1], umap_embeddings_3d[idx2, 1]],\n","        z=[umap_embeddings_3d[idx1, 2], umap_embeddings_3d[idx2, 2]],\n","        mode='lines',\n","        line=dict(color=color, width=4),\n","        showlegend=False\n","    ))\n","\n","# Add arrows for specific relationships\n","add_arrow(\"king\", \"man\", \"blue\")\n","add_arrow(\"queen\", \"woman\", \"blue\")\n","add_arrow(\"bird\", \"air\", \"green\")\n","add_arrow(\"fish\", \"water\", \"green\")\n","add_arrow(\"banana\", \"yellow\", \"orange\")\n","add_arrow(\"apple\", \"red\", \"orange\")\n","\n","# Customize layout with camera settings\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of UMAP Word Embeddings\",\n","    scene=dict(\n","        xaxis_title='UMAP Component 1',\n","        yaxis_title='UMAP Component 2',\n","        zaxis_title='UMAP Component 3',\n","        xaxis=dict(range=[-3, 3]),\n","        yaxis=dict(range=[-3, 3]),\n","        zaxis=dict(range=[-3, 3]),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"xNVabBBf49ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","import umap\n","\n","# Generate random BERT embeddings (replace with real BERT embeddings)\n","np.random.seed(42)\n","words = [\"king\", \"queen\", \"man\", \"feather\", \"scale\", \"fish\", \"air\", \"water\", \"bird\",\n","         \"woman\", \"apple\", \"car\", \"train\", \"dog\", \"red\", \"yellow\", \"banana\"]\n","bert_embeddings = np.random.rand(len(words), 8) * 2 - 1  # Random 8D embeddings\n","\n","# Reduce dimensionality to 3D using UMAP\n","umap_reducer = umap.UMAP(n_components=3, random_state=42)\n","umap_embeddings_3d = umap_reducer.fit_transform(bert_embeddings)\n","\n","# Check if UMAP embeddings contain reasonable values\n","print(\"UMAP Embeddings:\\n\", umap_embeddings_3d)\n","\n","# Create an interactive 3D scatter plot with Plotly using UMAP embeddings\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[i, 0]], y=[umap_embeddings_3d[i, 1]], z=[umap_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=12, color='dodgerblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Customize layout with expanded axis ranges\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of UMAP Word Embeddings\",\n","    scene=dict(\n","        xaxis_title='UMAP Component 1',\n","        yaxis_title='UMAP Component 2',\n","        zaxis_title='UMAP Component 3',\n","        xaxis=dict(range=[-10, 10]),  # Expanded range\n","        yaxis=dict(range=[-10, 10]),  # Expanded range\n","        zaxis=dict(range=[-10, 10]),  # Expanded range\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"5y1Ze5ABT9lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","import umap\n","\n","# Generate random BERT embeddings (replace with real BERT embeddings)\n","np.random.seed(42)\n","words = [\"king\", \"queen\", \"man\", \"feather\", \"scale\", \"fish\", \"air\", \"water\", \"bird\",\n","         \"woman\", \"apple\", \"car\", \"train\", \"dog\", \"red\", \"yellow\", \"banana\"]\n","bert_embeddings = np.random.rand(len(words), 8) * 2 - 1  # Random 8D embeddings\n","\n","# Reduce dimensionality to 3D using UMAP\n","umap_reducer = umap.UMAP(n_components=3, random_state=42)\n","umap_embeddings_3d = umap_reducer.fit_transform(bert_embeddings)\n","\n","# Check if UMAP embeddings contain reasonable values\n","print(\"UMAP Embeddings:\\n\", umap_embeddings_3d)\n","\n","# Create an interactive 3D scatter plot with Plotly using UMAP embeddings\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[i, 0]], y=[umap_embeddings_3d[i, 1]], z=[umap_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=12, color='dodgerblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Customize layout with expanded axis ranges\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of UMAP Word Embeddings\",\n","    scene=dict(\n","        xaxis_title='UMAP Component 1',\n","        yaxis_title='UMAP Component 2',\n","        zaxis_title='UMAP Component 3',\n","        xaxis=dict(range=[-10, 10]),  # Expanded range\n","        yaxis=dict(range=[-10, 10]),  # Expanded range\n","        zaxis=dict(range=[-10, 10]),  # Expanded range\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"U7_8t5_rT9it"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# UMAP embeddings from your output\n","umap_embeddings_3d = np.array([\n","    [4.7127547, -4.5348916, -2.9967546],\n","    [3.5323043, -5.3822703, -3.7614396],\n","    [4.28121, -4.3525805, -3.8297942],\n","    [3.6564124, -4.8492675, -3.3479567],\n","    [4.193143, -4.2972136, -2.6517935],\n","    [4.373388, -5.121564, -4.453506],\n","    [4.0228915, -6.0763535, -3.1099324],\n","    [5.170019, -4.705997, -4.3024635],\n","    [4.730191, -5.5971584, -4.5858083],\n","    [3.975009, -4.086002, -3.069342],\n","    [4.83564, -5.858596, -3.4800653],\n","    [3.9853024, -6.034948, -3.7611744],\n","    [5.3133435, -5.61081, -3.9595273],\n","    [3.4316955, -4.6488657, -4.025736],\n","    [4.0231543, -5.5100274, -2.7066512],\n","    [5.3822894, -5.37214, -3.1826074],\n","    [5.349559, -4.4841084, -3.7119043]\n","])\n","\n","words = [\"king\", \"queen\", \"man\", \"feather\", \"scale\", \"fish\", \"air\", \"water\", \"bird\",\n","         \"woman\", \"apple\", \"car\", \"train\", \"dog\", \"red\", \"yellow\", \"banana\"]\n","\n","# Create an interactive 3D scatter plot with Plotly\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[i, 0]], y=[umap_embeddings_3d[i, 1]], z=[umap_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=12, color='dodgerblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Customize layout with tighter axis ranges\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of UMAP Word Embeddings\",\n","    scene=dict(\n","        xaxis_title='UMAP Component 1',\n","        yaxis_title='UMAP Component 2',\n","        zaxis_title='UMAP Component 3',\n","        xaxis=dict(range=[3, 6]),  # Adjusted to fit the data range\n","        yaxis=dict(range=[-7, -3]),\n","        zaxis=dict(range=[-5, -2]),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"oj_0X8ulT9fk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go\n","import numpy as np\n","\n","# UMAP embeddings from your output\n","umap_embeddings_3d = np.array([\n","    [4.7127547, -4.5348916, -2.9967546],\n","    [3.5323043, -5.3822703, -3.7614396],\n","    [4.28121, -4.3525805, -3.8297942],\n","    [3.6564124, -4.8492675, -3.3479567],\n","    [4.193143, -4.2972136, -2.6517935],\n","    [4.373388, -5.121564, -4.453506],\n","    [4.0228915, -6.0763535, -3.1099324],\n","    [5.170019, -4.705997, -4.3024635],\n","    [4.730191, -5.5971584, -4.5858083],\n","    [3.975009, -4.086002, -3.069342],\n","    [4.83564, -5.858596, -3.4800653],\n","    [3.9853024, -6.034948, -3.7611744],\n","    [5.3133435, -5.61081, -3.9595273],\n","    [3.4316955, -4.6488657, -4.025736],\n","    [4.0231543, -5.5100274, -2.7066512],\n","    [5.3822894, -5.37214, -3.1826074],\n","    [5.349559, -4.4841084, -3.7119043]\n","])\n","\n","words = [\"king\", \"queen\", \"man\", \"feather\", \"scale\", \"fish\", \"air\", \"water\", \"bird\",\n","         \"woman\", \"apple\", \"car\", \"train\", \"dog\", \"red\", \"yellow\", \"banana\"]\n","\n","# Create an interactive 3D scatter plot with Plotly\n","fig = go.Figure()\n","\n","# Add word embeddings as large points\n","for i, word in enumerate(words):\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[i, 0]], y=[umap_embeddings_3d[i, 1]], z=[umap_embeddings_3d[i, 2]],\n","        mode='markers+text',\n","        marker=dict(size=12, color='dodgerblue', opacity=0.8),\n","        text=word,\n","        textposition='top center',\n","        name=word\n","    ))\n","\n","# Function to add arrows for word relationships\n","def add_arrow(word1, word2, color):\n","    idx1, idx2 = words.index(word1), words.index(word2)\n","    fig.add_trace(go.Scatter3d(\n","        x=[umap_embeddings_3d[idx1, 0], umap_embeddings_3d[idx2, 0]],\n","        y=[umap_embeddings_3d[idx1, 1], umap_embeddings_3d[idx2, 1]],\n","        z=[umap_embeddings_3d[idx1, 2], umap_embeddings_3d[idx2, 2]],\n","        mode='lines',\n","        line=dict(color=color, width=4),\n","        showlegend=False\n","    ))\n","\n","# Add arrows for specific relationships\n","add_arrow(\"king\", \"man\", \"blue\")\n","add_arrow(\"queen\", \"woman\", \"blue\")\n","add_arrow(\"bird\", \"air\", \"green\")\n","add_arrow(\"fish\", \"water\", \"green\")\n","add_arrow(\"banana\", \"yellow\", \"orange\")\n","add_arrow(\"apple\", \"red\", \"orange\")\n","\n","# Customize layout with tighter axis ranges\n","fig.update_layout(\n","    title=\"Interactive 3D Visualization of UMAP Word Embeddings with Relationships\",\n","    scene=dict(\n","        xaxis_title='UMAP Component 1',\n","        yaxis_title='UMAP Component 2',\n","        zaxis_title='UMAP Component 3',\n","        xaxis=dict(range=[3, 6]),  # Adjusted to fit the data range\n","        yaxis=dict(range=[-7, -3]),\n","        zaxis=dict(range=[-5, -2]),\n","        aspectmode='cube'\n","    ),\n","    showlegend=False,\n","    margin=dict(l=0, r=0, t=50, b=0)\n",")\n","\n","fig.show()\n"],"metadata":{"id":"cu9j7KFrT9cR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"299u6UB_T9Y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fAUp050WT9Vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o9JlW5XpT9SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qtUH6A7dT9N3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1b_7q9_8T9J2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚õ≥ json to pdf + docx"],"metadata":{"id":"4v6-jGMeO9Zt"}},{"cell_type":"code","source":["\n","def json_to_html(json_data):\n","    # HTML-Template mit flexbox-basiertem Layout f√ºr \"alt\" und \"SEO\" nebeneinander\n","    html_template = \"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","        <title>Website Analyse</title>\n","        <style>\n","            body {\n","                font-family: Arial, sans-serif;\n","                margin: 20px;\n","                line-height: 1.6;\n","            }\n","            h1 {\n","                text-align: center;\n","                color: #333;\n","            }\n","            .section {\n","                margin-bottom: 20px;\n","            }\n","            .url {\n","                font-size: 1.2em;\n","                font-weight: bold;\n","                color: #007BFF;\n","                margin-bottom: 10px;\n","            }\n","            /* Flexbox f√ºr zwei Spalten nebeneinander */\n","            .compare-row {\n","                display: flex;\n","                flex-direction: row;\n","                gap: 20px; /* Abstand zwischen den Spalten */\n","                margin-bottom: 20px;\n","            }\n","            .column {\n","                flex: 1;\n","                border: 1px solid #ccc;\n","                padding: 10px;\n","                box-sizing: border-box;\n","            }\n","            .header {\n","                font-size: 1.1em;\n","                font-weight: bold;\n","                color: #555;\n","                margin-bottom: 10px;\n","            }\n","            .content {\n","                white-space: normal;\n","            }\n","            /* Um Zeilenumbr√ºche aus dem JSON in <br> umzuwandeln */\n","            .page-break {\n","                page-break-after: always;\n","            }\n","        </style>\n","    </head>\n","    <body>\n","        <h1>Website Analyse & SEO</h1>\n","        {% for url, sections in data.items() %}\n","        <div class=\"section\">\n","            <!-- Website-URL -->\n","            <p class=\"url\">Website: {{ url }}</p>\n","\n","            <!-- Beispiel: Andere Felder wie Analyse und Erkl√§rung einfach \"normal\" untereinander -->\n","            <p class=\"header\">Analyse</p>\n","            <p class=\"content\">{{ sections.Analyse | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <p class=\"header\">Erkl√§rung</p>\n","            <p class=\"content\">{{ sections.Erkl√§rung | replace('\\\\n','<br>') | safe }}</p>\n","\n","            <!-- Jetzt die beiden Felder \"alt\" und \"SEO\" nebeneinander -->\n","            <div class=\"compare-row\">\n","                <!-- linke Spalte: alt -->\n","                <div class=\"column\">\n","                    <p class=\"header\">alt</p>\n","                    <p class=\"content\">{{ sections.alt | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","                <!-- rechte Spalte: SEO -->\n","                <div class=\"column\">\n","                    <p class=\"header\">SEO</p>\n","                    <p class=\"content\">{{ sections.SEO | replace('\\\\n','<br>') | safe }}</p>\n","                </div>\n","            </div>\n","        </div>\n","        <div class=\"page-break\"></div>\n","        {% endfor %}\n","    </body>\n","    </html>\n","    \"\"\"\n","    # Jinja2-Template Rendering\n","    template = Template(html_template)\n","    html_output = template.render(data=json_data)\n","    return html_output\n","\n","\n","html_output = json_to_html(seo_json)\n","\n","# Speichere das HTML (Beispiel)\n","gdrive_seo_folder = userdata.get('gdrive_seo_folder')\n","with open(\"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(html_output)\n"],"metadata":{"id":"f4BFMU0q4Sig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def html_to_pdf_playwright(html_input, output_file):\n","    \"\"\"\n","    Nutzt das Headless Chromium von Playwright, um die HTML-Datei zu rendern\n","    und anschlie√üend als PDF zu speichern.\n","    \"\"\"\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch()\n","        page = await browser.new_page()\n","\n","        # Lokale Datei per file:// - Protokoll laden\n","        # oder du kannst stattdessen \"page.set_content()\" verwenden\n","        url = \"file://\" + html_input  # z.B. \"file:///content/drive/MyDrive/.../preview.html\"\n","        await page.goto(url, wait_until=\"load\")\n","\n","        # PDF erzeugen (A4, R√§nder anpassen etc.)\n","        await page.pdf(\n","            path=output_file,\n","            format=\"A4\",\n","            margin={\"top\": \"1cm\", \"right\": \"1cm\", \"bottom\": \"1cm\", \"left\": \"1cm\"}\n","        )\n","\n","        await browser.close()\n","\n","# Aufruf in Colab:\n","html_input = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.pdf\"\n","\n","# Instead of using asyncio.run(), use the following inside a notebook cell:\n","import nest_asyncio\n","nest_asyncio.apply() # This line applies a patch to allow nested event loops.\n","asyncio.run(html_to_pdf_playwright(html_input, output_file))\n","print(\"PDF mit Playwright erstellt.\")"],"metadata":{"id":"tyAM30Q56WUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/preview.html\"\n","output_file = \"/content/drive/MyDrive/\" + gdrive_seo_folder + \"/output.docx\"\n","\n","pypandoc.convert_file(\n","    source_file=input_file,\n","    to=\"docx\",\n","    outputfile=output_file,\n","    extra_args=[\"--standalone\"]\n",")\n","print(\"Konvertierung nach DOCX abgeschlossen.\")\n"],"metadata":{"id":"IkQFiA767hJS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üì• RAG"],"metadata":{"id":"HraJ3_5SJHSv"}},{"cell_type":"code","source":["\"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","\n","\"Als in Kenia geborene Kieferorthop√§din bringt Dr. Graf eine multikulturelle Perspektive mit und spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","\n","\"Die Hauptschwachstellen sind:\"\n","\n","\"Sie hat ihren Master of Science in Kieferorthop√§die von der Danube Private University, Krems, √ñsterreich, und hat an der Heinrich-Heine-Universit√§t D√ºsseldorf abgeschlossen.\"\n","\n","\"Ihre Qualifikationen umfassen nicht nur Fachwissen, sondern auch eine besondere Hingabe zu einem √§sthetischen L√§cheln. \"\n","\n","\"behandlungsorientierte Zahnberatung\"\n","\n","\"√§stehthetisches L√§cheln\""],"metadata":{"id":"fH4mRdPXceeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install langchain faiss-cpu\n"],"metadata":{"id":"4DOE2XX9dmow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install -U langchain-community"],"metadata":{"id":"x-BBLqldKMce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","pip install tiktoken"],"metadata":{"id":"-QDhcWZAKUsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import os\n","import openai\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.docstore.document import Document\n","\n","# 0) Vector Index (FAISS) initialisieren\n","#    (Sp√§ter im Code k√∂nnen wir den Index persistent speichern/neu laden)\n","# os.environ[\"OPENAI_API_KEY\"] = \"DEIN_OPENAI_API_KEY\"\n","\n","embeddings = OpenAIEmbeddings()\n","\n","# Beispiel-Fehler als \"Dokument\" f√ºr den Vector Store\n","# \"page_content\" = Text, \"metadata\" = beliebige Zusatzinfos\n","known_error_text = \"\"\"\n","Fehler: \"Klaren Aligner\" wird f√§lschlicherweise als Eigenname verwendet,\n","         obwohl es grammatisch richtig \"klaren Alignern\" sein sollte.\n","\n","Richtige Anwendung:\n","- Sagen: \"Entdecken Sie die Vorteile von klaren Alignern.\"\n","- Oder: \"Klare Aligner sind die ...\"\n","\n","Zus√§tzliche Hinweise:\n","- Beim Eindeutschen englischer Fachbegriffe auf die Pluralbildung achten.\n","\"\"\"\n","\n","doc = Document(\n","    page_content=known_error_text,\n","    metadata={\"error_type\": \"grammar/de-english\", \"example_id\": \"klaren-aligner\"}\n",")\n","\n","# Vektorindex erzeugen und das \"bekannte Fehler\"-Dokument ablegen\n","vector_store = FAISS.from_documents([doc], embeddings)\n"],"metadata":{"id":"PHubUMS4Jirw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","from langchain.docstore.document import Document\n","\n","# 1. Neuer Fehler: \"Kauen- und Sprechprobleme\" statt \"Kau- und Sprechprobleme\"\n","doc1_text = \"\"\"\n","Fehler: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kauen- und Sprechprobleme effektiv behandeln.\"\n","Richtig: \"Eine Zahnspange kann Kiefergelenksbeschwerden, Kau- und Sprechprobleme effektiv behandeln.\"\n","\n","Grund:\n","- Falsche Rechtschreibung/Zusammensetzung bei \"Kauen-\".\n","- Richtig ist \"Kau- und Sprechprobleme\".\n","\"\"\"\n","\n","doc1 = Document(\n","    page_content=doc1_text,\n","    metadata={\n","        \"error_type\": \"grammar/spelling\",\n","        \"example_id\": \"kauen-sprechprobleme\"\n","    }\n",")\n","\n","# 2. Neuer Fehler: falsche Formulierung bei Sprachen\n","doc2_text = \"\"\"\n","Fehler: \"Als in Kenia geborene Kieferorthop√§din ... spricht neben Deutsch auch Englisch, Swahili sowie √ºber Grundkenntnisse in Arabisch und Anf√§ngerkenntnisse in Spanisch.\"\n","Richtig: \"Als in Kenia geborene Kieferorthop√§din ... spricht neben Deutsch auch Englisch und Swahili und verf√ºgt √ºber Grundkenntnisse in Arabisch und Spanisch.\"\n","\n","Grund:\n","- Bessere Formulierung, um '√ºber Grundkenntnisse' mit 'verf√ºgt √ºber Grundkenntnisse' zu vereinen.\n","- Straffere und klarere Satzstruktur.\n","\"\"\"\n","\n","doc2 = Document(\n","    page_content=doc2_text,\n","    metadata={\n","        \"error_type\": \"grammar/style\",\n","        \"example_id\": \"languages-phrase\"\n","    }\n",")\n","\n","\n","# Angenommen, du hast bereits:\n","# embeddings = OpenAIEmbeddings()\n","# vector_store = FAISS.from_documents([some_initial_docs], embeddings)\n","#\n","# -> Dann f√ºgen wir jetzt doc1 und doc2 hinzu:\n","\n","vector_store.add_documents([doc1, doc2])\n"],"metadata":{"id":"ViQDjBE8WqAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# faiss_index_path = userdata.get('gdrive_seo_folder') + '/faiss_index'\n","# vector_store.save_local(faiss_index_path)"],"metadata":{"id":"ImJV6NYJ1p3u"},"execution_count":null,"outputs":[]},{"source":["# FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)"],"cell_type":"code","metadata":{"id":"CpXa96cN2Hdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def chunk_text_2(text, chunk_size=500):\n","    \"\"\"\n","    Beispiel: einfach alle 500 Zeichen ein Chunk.\n","    F√ºr echte Token-Logik kann man tiktoken oder langchain-Splitter nutzen.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start = end\n","    return chunks\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    text_chunks = chunk_text_2(seo_text, chunk_size=500)\n","    chunked_texts.append(text_chunks)\n","\n","# chunked_texts = [\n","#   [chunk1_of_text1, chunk2_of_text1, ...],\n","#   [chunk1_of_text2, ...],\n","#   ...\n","# ]\n"],"metadata":{"id":"V6Uh5AAO1Ol3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","from langchain.text_splitter import TokenTextSplitter\n","\n","def chunk_text_langchain(text, max_tokens=500, overlap=50):\n","    \"\"\"\n","    Teilt den Text anhand der Tokenanzahl auf. Nutzt daf√ºr LangChain's TokenTextSplitter.\n","    - max_tokens: maximale Tokens pro Chunk\n","    - overlap: wie viele Tokens √úberschneidung zum vorherigen Chunk\n","    \"\"\"\n","    splitter = TokenTextSplitter(\n","        encoding_name=\"cl100k_base\",  # oder passend zu deinem Modell (z.B. \"gpt-3.5-turbo\")\n","        chunk_size=max_tokens,         # maximale Anzahl Tokens pro Chunk\n","        chunk_overlap=overlap          # Tokens, die sich mit dem vorigen Chunk √ºberschneiden (Kontext)\n","    )\n","\n","    chunks = splitter.split_text(text)\n","    return chunks\n","\n","# Beispielanwendung:\n","# seo_text = \"\"\"Hier Dein langer Text, den du chunken willst ...\"\"\"\n","# chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","# print(chunked)\n","\n","\n","\n","chunked_texts = []\n","for seo_text in page_text_list:\n","    # Chunking pro SEO-Text\n","    chunked = chunk_text_langchain(seo_text, max_tokens=500, overlap=50)\n","    chunked_texts.append(text_chunks)\n"],"metadata":{"id":"FspYaRcb_hOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","def get_context_from_vector_store(chunk):\n","    \"\"\"\n","    Sucht im FAISS-Index nach passenden Dokumenten zum gegebenen Chunk,\n","    z. B. bekannte Fehler, die diesem Chunk √§hneln.\n","    \"\"\"\n","    # top_k=2 oder so, je nach Bedarf\n","    results = vector_store.similarity_search(chunk, k=2)\n","    # results ist eine Liste von Document-Objekten\n","\n","    # Wir wollen z. B. den Inhalt zusammenf√ºgen als \"Kontext\":\n","    context_text = \"\\n---\\n\".join([doc.page_content for doc in results])\n","    return context_text\n","\n","# Beispielhafte Abfrage pro Chunk\n","# test_chunk = chunked_texts[0][0]  # Erster Chunk des ersten Textes\n","# retrieved_context = get_context_from_vector_store(test_chunk)\n","# print(\"Kontext aus Vektorindex:\\n\", retrieved_context)\n"],"metadata":{"id":"XmYDr_FU2ahX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import json\n","\n","def proofread_text_with_context(chunk, context):\n","    \"\"\"\n","    Fragt ChatGPT (mittels der Chatbot-Klasse) an, um den Textchunk auf Fehler zu pr√ºfen und zu korrigieren.\n","    Nutzt den Kontext aus dem Vector Store, um bekannte Fehler zu ber√ºcksichtigen.\n","\n","    Erwartete Antwortstruktur (JSON):\n","\n","    {\n","      \"corrected_text\": \"...\",\n","      \"new_mistakes_found\": [\n","        {\n","          \"description\": \"Beschreibung des neuen Fehlers\",\n","          \"original_snippet\": \"Die fehlerhafte Passage\"\n","        },\n","        ...\n","      ]\n","    }\n","    \"\"\"\n","\n","    # 1. System Prompt\n","    system_prompt = (\n","        \"Du bist ein professioneller Lektor und Grammatik-Experte. \"\n","        \"Du kennst deutsche Grammatik, Rechtschreibung und eingedeutschte Fachbegriffe.\"\n","    )\n","\n","    # 2. User Prompt\n","    #    Wir kombinieren den Kontext und unseren zu pr√ºfenden Text, plus\n","    #    die Anweisung, nur JSON auszugeben.\n","    user_prompt = f\"\"\"\n","Im Folgenden siehst du bereits bekannte Fehlerhinweise (Kontext). Nutze diese Infos,\n","um den Text zu pr√ºfen und zu korrigieren. Solltest du neue Fehler (Grammatik,\n","falsch eingedeutschte Worte, Satzstellung etc.) finden, liste sie gesondert auf.\n","\n","Bekannte Fehler (Kontext):\n","{context}\n","\n","Text zur Pr√ºfung:\n","{chunk}\n","\n","Anweisung:\n","1) Analysiere den Text gr√ºndlich auf sprachliche/grammatische Fehler.\n","2) Nutze ggf. den Kontext.\n","3) Korrigiere diese Fehler im Text, ohne den Sinn zu ver√§ndern.\n","4) Liste alle neu gefundenen Fehler (noch nicht im Kontext) zus√§tzlich auf.\n","5) Antworte in folgendem JSON-Format (ohne weitere Worte davor oder danach!):\n","\n","{{\n","  \"corrected_text\": \"TEXTVERSION KORRIGIERT\",\n","  \"new_mistakes_found\": [\n","    {{\n","      \"description\": \"Beschreibung des Fehlers\",\n","      \"original_snippet\": \"Snippet der Original-Passage\"\n","    }}\n","  ]\n","}}\n","\"\"\"\n","\n","    # 3. Chatbot verwenden:\n","    cb = Chatbot(systemprompt=system_prompt, prompt=user_prompt)\n","\n","    # Da wir keine Streaming-Ausgabe brauchen, nutzen wir hier `chat()` statt `chat_with_streaming()`.\n","    response_raw = cb.chat()\n","\n","    # 4. JSON parsen\n","    try:\n","        parsed = json.loads(response_raw)\n","        # parsed = {\n","        #   \"corrected_text\": \"...\",\n","        #   \"new_mistakes_found\": [...]\n","        # }\n","        return parsed\n","\n","    except json.JSONDecodeError:\n","        print(\"Fehler: ChatGPT hat kein g√ºltiges JSON zur√ºckgegeben.\")\n","        return {\n","            \"corrected_text\": \"Fehler: Keine g√ºltige JSON-Antwort.\",\n","            \"new_mistakes_found\": []\n","        }\n"],"metadata":{"id":"grgqAg0K3uWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","all_corrected_texts = []\n","all_new_mistakes = []\n","\n","#for text_chunks in chunked_texts:  # => Jede Liste von Chunks (pro SEO-Text)\n","#    corrected_text_chunks = []\n","\n","#    for chunk in text_chunks:\n","#        # 3a) Kontext abfragen\n","#        context = get_context_from_vector_store(chunk)\n","#\n","#\n","#       # 4a) Prompt ChatGPT (Korrektur)\n","#        result = proofread_text_with_context(chunk, context)\n","#\n","#        corrected_text = result[\"corrected_text\"]\n","#        new_mistakes = result[\"new_mistakes_found\"]\n","#\n","#        # Sammeln\n","#        corrected_text_chunks.append(corrected_text)\n","#        all_new_mistakes.extend(new_mistakes)\n","#\n","#    # Pro SEO-Text f√ºgen wir die korrigierten Chunks zusammen.\n","#    full_corrected_text = \"\\n\".join(corrected_text_chunks)\n","#    all_corrected_texts.append(full_corrected_text)\n","\n","# Jetzt haben wir:\n","# all_corrected_texts = [ \"korrigierter SEO Text Nr.1\", \"korrigierter SEO Text Nr.2\", ...]\n","# all_new_mistakes = Liste aller neu gefundenen Fehler\n"],"metadata":{"id":"hPEJ9JtX4u_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# for _ in all_corrected_texts:\n","#   print(_)"],"metadata":{"id":"qIUE9Dfl5YzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all_new_mistakes"],"metadata":{"id":"k_VEFRpM6DQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-CAHAO66ub3"},"execution_count":null,"outputs":[]}]}